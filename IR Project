<<<<<<< HEAD
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Information Retrieval Final Project\n","## Bar Dolev 318419512\n","## Ido Paretsky 318510252\n","\n","### https://github.com/IdoParetsky/information-retrieval-final-project\n","### https://drive.google.com/drive/u/0/folders/1LUf_YLUbEo4Qj1CTqvKgqYiMgHy9Ejgf"],"metadata":{"id":"pBIYmR2y0jQK"}},{"cell_type":"markdown","source":["### GitHub Repository init. from within Google Drive"],"metadata":{"id":"QijpDuYg0i0J"}},{"cell_type":"code","source":["\"\"\"\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/Information Retrieval/Project\n","!git init information-retrieval-final-project\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"nZpg47mx0iSL","executionInfo":{"status":"ok","timestamp":1673004939384,"user_tz":-120,"elapsed":435,"user":{"displayName":"Ido Paretsky","userId":"00866448096400107402"}},"outputId":"9c38bc54-783b-4f55-fbbc-4341c1bf36c0"},"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\\n%cd /content/drive/MyDrive/Information Retrieval/Project\\n!git init information-retrieval-final-project\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":1}]},{"cell_type":"markdown","source":["Commit Command"],"metadata":{"id":"n1sfpWma-Eut"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/Information Retrieval/Project/information-retrieval-final-project/\n","!git config --global user.email \"ido.paretsky@gmail.com\"\n","!git config --global user.name \"IdoParetsky\"\n","#!git pull\n","!git add .\n","!git status\n","\n","!git commit --amend \"Edited Run Enginge Chunk to compute InvertedIndex for Title, Body and Anchor Text\"\n","\n","USERNAME = \"IdoParetsky\"\n","REPOSITORY = \"information-retrieval-final-project\"\n","GIT_TOKEN = \"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\"\n","\n","!git remote add origin https://{GIT_TOKEN}@github.com/{USERNAME}/{REPOSITORY}.git\n","!git remote -v\n","!git push -u origin master"],"metadata":{"id":"X_v8hDGz1gnW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673005312741,"user_tz":-120,"elapsed":23329,"user":{"displayName":"Ido Paretsky","userId":"00866448096400107402"}},"outputId":"7426e442-7506-4c82-bfed-8837f88f5a0d"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Information Retrieval/Project/information-retrieval-final-project\n","On branch master\n","Your branch is up to date with 'origin/master'.\n","\n","Changes to be committed:\n","  (use \"git reset HEAD <file>...\" to unstage)\n","\n","\t\u001b[32mmodified:   IR Project\u001b[m\n","\n","error: pathspec 'Edited Run Enginge Chunk to compute InvertedIndex for Title, Body and Anchor Text' did not match any file(s) known to git.\n","fatal: remote origin already exists.\n","origin\thttps://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@github.com/IdoParetsky/information-retrieval-final-project.git (fetch)\n","origin\thttps://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@github.com/IdoParetsky/information-retrieval-final-project.git (push)\n","Branch 'master' set up to track remote branch 'master' from 'origin'.\n","Everything up-to-date\n"]}]},{"cell_type":"markdown","source":["### Imports"],"metadata":{"id":"6wl97Fl8czLX"}},{"cell_type":"code","source":["from functools import partial\n","from xml.etree import ElementTree\n","import csv\n","import gdown\n","import math\n","\n","#%load_ext google.colab.data_table\n","import bz2\n","from collections import Counter, OrderedDict, defaultdict\n","import heapq\n","import codecs\n","import os\n","from operator import itemgetter\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from pathlib import Path\n","from time import time\n","import hashlib\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","import nltk\n","nltk.download('stopwords')\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import pandas as pd\n","from sklearn.metrics.pairwise import cosine_similarity\n","import re\n","import pickle\n","import numpy as np\n","\n","!gcloud dataproc clusters list --region us-central1\n","!pip install -q google-cloud-storage==1.43.0\n","!pip install -q graphframes\n","!pip install -q pyspark\n","!pip install -U -q PyDrive\n","!apt install openjdk-8-jdk-headless -qq\n","!pip install -q graphframes\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","graphframes_jar = 'https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.2-s_2.12/graphframes-0.8.2-spark3.2-s_2.12.jar'\n","spark_jars = '/usr/local/lib/python3.7/dist-packages/pyspark/jars'\n","!wget -N -P $spark_jars $graphframes_jar\n","import pyspark\n","from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","from graphframes import *\n","\n","import sys\n","from itertools import islice, count, groupby, chain\n","from time import time\n","from google.cloud import storage\n","\n","from tqdm import tqdm\n","from contextlib import closing\n","\n","import json\n","from io import StringIO\n","\n","!gcloud dataproc clusters list --region us-central1\n","!ls -l /usr/lib/spark/jars/graph*"],"metadata":{"id":"7LQSHUUicyup","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673005010115,"user_tz":-120,"elapsed":30105,"user":{"displayName":"Ido Paretsky","userId":"00866448096400107402"}},"outputId":"3357a8ac-8287-4822-c584-7727e5a3806f"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["NAME            PLATFORM  WORKER_COUNT  PREEMPTIBLE_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\n","projectcluster  GCE       4                                       RUNNING  us-central1-a\n","openjdk-8-jdk-headless is already the newest version (8u352-ga-1~18.04).\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'apt autoremove' to remove it.\n","0 upgraded, 0 newly installed, 0 to remove and 20 not upgraded.\n","--2023-01-06 11:36:46--  https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.2-s_2.12/graphframes-0.8.2-spark3.2-s_2.12.jar\n","Resolving repos.spark-packages.org (repos.spark-packages.org)... 108.156.83.116, 108.156.83.15, 108.156.83.69, ...\n","Connecting to repos.spark-packages.org (repos.spark-packages.org)|108.156.83.116|:443... connected.\n","HTTP request sent, awaiting response... 304 Not Modified\n","File ‘/usr/local/lib/python3.7/dist-packages/pyspark/jars/graphframes-0.8.2-spark3.2-s_2.12.jar’ not modified on server. Omitting download.\n","\n","NAME            PLATFORM  WORKER_COUNT  PREEMPTIBLE_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\n","projectcluster  GCE       4                                       RUNNING  us-central1-a\n","ls: cannot access '/usr/lib/spark/jars/graph*': No such file or directory\n"]}]},{"cell_type":"markdown","source":["### Colab Debugging Chunk"],"metadata":{"id":"1uwkAM2enkU-"}},{"cell_type":"code","source":["#@title Colab dependencies conditionally fused into Run Engine Chunk, for explicit colab debug run - unhide and uncomment\n","\"\"\"\n","# Initializing spark context\n","# create a spark context and session\n","conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n","conf.set(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.2-s_2.12\")\n","sc = pyspark.SparkContext(conf=conf)\n","sc.addPyFile(str(Path(spark_jars) / Path(graphframes_jar).name))\n","spark = SparkSession.builder.getOrCreate()\n","\n","\n","# Authenticate your user\n","# The authentication should be done with the email connected to your GCP account\n","from google.colab import auth\n","import signal\n","\n","AUTH_TIMEOUT = 60\n","\n","def handler(signum, frame):\n","  raise Exception(\"Authentication timeout!\")\n","\n","signal.signal(signal.SIGALRM, handler)\n","signal.alarm(AUTH_TIMEOUT)\n","\n","try:\n","   auth.authenticate_user()\n","except: \n","   pass\n","\n","\n","# Copy one wikidumps files \n","import os\n","from pathlib import Path\n","from google.colab import auth\n","## RENAME the project_id to yours project id from the project you created in GCP \n","project_id = 'crypto-lexicon-370515'\n","!gcloud config set project {project_id}\n","\n","data_bucket_name = 'wikidata_preprocessed'\n","try:\n","    if os.environ[\"wikidata_preprocessed\"] is not None:\n","        pass  \n","except:\n","      !mkdir wikidumps\n","      !gsutil -u {project_id} cp gs://{data_bucket_name}/multistream1_preprocessed.parquet \"wikidumps/\" \n","\n","\n","try:\n","    if os.environ[\"wikidata_preprocessed\"] is not None:\n","      path = os.environ[\"wikidata_preprocessed\"]+\"/wikidumps/*\"\n","except:\n","      path = \"wikidumps/*\"\n","\n","parquetFile = spark.read.parquet(path)\n","# take the 'text' and 'id' or the first 1000 rows and create an RDD from it\n","doc_text_pairs = parquetFile.limit(1000).select(\"title\", \"text\", \"anchor_text\", \"id\").rdd\n","\n","\n","english_stopwords = frozenset(stopwords.words('english'))\n","# We queried ChatGPT for Wikipedia-specific StopWords and added some of our own\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n","                    \"many\", \"however\", \"would\", \"became\", \"page\", \"article\",\n","                    \"information\", \"data\", \"reference\", \"source\", \"content\",\n","                    \"fact\", \"time\", \"year\", \"date\", \"place\", \"wiki\",\n","                    \"edit\", \"version\", \"user\", \"talk\", \"discussion\", \"template\",\n","                    \"category\", \"portal\", \"project\", \"author\", \"writer\",\n","                    \"creator\", \"publisher\", \"editor\", \"publication\", \"edition\",\n","                    \"volume\", \"number\", \"issue\", \"chapter\"]\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","\n","def word_count(text, id):\n","  ''' Count the frequency of each word in `text` (tf) that is not included in \n","  `all_stopwords` and return entries that will go into our posting lists. \n","  Parameters:\n","  -----------\n","    text: str\n","      Text of one document\n","    id: int\n","      Document id\n","  Returns:\n","  --------\n","    List of tuples\n","      A list of (token, (doc_id, tf)) pairs \n","      for example: [(\"Anarchism\", (12, 5)), ...]\n","  '''\n","  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","  # YOUR CODE HERE\n","  tokens = [t for t in tokens if t not in all_stopwords]\n","  word_counts = Counter()\n","  for w in tokens:\n","    count = word_counts.get(w, None)\n","    if not count:\n","        word_counts[w] = (id, 1)\n","    else:\n","      word_counts[w] = (id, word_counts.get(w)[1] + 1)\n","  return list(word_counts.items())\n","\n","word_counts = doc_text_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n","\n","\n","def reduce_word_counts(unsorted_pl):\n","  ''' Returns a sorted posting list by wiki_id.\n","  Parameters:\n","  -----------\n","    unsorted_pl: list of tuples\n","      A list of (wiki_id, tf) tuples \n","  Returns:\n","  --------\n","    list of tuples\n","      A sorted posting list.\n","  '''\n","  # YOUR CODE HERE\n","  return sorted(unsorted_pl, key=lambda t: t[0])\n","\n","postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n","\n","\n","postings_filtered = postings.filter(lambda x: len(x[1])>10)\n","\n","\n","def calculate_df(postings):\n","  ''' Takes a posting list RDD and calculate the df for each token.\n","  Parameters:\n","  -----------\n","    postings: RDD\n","      An RDD where each element is a (token, posting_list) pair.\n","  Returns:\n","  --------\n","    RDD\n","      An RDD where each element is a (token, df) pair.\n","  '''\n","  # YOUR CODE HERE\n","  return postings.map(lambda t: (t[0], len(t[1])))\n","\n","w2df = calculate_df(postings_filtered)\n","w2df_dict = w2df.collectAsMap()\n","\n","NUM_BUCKETS = 248\n","def token2bucket_id(token):\n","  return int(_hash(token),16) % NUM_BUCKETS\n","\n","def partition_postings_and_write(postings):\n","  ''' A function that partitions the posting lists into buckets, writes out \n","  all posting lists in a bucket to disk, and returns the posting locations for \n","  each bucket. Partitioning should be done through the use of `token2bucket` \n","  above. Writing to disk should use the function  `write_a_posting_list_colab`, a \n","  static method implemented in inverted_index_colab.py under the InvertedIndex \n","  class. \n","  Parameters:\n","  -----------\n","    postings: RDD\n","      An RDD where each item is a (w, posting_list) pair.\n","  Returns:\n","  --------\n","    RDD\n","      An RDD where each item is a posting locations dictionary for a bucket. The\n","      posting locations maintain a list for each word of file locations and \n","      offsets its posting list was written to. See `write_a_posting_list_colab` for \n","      more details.\n","  '''\n","  # YOUR CODE HERE\n","  bucket_id_posting_locs_tup_list = []\n","  b_w_pl_rdd = postings.map(lambda w_pl: (token2bucket_id(w_pl[0]), [w_pl])).reduceByKey(lambda a, b: a + b)\n","  return b_w_pl_rdd.map(lambda b_w_pl: InvertedIndex.write_a_posting_list_colab(b_w_pl))\n","\n","posting_locs_list = partition_postings_and_write(postings_filtered).collect()\n","\n","super_posting_locs = defaultdict(list)\n","for posting_loc in posting_locs_list:\n","  for k, v in posting_loc.items():\n","    super_posting_locs[k].extend(v)\n","\n","inverted = InvertedIndex()\n","inverted.posting_locs = super_posting_locs\n","inverted.df = w2df_dict\n","inverted.write_index('.', 'index')\n","\n","\n","TUPLE_SIZE = 6       \n","TF_MASK = 2 ** 16 - 1 # Masking the 16 low bits of an integer\n","from contextlib import closing\n","\n","def read_posting_list(inverted, w):\n","  with closing(MultiFileReader()) as reader:\n","    locs = inverted.posting_locs[w]\n","    b = reader.read(locs, inverted.df[w] * TUPLE_SIZE)\n","    posting_list = []\n","    for i in range(inverted.df[w]):\n","      doc_id = int.from_bytes(b[i*TUPLE_SIZE:i*TUPLE_SIZE+4], 'big')\n","      tf = int.from_bytes(b[i*TUPLE_SIZE+4:(i+1)*TUPLE_SIZE], 'big')\n","      posting_list.append((doc_id, tf))\n","    return posting_list\n","\n","\n","pages_links = spark.read.parquet(path).limit(1000).select(\"id\", \"anchor_text\").rdd\n","\n","def generate_graph(pages):\n","  ''' Compute the directed graph generated by wiki links.\n","  Parameters:\n","  -----------\n","    pages: RDD\n","      An RDD where each row consists of one wikipedia articles with 'id' and \n","      'anchor_text'.\n","  Returns:\n","  --------\n","    edges: RDD\n","      An RDD where each row represents an edge in the directed graph created by\n","      the wikipedia links. The first entry should the source page id and the \n","      second entry is the destination page id. No duplicates should be present. \n","    vertices: RDD\n","      An RDD where each row represents a vetrix (node) in the directed graph \n","      created by the wikipedia links. No duplicates should be present. \n","  '''\n","  # YOUR CODE HERE\n","  articles_to_link_lists = pages.mapValues(lambda anchor_text: [link[0] for link in anchor_text])\n","  edges = articles_to_link_lists.flatMap(lambda article_to_link_list: [(article_to_link_list[0], link) for link in article_to_link_list[1]]).distinct()\n","  vertices = edges.flatMap(lambda edge: edge).map(lambda v: (v,)).distinct()\n","  return edges, vertices\n","\n","edges, vertices = generate_graph(pages_links)\n","\n","edgesDF = edges.toDF(['src', 'dst']).repartition(4, 'src')\n","verticesDF = vertices.toDF(['id']).repartition(4, 'id')\n","g = GraphFrame(verticesDF, edgesDF)\n","pr_results = g.pageRank(resetProbability=0.15, maxIter=10)\n","pr = pr_results.vertices.select(\"id\", \"pagerank\")\n","pr = pr.sort(col('pagerank').desc())\n","pr.repartition(1).write.csv('pr', compression=\"gzip\")\n","pr.show()\n","\n","#If you have decided to do the bonus task - please copy the code here \n","\n","bonus_flag = True # Turn flag on (True) if you have implemented this part\n","\n","t_start = time()\n","\n","# PLACE YOUR CODE HERE\n","def divide_rank(v, links, rank):\n","    ranks = [(v, 0)]  # Vertex Rank sent to itself\n","    if len(links) > 0:\n","        rank_sent = rank / len(links)\n","        for l in links:\n","            ranks.append((l, rank_sent))\n","    return ranks\n","\n","def PageRank(resetProbability=0.15, maxIter=10):\n","    page_cnt = pages_links.count()\n","\n","    pr = pages_links.mapValues(lambda weight: 1 / page_cnt)  # Initial PageRank of 1 divided by page_cnt\n","\n","    for i in range(maxIter):  # Instead of until convergence, compute MaxIter times\n","        weighted_pages = pages_links.join(pr).flatMap(lambda title_links_pr: \n","                                                           divide_rank(title_links_pr[0], title_links_pr[1][0], title_links_pr[1][1]))\n","\n","        # Sum rank sent to each node\n","        pr = weighted_pages.reduceByKey(lambda r1, r2: r1 + r2) \\\n","            .mapValues(lambda agg_rank: (resetProbability / page_cnt) + (agg_rank * (1 - resetProbability)))\n","\n","    return pr.sortBy(lambda p: p[1], ascending=False)\n","\n","pr = PageRank(resetProbability=0.15, maxIter=10)\n","pr.show()\n","\n","pr_time_Bonus = time() - t_start\n","\"\"\""],"metadata":{"id":"9om5ZhuunjgH","colab":{"base_uri":"https://localhost:8080/","height":148},"executionInfo":{"status":"error","timestamp":1673005010117,"user_tz":-120,"elapsed":31,"user":{"displayName":"Ido Paretsky","userId":"00866448096400107402"}},"outputId":"36144d92-3040-4ad9-d396-84cf14fbf9c4"},"execution_count":4,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-e4635575d708>\"\u001b[0;36m, line \u001b[0;32m71\u001b[0m\n\u001b[0;31m    RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\u001b[0m\n\u001b[0m                                                                     \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected character after line continuation character\n"]}]},{"cell_type":"markdown","source":["### Utilities"],"metadata":{"id":"4BeQXPCn1Dkg"}},{"cell_type":"code","source":["english_stopwords = frozenset(stopwords.words('english'))\n","# We queried ChatGPT for Wikipedia-specific StopWords and added some of our own\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n","                    \"many\", \"however\", \"would\", \"became\", \"page\", \"article\",\n","                    \"information\", \"data\", \"reference\", \"source\", \"content\",\n","                    \"fact\", \"time\", \"year\", \"date\", \"place\", \"wiki\",\n","                    \"edit\", \"version\", \"user\", \"talk\", \"discussion\", \"template\",\n","                    \"category\", \"portal\", \"project\", \"author\", \"writer\",\n","                    \"creator\", \"publisher\", \"editor\", \"publication\", \"edition\",\n","                    \"volume\", \"number\", \"issue\", \"chapter\"]\n","\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","\n","group_to_idx = {'title': 0, 'body': 1, 'anchor': 2, 'id': 3}\n","\n","NUM_BUCKETS = 1024  # Applying a higher NUM_BUCKETS to investigate the trade-off between search time and memory constraints\n","def token2bucket_id(token, group):\n","  return int(_hash(token),16) % NUM_BUCKETS + (NUM_BUCKETS * group_to_idx[group])  # group: 0 - 'title', 1 - 'body', 2 - 'anchor'\n","\n","# PLACE YOUR CODE HERE\n","def word_count(group, id):\n","  ''' Count the frequency of each word in group (tf of title, text or anchor_text) that is not included in \n","  `all_stopwords` and return entries that will go into our posting lists. \n","  Parameters:\n","  -----------\n","    group: str\n","      Title, Text or Anchor Text of one document\n","    id: int\n","      Document id\n","  Returns:\n","  --------\n","    List of tuples\n","      A list of (token, (doc_id, tf)) pairs \n","      for example: [(\"Anarchism\", (12, 5)), ...]\n","  '''\n","  tokens = [token.group() for token in RE_WORD.finditer(group.lower())]\n","  # YOUR CODE HERE\n","  tokens = [t for t in tokens if t not in all_stopwords]\n","  word_counts = Counter()\n","  for w in tokens:\n","    count = word_counts.get(w, None)\n","    if not count:\n","        word_counts[w] = (id, 1)\n","    else:\n","      word_counts[w] = (id, word_counts.get(w)[1] + 1)\n","  return list(word_counts.items())\n","\n","def reduce_word_counts(unsorted_pl):\n","  ''' Returns a sorted posting list by wiki_id.\n","  Parameters:\n","  -----------\n","    unsorted_pl: list of tuples\n","      A list of (wiki_id, tf) tuples \n","  Returns:\n","  --------\n","    list of tuples\n","      A sorted posting list.\n","  '''\n","  # YOUR CODE HERE\n","  return sorted(unsorted_pl, key=lambda t: t[0]) \n","\n","def calculate_df(postings):\n","  ''' Takes a posting list RDD and calculate the df for each token.\n","  Parameters:\n","  -----------\n","    postings: RDD\n","      An RDD where each element is a (token, posting_list) pair.\n","  Returns:\n","  --------\n","    RDD\n","      An RDD where each element is a (token, df) pair.\n","  '''\n","  # YOUR CODE HERE\n","  return postings.map(lambda t: (t[0], len(t[1])))\n","\n","def partition_postings_and_write(postings, group):\n","  ''' A function that partitions the posting lists into buckets, writes out \n","  all posting lists in a bucket to disk, and returns the posting locations for \n","  each bucket. Partitioning should be done through the use of `token2bucket` \n","  above. Writing to disk should use the function  `write_a_posting_list_gcp`, a \n","  static method implemented in inverted_index_colab.py under the InvertedIndex \n","  class. \n","  Parameters:\n","  -----------\n","    postings: RDD\n","      An RDD where each item is a (w, posting_list) pair.\n","    \n","    group: string \n","      'title', 'body' or 'anchor'\n","\n","  Returns:\n","  --------\n","    RDD\n","      An RDD where each item is a posting locations dictionary for a bucket. The\n","      posting locations maintain a list for each word of file locations and \n","      offsets its posting list was written to. See `write_a_posting_list_gcp` for \n","      more details.\n","  '''\n","  # YOUR CODE HERE\n","  bucket_id_posting_locs_tup_list = []\n","  b_w_pl_rdd = postings.map(lambda w_pl: (token2bucket_id(w_pl[0], group_to_idx[group]), [w_pl])).reduceByKey(lambda a, b: a + b)\n","  return b_w_pl_rdd.map(lambda b_w_pl: InvertedIndex.write_a_posting_list_gcp(b_w_pl, '318419512_318510252') if is_gcp else \n","                                       InvertedIndex.write_a_posting_list_colab(b_w_pl))\n","\n","def count_words_and_write_filtered_posting_list(doc_title_text_anchor_quadruplets, group):\n","  \"\"\"\n","    Binds together the usage of word_count, reduce_word_counts, posting list filtering, calculate_df,\n","    mapping w2df into a dictionary and writing the posting list to prevent code duplication, as these\n","    operations to be applied thrice - on Title, Body and Anchor Text.\n","    \n","    Parameters:\n","    -----------\n","    group: string \n","      'title', 'body' or 'anchor'. ('id' is used in every iteration)\n","    \n","    Returns:\n","    -----------\n","    2-Tuple of (w2df_dict, posting_locs_list)\n","  \"\"\"\n","  word_counts = doc_title_text_anchor_quadruplets.flatMap(lambda x: word_count(x[group_to_idx[group]], x[group_to_idx['id']]))\n","  postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n","  postings_filtered = postings.filter(lambda x: len(x[1])>50)\n","  w2df = calculate_df(postings_filtered)\n","  w2df_dict = w2df.collectAsMap()\n","  posting_locs_list = partition_postings_and_write(postings_filtered, group).collect()\n","  return w2df_dict, posting_locs_list\n","\n","def create_and_upload_inverted_index_instance(w2df_dict, posting_locs_list, group):\n","  \"\"\"\n","    Collects all posting lists locations into one super-set and create an inverted index instance\n","    \n","    Parameters:\n","    -----------\n","    w2df: dict \n","      Word to Document Frequency Dicationary\n","\n","    posting_locs_list: list\n","      List of posting lists' locations\n","\n","    group: str\n","      'title', 'body' or 'anchor'\n","    \n","    Returns:\n","    -----------\n","    inverted_index.InvertedIndex instance\n","  \"\"\"\n","  # collect all posting lists locations into one super-set\n","  if not is_gcp:  # colab\n","    super_posting_locs = defaultdict(list)\n","    for posting_loc in posting_locs_list:\n","      for k, v in posting_loc.items():\n","        super_posting_locs[k].extend(v)\n","  else:  # GCP\n","    super_posting_locs = defaultdict(list)\n","    for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n","      if not blob.name.endswith(\"pickle\"):\n","        continue\n","      with blob.open(\"rb\") as f:\n","        posting_locs = pickle.load(f)\n","        for k, v in posting_locs.items():\n","          super_posting_locs[k].extend(v)\n","\n","  # Create inverted index instance\n","  inverted = InvertedIndex()\n","  # Adding the posting locations dictionary to the inverted index\n","  inverted.posting_locs = super_posting_locs\n","  # Add the token - df dictionary to the inverted index\n","  inverted.df = w2df_dict\n","  # write the global stats out\n","  inverted.write_index('.', f'{group}_index')\n","  if is_gcp:\n","  # upload to gs\n","    index_src = f\"{group}_index.pkl\"\n","    index_dst = f'gs://{bucket_name}/postings_gcp/{index_src}'\n","    !gsutil cp $index_src $index_dst\n","    !gsutil ls -lh $index_dst\n","  return inverted\n","\n","def generate_graph(pages):\n","  ''' Compute the directed graph generated by wiki links.\n","  Parameters:\n","  -----------\n","    pages: RDD\n","      An RDD where each row consists of one wikipedia articles with 'id' and \n","      'anchor_text'.\n","  Returns:\n","  --------\n","    edges: RDD\n","      An RDD where each row represents an edge in the directed graph created by\n","      the wikipedia links. The first entry should the source page id and the \n","      second entry is the destination page id. No duplicates should be present. \n","    vertices: RDD\n","      An RDD where each row represents a vetrix (node) in the directed graph \n","      created by the wikipedia links. No duplicates should be present. \n","  '''\n","  # YOUR CODE HERE\n","  articles_to_link_lists = pages.mapValues(lambda anchor_text: [link[0] for link in anchor_text])\n","  edges = articles_to_link_lists.flatMap(lambda article_to_link_list: [(article_to_link_list[0], link) for link in article_to_link_list[1]]).distinct()\n","  vertices = edges.flatMap(lambda edge: edge).map(lambda v: (v,)).distinct()\n","  return edges, vertices\n","\n","# Self-made PageRanks, to be tested against the given GraphFrame implementation\n","\"\"\"\n","def divide_rank(v, links, rank):\n","    ranks = [(v, 0)]  # Vertex Rank sent to itself\n","    if len(links) > 0:\n","        rank_sent = rank / len(links)\n","        for l in links:\n","            ranks.append((l, rank_sent))\n","    return ranks\n","\n","def PageRank(resetProbability=0.15, maxIter=10):\n","    page_cnt = pages_links.count()\n","\n","    pr = pages_links.mapValues(lambda weight: 1 / page_cnt)  # Initial PageRank of 1 divided by page_cnt\n","\n","    for i in range(maxIter):  # Instead of until convergence, compute MaxIter times\n","        weighted_pages = pages_links.join(pr).flatMap(lambda title_links_pr: \n","                                                           divide_rank(title_links_pr[0], title_links_pr[1][0], title_links_pr[1][1]))\n","\n","        # Sum rank sent to each node\n","        pr = weighted_pages.reduceByKey(lambda r1, r2: r1 + r2) \\\n","            .mapValues(lambda agg_rank: (resetProbability / page_cnt) + (agg_rank * (1 - resetProbability)))\n","\n","    return pr.sortBy(lambda p: p[1], ascending=False)\n","\n","pr = PageRank(resetProbability=0.15, maxIter=10)\n","pr.show()\n","\"\"\""],"metadata":{"id":"3a2SxV06xAmM","executionInfo":{"status":"aborted","timestamp":1673005010118,"user_tz":-120,"elapsed":25,"user":{"displayName":"Ido Paretsky","userId":"00866448096400107402"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Ranking"],"metadata":{"id":"TsPEAk-X5GRE"}},{"cell_type":"code","source":["def tf_idf_scores(data):\n","    \"\"\"\n","    This function calculates the tfidf for each word in a single document utilizing TfidfVectorizer via sklearn.\n","\n","    Parameters:\n","    -----------\n","      data: list of strings.  # TODO: Rewrite for GCP-scale data using PySpark MapReduce Arch!\n","    \n","    Returns:\n","    --------\n","      Two objects as follows:\n","                                a) DataFrame, documents as rows (i.e., 0,1,2,3, etc'), terms as columns ('bird','bright', etc').\n","                                b) TfidfVectorizer object.\n","\n","    \"\"\"\n","    # YOUR CODE HERE\n","    tf_idf_vectorizer = TfidfVectorizer(stop_words=\"english\")\n","    return pd.DataFrame(tf_idf_vectorizer.fit_transform(data).todense().tolist(), \n","                        columns=tf_idf_vectorizer.get_feature_names_out()), \\\n","           tf_idf_vectorizer\n","    \n","df_tfidfvect, tfidfvectorizer = tf_idf_scores(data)\n","\n","#queries_vector = tfidfvectorizer.transform(queries)\n","\n","\n","def cosine_sim_using_sklearn(queries,tfidf):\n","    \"\"\"\n","    In this function you need to utilize the cosine_similarity function from sklearn.\n","    You need to compute the similarity between the queries and the given documents.\n","    This function will return a DataFrame in the following shape: (# of queries, # of documents).\n","    Each value in the DataFrame will represent the cosine_similarity between given query and document.\n","    \n","    Parameters:\n","    -----------\n","      queries: sparse matrix represent the queries after transformation of tfidfvectorizer.\n","      documents: sparse matrix represent the documents.  # TODO: Rewrite for GCP-scale data using PySpark MapReduce Arch!\n","      \n","    Returns:\n","    --------\n","      DataFrame: This function will return a DataFrame in the following shape: (# of queries, # of documents).\n","      Each value in the DataFrame will represent the cosine_similarity between given query and document.\n","    \"\"\"\n","    # YOUR CODE HERE\n","    return pd.DataFrame(cosine_similarity(X=queries, Y=tfidf))\n","\n","#cosine_sim_df = cosine_sim_using_sklearn(queries_vector,df_tfidfvect)\n","\n","\n","def tokenize(text):\n","    \"\"\"\n","    This function aims in tokenize a text into a list of tokens. Moreover, it filter stopwords.\n","    \n","    Parameters:\n","    -----------\n","    text: string , represting the text to tokenize.    \n","    \n","    Returns:\n","    -----------\n","    list of tokens (e.g., list of tokens).\n","    \"\"\"\n","    list_of_tokens =  [token.group() for token in RE_WORD.finditer(text.lower()) if token.group() not in all_stopwords]    \n","    return list_of_tokens\n","\n","\n","#clean_data = [tokenize(doc) for doc in data]\n","\n","\n","# When preprocessing the data have a dictionary of document length for each document saved in a variable called `DL`.\n","class BM25_from_index:\n","    \"\"\"\n","    Best Match 25.    \n","    ----------\n","    k1 : float, default 1.5\n","\n","    b : float, default 0.75\n","\n","    index: inverted index\n","    \"\"\"\n","\n","    def __init__(self,index,k1=1.5, b=0.75):\n","        self.b = b\n","        self.k1 = k1\n","        self.index = index\n","        self.N = len(DL)\n","        self.AVGDL = sum(DL.values())/self.N\n","        self.words, self.pls = zip(*self.index.posting_lists_iter())        \n","\n","    def calc_idf(self,list_of_tokens):\n","        \"\"\"\n","        This function calculate the idf values according to the BM25 idf formula for each term in the query.\n","        \n","        Parameters:\n","        -----------\n","        query: list of token representing the query. For example: ['look', 'blue', 'sky']\n","        \n","        Returns:\n","        -----------\n","        idf: dictionary of idf scores. As follows: \n","                                                    key: term\n","                                                    value: bm25 idf score\n","        \"\"\"        \n","        idf = {}        \n","        for term in list_of_tokens:            \n","            if term in self.index.df.keys():\n","                n_ti = self.index.df[term]\n","                idf[term] = math.log(1 + (self.N - n_ti + 0.5) / (n_ti + 0.5))\n","            else:\n","                pass                             \n","        return idf\n","        \n","\n","    def search(self, queries,N=3):\n","        \"\"\"\n","        This function calculate the bm25 score for given query and document.\n","        We need to check only documents which are 'candidates' for a given query. \n","        This function return a dictionary of scores as the following:\n","                                                                    key: query_id\n","                                                                    value: a ranked list of pairs (doc_id, score) in the length of N.\n","        \n","        Parameters:\n","        -----------\n","        query: list of token representing the query. For example: ['look', 'blue', 'sky']\n","        doc_id: integer, document id.\n","        \n","        Returns:\n","        -----------\n","        score: float, bm25 score.\n","        \"\"\"\n","        # YOUR CODE HERE\n","        return {i: sorted(list(set([(doc_id, round(self._score(q, doc_id), 5)) for \n","                                    doc_id, score in get_candidate_documents_and_scores(q, self.index, self.words, self.pls)])), \n","                          key=lambda doc_id_score: doc_id_score[1], reverse=True)[:N] \n","                   for i, q in queries.items()}\n","\n","    def _score(self, query, doc_id):\n","        \"\"\"\n","        This function calculate the bm25 score for given query and document.\n","        \n","        Parameters:\n","        -----------\n","        query: list of token representing the query. For example: ['look', 'blue', 'sky']\n","        doc_id: integer, document id.\n","        \n","        Returns:\n","        -----------\n","        score: float, bm25 score.\n","        \"\"\"        \n","        score = 0.0        \n","        doc_len = DL[str(doc_id)]        \n","        self.idf = self.calc_idf(query)\n","\n","        for term in query:\n","            if term in self.index.term_total.keys():                \n","                term_frequencies = dict(self.pls[self.words.index(term)])                \n","                if doc_id in term_frequencies.keys():            \n","                    freq = term_frequencies[doc_id]\n","                    numerator = self.idf[term] * freq * (self.k1 + 1)\n","                    denominator = freq + self.k1 * (1 - self.b + self.b * doc_len / self.AVGDL)\n","                    score += (numerator / denominator)\n","        return score\n","\n","\n","def top_N_documents(df,N):\n","    \"\"\"\n","    This function sort and filter the top N docuemnts (by score) for each query.\n","    \n","    Parameters\n","    ----------    \n","    df: DataFrame (queries as rows, documents as columns)  # TODO: Rewrite for GCP-scale data using PySpark MapReduce Arch!\n","    N: Integer (how many document to retrieve for each query)    \n","\n","    Returns:\n","    ----------\n","    top_N: dictionary is the following stracture:\n","          key - query id.\n","          value - sorted (according to score) list of pairs lengh of N. Eac pair within the list provide the following information (doc id, score)\n","    \"\"\"    \n","    # YOUR CODE HERE\n","    return {i: list(row.sort_values(ascending=False)[:N].items()) for i, row in df.iterrows()}\n","\n","# top_10_docs = top_N_documents(cosine_sim_df,10)\n","\n","\n","# All cran data shall be replaced with the whole wiki dump via (for example):\n","\n","#parquetFile = spark.read.parquet(*paths)\n","#doc_text_pairs = parquetFile.select(\"text\", \"id\").rdd\n","\n","# word_counts = doc_text_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n","# postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n","# postings_filtered = postings.filter(lambda x: len(x[1])>50)\n","# w2df = calculate_df(postings_filtered)\n","# w2df_dict = w2df.collectAsMap()\n","# _ = partition_postings_and_write(postings_filtered).collect()\n","\n","# pages_links = spark.read.parquet(\"gs://wikidata20210801_preprocessed/*\").select(\"id\", \"anchor_text\").rdd\n","\n","# index_titles = InvertedIndex(docs=cran_txt_data_titles)\n","# index_text = InvertedIndex(docs=cran_txt_data_text)\n","# index_anchor = InvertedIndex(docs=cran_txt_data_anchor)\n","# create directories for the different indices \n","# !mkdir body_index title_index anchor_index\n","# index_titles.write('title_index','title')\n","# index_text.write('body_index','body')\n","# index_anchor.write('anchor_index','anchor')\n","\n","#idx_title = InvertedIndex.read_index('title_index', 'title')\n","#idx_body = InvertedIndex.read_index('body_index', 'body')\n","#idx_anchor = InvertedIndex.read_index('anchor_index', 'anchor')\n","## read posting lists from disk - unfit for GCP-scale data!\n","#title_words, title_pls = zip(*idx_title.posting_lists_iter())\n","#body_words, body_pls = zip(*idx_body.posting_lists_iter())\n","#anchor_words, anchor_pls = zip(*idx_anchor.posting_lists_iter())\n","\n","# bm25_title = BM25_from_index(idx_title)\n","# bm25_body = BM25_from_index(idx_body)\n","# bm25_anchor = BM25_from_index(idx_anchor)\n","# bm25_queries_score_train_title = bm25_title.search(cran_txt_query_text_train)\n","# bm25_queries_score_train_body = bm25_body.search(cran_txt_query_text_train)\n","# bm25_queries_score_train_anchor = bm25_anchor.search(cran_txt_query_text_train)\n","\n","\n","def generate_query_tfidf_vector(query_to_search,index):\n","    \"\"\" \n","    Generate a vector representing the query. Each entry within this vector represents a tfidf score.\n","    The terms representing the query will be the unique terms in the index.\n","\n","    We will use tfidf on the query as well. \n","    For calculation of IDF, use log with base 10.\n","    tf will be normalized based on the length of the query.    \n","\n","    Parameters:\n","    -----------\n","    query_to_search: list of tokens (str). This list will be preprocessed in advance (e.g., lower case, filtering stopwords, etc.'). \n","                     Example: 'Hello, I love information retrival' --->  ['hello','love','information','retrieval']\n","\n","    index:           inverted index loaded from the corresponding files.    \n","    \n","    Returns:\n","    -----------\n","    vectorized query with tfidf scores\n","    \"\"\"\n","    \n","    epsilon = .0000001\n","    total_vocab_size = len(index.term_total)\n","    Q = np.zeros((total_vocab_size))\n","    term_vector = list(index.term_total.keys())    \n","    counter = Counter(query_to_search)\n","    for token in np.unique(query_to_search):\n","        if token in index.term_total.keys(): #avoid terms that do not appear in the index.               \n","            tf = counter[token]/len(query_to_search) # term frequency divded by the length of the query\n","            df = index.df[token]            \n","            idf = math.log((len(DL))/(df+epsilon),10) #smoothing\n","            \n","            try:\n","                ind = term_vector.index(token)\n","                Q[ind] = tf*idf                    \n","            except:\n","                pass\n","    return Q\n","\n","def get_posting_iter(index):\n","    \"\"\"\n","    This function returning the iterator working with posting list.\n","    \n","    Parameters:\n","    ----------\n","    index: inverted index    \n","    \"\"\"\n","    words, pls = zip(*index.posting_lists_iter())\n","    return words,pls\n","\n","\n","def get_candidate_documents_and_scores(query_to_search,index,words,pls):\n","    \"\"\"\n","    Generate a dictionary representing a pool of candidate documents for a given query. This function will go through every token in query_to_search\n","    and fetch the corresponding information (e.g., term frequency, document frequency, etc.') needed to calculate TF-IDF from the posting list.\n","    Then it will populate the dictionary 'candidates.'\n","    For calculation of IDF, use log with base 10.\n","    tf will be normalized based on the length of the document.\n","    \n","    Parameters:\n","    -----------\n","    query_to_search: list of tokens (str). This list will be preprocessed in advance (e.g., lower case, filtering stopwords, etc.'). \n","                     Example: 'Hello, I love information retrival' --->  ['hello','love','information','retrieval']\n","\n","    index:           inverted index loaded from the corresponding files.\n","\n","    words,pls: iterator for working with posting.\n","    \n","    Returns:\n","    -----------\n","    dictionary of candidates. In the following format:\n","                                                               key: pair (doc_id,term)\n","                                                               value: tfidf score. \n","    \"\"\"\n","    candidates = {}\n","    for term in np.unique(query_to_search):\n","        if term in words:            \n","            list_of_doc = pls[words.index(term)]            \n","            normlized_tfidf = [(doc_id,(freq/DL[str(doc_id)])*math.log(len(DL)/index.df[term],10)) for doc_id, freq in list_of_doc]\n","            \n","            for doc_id, tfidf in normlized_tfidf:\n","                candidates[(doc_id,term)] = candidates.get((doc_id,term), 0) + tfidf               \n","\n","    return candidates\n","\n","\n","def generate_document_tfidf_matrix(query_to_search,index,words,pls):\n","    \"\"\"\n","    Generate a DataFrame `D` of tfidf scores for a given query. \n","    Rows will be the documents candidates for a given query\n","    Columns will be the unique terms in the index.\n","    The value for a given document and term will be its tfidf score.\n","    \n","    Parameters:\n","    -----------\n","    query_to_search: list of tokens (str). This list will be preprocessed in advance (e.g., lower case, filtering stopwords, etc.'). \n","                     Example: 'Hello, I love information retrival' --->  ['hello','love','information','retrieval']\n","\n","    index:           inverted index loaded from the corresponding files.\n","\n","    \n","    words,pls: iterator for working with posting.\n","\n","    Returns:\n","    -----------\n","    DataFrame of tfidf scores.\n","    \"\"\"\n","    \n","    total_vocab_size = len(index.term_total)\n","    candidates_scores = get_candidate_documents_and_scores(query_to_search,index,words,pls) #We do not need to utilize all document. Only the docuemnts which have corrspoinding terms with the query.\n","    unique_candidates = np.unique([doc_id for doc_id, freq in candidates_scores.keys()])\n","    D = np.zeros((len(unique_candidates), total_vocab_size))\n","    D = pd.DataFrame(D)\n","    \n","    D.index = unique_candidates\n","    D.columns = index.term_total.keys()\n","\n","    for key in candidates_scores:\n","        tfidf = candidates_scores[key]\n","        doc_id, term = key    \n","        D.loc[doc_id][term] = tfidf\n","\n","    return D\n","\n","\n","def cosine_similarity(D,Q):\n","    \"\"\"\n","    Calculate the cosine similarity for each candidate document in D and a given query (e.g., Q).\n","    Generate a dictionary of cosine similarity scores \n","    key: doc_id\n","    value: cosine similarity score\n","    \n","    Parameters:\n","    -----------\n","    D: DataFrame of tfidf scores.\n","\n","    Q: vectorized query with tfidf scores\n","    \n","    Returns:\n","    -----------\n","    dictionary of cosine similarity score as follows:\n","                                                                key: document id (e.g., doc_id)\n","                                                                value: cosine similarty score.\n","    \"\"\"\n","    # YOUR CODE HERE\n","    return {i: sum(R * Q) / (math.sqrt(pow(R, 2).sum() * pow(Q, 2).sum())) for i, R in D.iterrows()}\n","\n","\n","def get_top_n(sim_dict,N=3):\n","    \"\"\" \n","    Sort and return the highest N documents according to the cosine similarity score.\n","    Generate a dictionary of cosine similarity scores \n","   \n","    Parameters:\n","    -----------\n","    sim_dict: a dictionary of similarity score as follows:\n","                                                                key: document id (e.g., doc_id)\n","                                                                value: similarity score. We keep up to 5 digits after the decimal point. (e.g., round(score,5))\n","\n","    N: Integer (how many documents to retrieve). By default N = 3\n","    \n","    Returns:\n","    -----------\n","    a ranked list of pairs (doc_id, score) in the length of N.\n","    \"\"\"\n","    \n","    return sorted([(doc_id,round(score,5)) for doc_id, score in sim_dict.items()], key = lambda x: x[1],reverse=True)[:N]\n","\n","\n","def get_topN_score_for_queries(queries_to_search,index,N=3):\n","    \"\"\" \n","    Generate a dictionary that gathers for every query its topN score.\n","    \n","    Parameters:\n","    -----------\n","    queries_to_search: a dictionary of queries as follows: \n","                                                        key: query_id\n","                                                        value: list of tokens.\n","    index:           inverted index loaded from the corresponding files.    \n","    N: Integer. How many documents to retrieve. This argument is passed to the topN function. By default N = 3, for the topN function. \n","    \n","    Returns:\n","    -----------\n","    return: a dictionary of queries and topN pairs as follows:\n","                                                        key: query_id\n","                                                        value: list of pairs in the following format:(doc_id, score). \n","    \"\"\"\n","    # YOUR CODE HERE\n","    w, p = get_posting_iter(index)  # TODO: not usable on GCP-scale data\n","    return {i: get_top_n(cosine_similarity(\n","                         generate_document_tfidf_matrix(q, index, w, p), generate_query_tfidf_vector(q, index)), N=N) \n","               for i, q in queries_to_search.items()}\n","\n","# tfidf_queries_score_train = get_topN_score_for_queries(cran_txt_query_text_train,idx_title)\n","\n","\n","def merge_results(title_scores,body_scores, anchor_scores, title_weight=1/3, text_weight=1/3, anchor_weight=1/3, N = 3):    \n","    \"\"\"\n","    This function merge and sort documents retrieved by its weighted score (title, body and anchor). \n","\n","    Parameters:\n","    -----------\n","    title_scores: a dictionary build upon the title index of queries and tuples representing scores as follows: \n","                                                                            key: query_id\n","                                                                            value: list of pairs in the following format:(doc_id,score)\n","                \n","    body_scores: a dictionary build upon the body/text index of queries and tuples representing scores as follows: \n","                                                                            key: query_id\n","                                                                            value: list of pairs in the following format:(doc_id,score)\n","    \n","    anchor_scores: a dictionary build upon the anchor index of queries and tuples representing scores as follows: \n","                                                                            key: query_id\n","                                                                            value: list of pairs in the following format:(doc_id,score)\n","\n","    title_weight: float, for weighted average utilizing title, body and anchor scores\n","    text_weight: float, for weighted average utilizing title, body and anchor scores\n","    anchor_weight: float, for weighted average utilizing title, body and anchor scores\n","\n","    N: Integer. How many document to retrieve. This argument is passed to topN function. By default N = 3, for the topN function. \n","    \n","    Returns:\n","    -----------\n","    dictionary of querires and topN pairs as follows:\n","                                                        key: query_id\n","                                                        value: list of pairs in the following format:(doc_id,score). \n","    \"\"\"\n","    # YOUR CODE HERE\n","    titles = {i: [(doc_id, title_weight * score) for doc_id, score in doc_id_score] for i, doc_id_score in title_scores.items()}\n","    bodies = {j: [(doc_id, text_weight * score) for doc_id, score in doc_id_score] for j, doc_id_score in body_scores.items()}\n","    anchors = {j: [(doc_id, anchor_weight * score) for doc_id, score in doc_id_score] for j, doc_id_score in anchor_scores.items()}\n","    merged = {}\n","    for (i, title), (j, body), (k, anchor) in zip(titles.items(), bodies.items(), anchors.items()):\n","      concat = title + body + anchor\n","      merged[i] = defaultdict(int)\n","      for doc_id, score in concat:\n","          merged[i][doc_id] += score\n","      merged[i] = sorted(list(merged[i].items()), key=lambda doc_id_score: doc_id_score[1], reverse=True)[:N]\n","    return merged\n","\n","# thirds = merge_results(bm25_queries_score_train_title,bm25_queries_score_train_body, bm25_queries_score_train_anchor)        \n"],"metadata":{"id":"KEVb3zCuclP4","executionInfo":{"status":"aborted","timestamp":1673005010120,"user_tz":-120,"elapsed":26,"user":{"displayName":"Ido Paretsky","userId":"00866448096400107402"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Run Engine"],"metadata":{"id":"SYvvc0JfPESu"}},{"cell_type":"code","source":["is_gcp = False # Whether a colab debug run or GCP run is desired\n","\n","if not is_gcp:  # colab debug run\n","  # Initializing spark context\n","  # create a spark context and session\n","  conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n","  conf.set(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.2-s_2.12\")\n","  sc = pyspark.SparkContext(conf=conf)\n","  sc.addPyFile(str(Path(spark_jars) / Path(graphframes_jar).name))\n","  spark = SparkSession.builder.getOrCreate()\n","\n","\n","  # Authenticate your user\n","  # The authentication should be done with the email connected to your GCP account\n","  from google.colab import auth\n","  import signal\n","\n","  AUTH_TIMEOUT = 60\n","\n","  def handler(signum, frame):\n","    raise Exception(\"Authentication timeout!\")\n","\n","  signal.signal(signal.SIGALRM, handler)\n","  signal.alarm(AUTH_TIMEOUT)\n","\n","  try:\n","    auth.authenticate_user()\n","  except: \n","    pass\n","\n","\n","  # Copy one wikidumps files \n","  import os\n","  from pathlib import Path\n","  from google.colab import auth\n","  ## RENAME the project_id to yours project id from the project you created in GCP \n","  project_id = 'crypto-lexicon-370515'\n","  !gcloud config set project {project_id}\n","\n","  data_bucket_name = 'wikidata20210801_preprocessed'\n","  try:\n","      if os.environ[\"wikidata20210801_preprocessed\"] is not None:\n","          pass  \n","  except:\n","        !mkdir wikidumps\n","        !gsutil -u {project_id} cp gs://{data_bucket_name}/multistream1_preprocessed.parquet \"wikidumps/\" \n","\n","\n","  try:\n","      if os.environ[\"wikidata20210801_preprocessed\"] is not None:\n","        path = os.environ[\"wikidata20210801_preprocessed\"]+\"/wikidumps/*\"\n","  except:\n","        path = \"wikidumps/*\"\n","\n","  parquetFile = spark.read.parquet(path)\n","  # take the 'title', 'text', 'anchor_text' and 'id' or the first 1000 rows and create an RDD from it\n","  doc_title_text_anchor_quadruplets = parquetFile.limit(1000).select(\"title\", \"text\", \"anchor_text\", \"id\").rdd\n","\n","  from inverted_index import *\n","\n","else:  # GCP run\n","  # Put your bucket name below and make sure you can access it without an error\n","  bucket_name = '318419512_318510252' \n","  full_path = f\"gs://{318419512_318510252}/\"\n","  paths=[]\n","\n","  client = storage.Client()\n","  blobs = client.list_blobs(bucket_name)\n","  for b in blobs:\n","      if b.name != 'graphframes.sh':\n","          paths.append(full_path+b.name)\n","\n","\n","  parquetFile = spark.read.parquet(*paths)\n","  doc_title_text_anchor_quadruplets = parquetFile.select(\"title\", \"text\", \"anchor_text\", \"id\").rdd\n","  # if nothing prints here you forgot to upload the file inverted_index.py to the home dir\n","  %cd -q /home/dataproc\n","  !ls inverted_index.py\n","\n","  # adding our python module to the cluster\n","  sc.addFile(\"/home/dataproc/inverted_index.py\")\n","  sys.path.insert(0,SparkFiles.getRootDirectory())\n","\n","  from inverted_index import InvertedIndex\n","\n","title_w2df_dict, title_posting_locs_list = count_words_and_write_filtered_posting_list(doc_title_text_anchor_quadruplets, 'title')  # Title\n","body_w2df_dict, body_posting_locs_list = count_words_and_write_filtered_posting_list(doc_title_text_anchor_quadruplets, 'body')  # Body\n","anchor_w2df_dict, anchor_posting_locs_list = count_words_and_write_filtered_posting_list(doc_title_text_anchor_quadruplets, 'anchor')  # Anchor Text\n","\n","title_inverted = create_and_upload_inverted_index_instance(title_w2df_dict, title_posting_locs_list, 'title')\n","body_inverted = create_and_upload_inverted_index_instance(body_w2df_dict, body_posting_locs_list, 'body')\n","anchor_inverted = create_and_upload_inverted_index_instance(anchor_w2df_dict, anchor_posting_locs_list, 'anchor')\n","\n","pages_links = spark.read.parquet(\"gs://wikidata20210801_preprocessed/*\").select(\"id\", \"anchor_text\").rdd\n","\n","# construct the graph \n","edges, vertices = generate_graph(pages_links)\n","# compute PageRank\n","edgesDF = edges.toDF(['src', 'dst']).repartition(NUM_BUCKETS, 'src')\n","verticesDF = vertices.toDF(['id']).repartition(NUM_BUCKETS, 'id')\n","g = GraphFrame(verticesDF, edgesDF)\n","pr_results = g.pageRank(resetProbability=0.15, maxIter=6)\n","pr = pr_results.vertices.select(\"id\", \"pagerank\")\n","pr = pr.sort(col('pagerank').desc())\n","pr.repartition(1).write.csv(f'gs://{bucket_name}/pr', compression=\"gzip\")\n","pr.show()"],"metadata":{"id":"h8IJiX5wPDlA","executionInfo":{"status":"aborted","timestamp":1673005010121,"user_tz":-120,"elapsed":27,"user":{"displayName":"Ido Paretsky","userId":"00866448096400107402"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"rkCQMKS3PDKS"}}]}
=======
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Information Retrieval Final Project\n","## Bar Dolev 318419512\n","## Ido Paretsky 318510252\n","\n","### https://github.com/IdoParetsky/information-retrieval-final-project\n","### https://drive.google.com/drive/u/0/folders/1LUf_YLUbEo4Qj1CTqvKgqYiMgHy9Ejgf"],"metadata":{"id":"pBIYmR2y0jQK"}},{"cell_type":"markdown","source":["### GitHub Repository init. from within Google Drive"],"metadata":{"id":"QijpDuYg0i0J"}},{"cell_type":"code","source":["\"\"\"\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/Information Retrieval/Project\n","!git init information-retrieval-final-project\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nZpg47mx0iSL","executionInfo":{"status":"ok","timestamp":1672082446810,"user_tz":-120,"elapsed":2333,"user":{"displayName":"Ido Paretsky","userId":"00866448096400107402"}},"outputId":"41d79ccc-e727-484a-b2f7-b350f5dae4be"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/Information Retrieval/Project\n","Reinitialized existing Git repository in /content/drive/MyDrive/Information Retrieval/Project/information-retrieval-final-project/.git/\n","/content/drive/MyDrive/Information Retrieval/Project/information-retrieval-final-project\n"]}]},{"cell_type":"markdown","source":["Commit Command"],"metadata":{"id":"n1sfpWma-Eut"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/Information Retrieval/Project/information-retrieval-final-project/\n","!git config --global user.email \"ido.paretsky@gmail.com\"\n","!git config --global user.name \"IdoParetsky\"\n","#!git pull\n","!git add .\n","!git status\n","\n","!git commit -m \"Replaced inverted_index_gcp.py with a customized inverted_index.py, implemented initial Ranking code (Arch adaptation to MapReduce needed)\"\n","\n","USERNAME = \"IdoParetsky\"\n","REPOSITORY = \"information-retrieval-final-project\"\n","GIT_TOKEN = \"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\"\n","\n","!git remote add origin https://{GIT_TOKEN}@github.com/{USERNAME}/{REPOSITORY}.git\n","!git remote -v\n","!git push -u origin master"],"metadata":{"id":"X_v8hDGz1gnW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Imports"],"metadata":{"id":"6wl97Fl8czLX"}},{"cell_type":"code","source":["from functools import partial\n","from xml.etree import ElementTree\n","import csv\n","import gdown\n","import math\n","\n","#%load_ext google.colab.data_table\n","import bz2\n","from collections import Counter, OrderedDict, defaultdict\n","import heapq\n","import codecs\n","import os\n","from operator import itemgetter\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from pathlib import Path\n","from time import time\n","import hashlib\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","import nltk\n","nltk.download('stopwords')\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import pandas as pd\n","from sklearn.metrics.pairwise import cosine_similarity\n","import re\n","import pickle\n","import numpy as np\n","\n","!gcloud dataproc clusters list --region us-central1\n","!pip install -q google-cloud-storage==1.43.0\n","!pip install -q graphframes\n","!pip install -q pyspark\n","!pip install -U -q PyDrive\n","!apt install openjdk-8-jdk-headless -qq\n","!pip install -q graphframes\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","graphframes_jar = 'https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.2-s_2.12/graphframes-0.8.2-spark3.2-s_2.12.jar'\n","spark_jars = '/usr/local/lib/python3.7/dist-packages/pyspark/jars'\n","!wget -N -P $spark_jars $graphframes_jar\n","import pyspark\n","from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","from graphframes import *\n","\n","import sys\n","from itertools import islice, count, groupby, chain\n","from time import time\n","from google.cloud import storage\n","\n","from tqdm import tqdm\n","from contextlib import closing\n","\n","import json\n","from io import StringIO\n","\n","!gcloud dataproc clusters list --region us-central1\n","!ls -l /usr/lib/spark/jars/graph*"],"metadata":{"id":"7LQSHUUicyup","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673004728241,"user_tz":-120,"elapsed":32074,"user":{"displayName":"Ido Paretsky","userId":"00866448096400107402"}},"outputId":"da474256-555f-4123-9048-2a96154f039c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["NAME            PLATFORM  WORKER_COUNT  PREEMPTIBLE_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\n","projectcluster  GCE       4                                       RUNNING  us-central1-a\n","openjdk-8-jdk-headless is already the newest version (8u352-ga-1~18.04).\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'apt autoremove' to remove it.\n","0 upgraded, 0 newly installed, 0 to remove and 20 not upgraded.\n","--2023-01-06 11:32:04--  https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.2-s_2.12/graphframes-0.8.2-spark3.2-s_2.12.jar\n","Resolving repos.spark-packages.org (repos.spark-packages.org)... 108.156.83.37, 108.156.83.116, 108.156.83.15, ...\n","Connecting to repos.spark-packages.org (repos.spark-packages.org)|108.156.83.37|:443... connected.\n","HTTP request sent, awaiting response... 304 Not Modified\n","File ‘/usr/local/lib/python3.7/dist-packages/pyspark/jars/graphframes-0.8.2-spark3.2-s_2.12.jar’ not modified on server. Omitting download.\n","\n","NAME            PLATFORM  WORKER_COUNT  PREEMPTIBLE_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\n","projectcluster  GCE       4                                       RUNNING  us-central1-a\n","ls: cannot access '/usr/lib/spark/jars/graph*': No such file or directory\n"]}]},{"cell_type":"markdown","source":["### Colab Debugging Chunk"],"metadata":{"id":"1uwkAM2enkU-"}},{"cell_type":"code","source":["#@title Colab dependencies conditionally fused into Run Engine Chunk, for explicit colab debug run - unhide and uncomment\n","\"\"\"\n","# Initializing spark context\n","# create a spark context and session\n","conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n","conf.set(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.2-s_2.12\")\n","sc = pyspark.SparkContext(conf=conf)\n","sc.addPyFile(str(Path(spark_jars) / Path(graphframes_jar).name))\n","spark = SparkSession.builder.getOrCreate()\n","\n","\n","# Authenticate your user\n","# The authentication should be done with the email connected to your GCP account\n","from google.colab import auth\n","import signal\n","\n","AUTH_TIMEOUT = 60\n","\n","def handler(signum, frame):\n","  raise Exception(\"Authentication timeout!\")\n","\n","signal.signal(signal.SIGALRM, handler)\n","signal.alarm(AUTH_TIMEOUT)\n","\n","try:\n","   auth.authenticate_user()\n","except: \n","   pass\n","\n","\n","# Copy one wikidumps files \n","import os\n","from pathlib import Path\n","from google.colab import auth\n","## RENAME the project_id to yours project id from the project you created in GCP \n","project_id = 'crypto-lexicon-370515'\n","!gcloud config set project {project_id}\n","\n","data_bucket_name = 'wikidata_preprocessed'\n","try:\n","    if os.environ[\"wikidata_preprocessed\"] is not None:\n","        pass  \n","except:\n","      !mkdir wikidumps\n","      !gsutil -u {project_id} cp gs://{data_bucket_name}/multistream1_preprocessed.parquet \"wikidumps/\" \n","\n","\n","try:\n","    if os.environ[\"wikidata_preprocessed\"] is not None:\n","      path = os.environ[\"wikidata_preprocessed\"]+\"/wikidumps/*\"\n","except:\n","      path = \"wikidumps/*\"\n","\n","parquetFile = spark.read.parquet(path)\n","# take the 'text' and 'id' or the first 1000 rows and create an RDD from it\n","doc_text_pairs = parquetFile.limit(1000).select(\"title\", \"text\", \"anchor_text\", \"id\").rdd\n","\n","\n","english_stopwords = frozenset(stopwords.words('english'))\n","# We queried ChatGPT for Wikipedia-specific StopWords and added some of our own\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n","                    \"many\", \"however\", \"would\", \"became\", \"page\", \"article\",\n","                    \"information\", \"data\", \"reference\", \"source\", \"content\",\n","                    \"fact\", \"time\", \"year\", \"date\", \"place\", \"wiki\",\n","                    \"edit\", \"version\", \"user\", \"talk\", \"discussion\", \"template\",\n","                    \"category\", \"portal\", \"project\", \"author\", \"writer\",\n","                    \"creator\", \"publisher\", \"editor\", \"publication\", \"edition\",\n","                    \"volume\", \"number\", \"issue\", \"chapter\"]\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","\n","def word_count(text, id):\n","  ''' Count the frequency of each word in `text` (tf) that is not included in \n","  `all_stopwords` and return entries that will go into our posting lists. \n","  Parameters:\n","  -----------\n","    text: str\n","      Text of one document\n","    id: int\n","      Document id\n","  Returns:\n","  --------\n","    List of tuples\n","      A list of (token, (doc_id, tf)) pairs \n","      for example: [(\"Anarchism\", (12, 5)), ...]\n","  '''\n","  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","  # YOUR CODE HERE\n","  tokens = [t for t in tokens if t not in all_stopwords]\n","  word_counts = Counter()\n","  for w in tokens:\n","    count = word_counts.get(w, None)\n","    if not count:\n","        word_counts[w] = (id, 1)\n","    else:\n","      word_counts[w] = (id, word_counts.get(w)[1] + 1)\n","  return list(word_counts.items())\n","\n","word_counts = doc_text_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n","\n","\n","def reduce_word_counts(unsorted_pl):\n","  ''' Returns a sorted posting list by wiki_id.\n","  Parameters:\n","  -----------\n","    unsorted_pl: list of tuples\n","      A list of (wiki_id, tf) tuples \n","  Returns:\n","  --------\n","    list of tuples\n","      A sorted posting list.\n","  '''\n","  # YOUR CODE HERE\n","  return sorted(unsorted_pl, key=lambda t: t[0])\n","\n","postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n","\n","\n","postings_filtered = postings.filter(lambda x: len(x[1])>10)\n","\n","\n","def calculate_df(postings):\n","  ''' Takes a posting list RDD and calculate the df for each token.\n","  Parameters:\n","  -----------\n","    postings: RDD\n","      An RDD where each element is a (token, posting_list) pair.\n","  Returns:\n","  --------\n","    RDD\n","      An RDD where each element is a (token, df) pair.\n","  '''\n","  # YOUR CODE HERE\n","  return postings.map(lambda t: (t[0], len(t[1])))\n","\n","w2df = calculate_df(postings_filtered)\n","w2df_dict = w2df.collectAsMap()\n","\n","NUM_BUCKETS = 248\n","def token2bucket_id(token):\n","  return int(_hash(token),16) % NUM_BUCKETS\n","\n","def partition_postings_and_write(postings):\n","  ''' A function that partitions the posting lists into buckets, writes out \n","  all posting lists in a bucket to disk, and returns the posting locations for \n","  each bucket. Partitioning should be done through the use of `token2bucket` \n","  above. Writing to disk should use the function  `write_a_posting_list_colab`, a \n","  static method implemented in inverted_index_colab.py under the InvertedIndex \n","  class. \n","  Parameters:\n","  -----------\n","    postings: RDD\n","      An RDD where each item is a (w, posting_list) pair.\n","  Returns:\n","  --------\n","    RDD\n","      An RDD where each item is a posting locations dictionary for a bucket. The\n","      posting locations maintain a list for each word of file locations and \n","      offsets its posting list was written to. See `write_a_posting_list_colab` for \n","      more details.\n","  '''\n","  # YOUR CODE HERE\n","  bucket_id_posting_locs_tup_list = []\n","  b_w_pl_rdd = postings.map(lambda w_pl: (token2bucket_id(w_pl[0]), [w_pl])).reduceByKey(lambda a, b: a + b)\n","  return b_w_pl_rdd.map(lambda b_w_pl: InvertedIndex.write_a_posting_list_colab(b_w_pl))\n","\n","posting_locs_list = partition_postings_and_write(postings_filtered).collect()\n","\n","super_posting_locs = defaultdict(list)\n","for posting_loc in posting_locs_list:\n","  for k, v in posting_loc.items():\n","    super_posting_locs[k].extend(v)\n","\n","inverted = InvertedIndex()\n","inverted.posting_locs = super_posting_locs\n","inverted.df = w2df_dict\n","inverted.write_index('.', 'index')\n","\n","\n","TUPLE_SIZE = 6       \n","TF_MASK = 2 ** 16 - 1 # Masking the 16 low bits of an integer\n","from contextlib import closing\n","\n","def read_posting_list(inverted, w):\n","  with closing(MultiFileReader()) as reader:\n","    locs = inverted.posting_locs[w]\n","    b = reader.read(locs, inverted.df[w] * TUPLE_SIZE)\n","    posting_list = []\n","    for i in range(inverted.df[w]):\n","      doc_id = int.from_bytes(b[i*TUPLE_SIZE:i*TUPLE_SIZE+4], 'big')\n","      tf = int.from_bytes(b[i*TUPLE_SIZE+4:(i+1)*TUPLE_SIZE], 'big')\n","      posting_list.append((doc_id, tf))\n","    return posting_list\n","\n","\n","pages_links = spark.read.parquet(path).limit(1000).select(\"id\", \"anchor_text\").rdd\n","\n","def generate_graph(pages):\n","  ''' Compute the directed graph generated by wiki links.\n","  Parameters:\n","  -----------\n","    pages: RDD\n","      An RDD where each row consists of one wikipedia articles with 'id' and \n","      'anchor_text'.\n","  Returns:\n","  --------\n","    edges: RDD\n","      An RDD where each row represents an edge in the directed graph created by\n","      the wikipedia links. The first entry should the source page id and the \n","      second entry is the destination page id. No duplicates should be present. \n","    vertices: RDD\n","      An RDD where each row represents a vetrix (node) in the directed graph \n","      created by the wikipedia links. No duplicates should be present. \n","  '''\n","  # YOUR CODE HERE\n","  articles_to_link_lists = pages.mapValues(lambda anchor_text: [link[0] for link in anchor_text])\n","  edges = articles_to_link_lists.flatMap(lambda article_to_link_list: [(article_to_link_list[0], link) for link in article_to_link_list[1]]).distinct()\n","  vertices = edges.flatMap(lambda edge: edge).map(lambda v: (v,)).distinct()\n","  return edges, vertices\n","\n","edges, vertices = generate_graph(pages_links)\n","\n","edgesDF = edges.toDF(['src', 'dst']).repartition(4, 'src')\n","verticesDF = vertices.toDF(['id']).repartition(4, 'id')\n","g = GraphFrame(verticesDF, edgesDF)\n","pr_results = g.pageRank(resetProbability=0.15, maxIter=10)\n","pr = pr_results.vertices.select(\"id\", \"pagerank\")\n","pr = pr.sort(col('pagerank').desc())\n","pr.repartition(1).write.csv('pr', compression=\"gzip\")\n","pr.show()\n","\n","#If you have decided to do the bonus task - please copy the code here \n","\n","bonus_flag = True # Turn flag on (True) if you have implemented this part\n","\n","t_start = time()\n","\n","# PLACE YOUR CODE HERE\n","def divide_rank(v, links, rank):\n","    ranks = [(v, 0)]  # Vertex Rank sent to itself\n","    if len(links) > 0:\n","        rank_sent = rank / len(links)\n","        for l in links:\n","            ranks.append((l, rank_sent))\n","    return ranks\n","\n","def PageRank(resetProbability=0.15, maxIter=10):\n","    page_cnt = pages_links.count()\n","\n","    pr = pages_links.mapValues(lambda weight: 1 / page_cnt)  # Initial PageRank of 1 divided by page_cnt\n","\n","    for i in range(maxIter):  # Instead of until convergence, compute MaxIter times\n","        weighted_pages = pages_links.join(pr).flatMap(lambda title_links_pr: \n","                                                           divide_rank(title_links_pr[0], title_links_pr[1][0], title_links_pr[1][1]))\n","\n","        # Sum rank sent to each node\n","        pr = weighted_pages.reduceByKey(lambda r1, r2: r1 + r2) \\\n","            .mapValues(lambda agg_rank: (resetProbability / page_cnt) + (agg_rank * (1 - resetProbability)))\n","\n","    return pr.sortBy(lambda p: p[1], ascending=False)\n","\n","pr = PageRank(resetProbability=0.15, maxIter=10)\n","pr.show()\n","\n","pr_time_Bonus = time() - t_start\n","\"\"\""],"metadata":{"id":"9om5ZhuunjgH","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Utilities"],"metadata":{"id":"4BeQXPCn1Dkg"}},{"cell_type":"code","source":["english_stopwords = frozenset(stopwords.words('english'))\n","# We queried ChatGPT for Wikipedia-specific StopWords and added some of our own\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n","                    \"many\", \"however\", \"would\", \"became\", \"page\", \"article\",\n","                    \"information\", \"data\", \"reference\", \"source\", \"content\",\n","                    \"fact\", \"time\", \"year\", \"date\", \"place\", \"wiki\",\n","                    \"edit\", \"version\", \"user\", \"talk\", \"discussion\", \"template\",\n","                    \"category\", \"portal\", \"project\", \"author\", \"writer\",\n","                    \"creator\", \"publisher\", \"editor\", \"publication\", \"edition\",\n","                    \"volume\", \"number\", \"issue\", \"chapter\"]\n","\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","\n","group_to_idx = {'title': 0, 'body': 1, 'anchor': 2, 'id': 3}\n","\n","NUM_BUCKETS = 1024  # Applying a higher NUM_BUCKETS to investigate the trade-off between search time and memory constraints\n","def token2bucket_id(token, group):\n","  return int(_hash(token),16) % NUM_BUCKETS + (NUM_BUCKETS * group_to_idx[group])  # group: 0 - 'title', 1 - 'body', 2 - 'anchor'\n","\n","# PLACE YOUR CODE HERE\n","def word_count(group, id):\n","  ''' Count the frequency of each word in group (tf of title, text or anchor_text) that is not included in \n","  `all_stopwords` and return entries that will go into our posting lists. \n","  Parameters:\n","  -----------\n","    group: str\n","      Title, Text or Anchor Text of one document\n","    id: int\n","      Document id\n","  Returns:\n","  --------\n","    List of tuples\n","      A list of (token, (doc_id, tf)) pairs \n","      for example: [(\"Anarchism\", (12, 5)), ...]\n","  '''\n","  tokens = [token.group() for token in RE_WORD.finditer(group.lower())]\n","  # YOUR CODE HERE\n","  tokens = [t for t in tokens if t not in all_stopwords]\n","  word_counts = Counter()\n","  for w in tokens:\n","    count = word_counts.get(w, None)\n","    if not count:\n","        word_counts[w] = (id, 1)\n","    else:\n","      word_counts[w] = (id, word_counts.get(w)[1] + 1)\n","  return list(word_counts.items())\n","\n","def reduce_word_counts(unsorted_pl):\n","  ''' Returns a sorted posting list by wiki_id.\n","  Parameters:\n","  -----------\n","    unsorted_pl: list of tuples\n","      A list of (wiki_id, tf) tuples \n","  Returns:\n","  --------\n","    list of tuples\n","      A sorted posting list.\n","  '''\n","  # YOUR CODE HERE\n","  return sorted(unsorted_pl, key=lambda t: t[0]) \n","\n","def calculate_df(postings):\n","  ''' Takes a posting list RDD and calculate the df for each token.\n","  Parameters:\n","  -----------\n","    postings: RDD\n","      An RDD where each element is a (token, posting_list) pair.\n","  Returns:\n","  --------\n","    RDD\n","      An RDD where each element is a (token, df) pair.\n","  '''\n","  # YOUR CODE HERE\n","  return postings.map(lambda t: (t[0], len(t[1])))\n","\n","def partition_postings_and_write(postings, group):\n","  ''' A function that partitions the posting lists into buckets, writes out \n","  all posting lists in a bucket to disk, and returns the posting locations for \n","  each bucket. Partitioning should be done through the use of `token2bucket` \n","  above. Writing to disk should use the function  `write_a_posting_list_gcp`, a \n","  static method implemented in inverted_index_colab.py under the InvertedIndex \n","  class. \n","  Parameters:\n","  -----------\n","    postings: RDD\n","      An RDD where each item is a (w, posting_list) pair.\n","    \n","    group: string \n","      'title', 'body' or 'anchor'\n","\n","  Returns:\n","  --------\n","    RDD\n","      An RDD where each item is a posting locations dictionary for a bucket. The\n","      posting locations maintain a list for each word of file locations and \n","      offsets its posting list was written to. See `write_a_posting_list_gcp` for \n","      more details.\n","  '''\n","  # YOUR CODE HERE\n","  bucket_id_posting_locs_tup_list = []\n","  b_w_pl_rdd = postings.map(lambda w_pl: (token2bucket_id(w_pl[0], group_to_idx[group]), [w_pl])).reduceByKey(lambda a, b: a + b)\n","  return b_w_pl_rdd.map(lambda b_w_pl: InvertedIndex.write_a_posting_list_gcp(b_w_pl, '318419512_318510252') if is_gcp else \n","                                       InvertedIndex.write_a_posting_list_colab(b_w_pl))\n","\n","def count_words_and_write_filtered_posting_list(doc_title_text_anchor_quadruplets, group):\n","  \"\"\"\n","    Binds together the usage of word_count, reduce_word_counts, posting list filtering, calculate_df,\n","    mapping w2df into a dictionary and writing the posting list to prevent code duplication, as these\n","    operations to be applied thrice - on Title, Body and Anchor Text.\n","    \n","    Parameters:\n","    -----------\n","    group: string \n","      'title', 'body' or 'anchor'. ('id' is used in every iteration)\n","    \n","    Returns:\n","    -----------\n","    2-Tuple of (w2df_dict, posting_locs_list)\n","  \"\"\"\n","  word_counts = doc_title_text_anchor_quadruplets.flatMap(lambda x: word_count(x[group_to_idx[group]], x[group_to_idx['id']]))\n","  postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n","  postings_filtered = postings.filter(lambda x: len(x[1])>50)\n","  w2df = calculate_df(postings_filtered)\n","  w2df_dict = w2df.collectAsMap()\n","  posting_locs_list = partition_postings_and_write(postings_filtered, group).collect()\n","  return w2df_dict, posting_locs_list\n","\n","def create_and_upload_inverted_index_instance(w2df_dict, posting_locs_list, group):\n","  \"\"\"\n","    Collects all posting lists locations into one super-set and create an inverted index instance\n","    \n","    Parameters:\n","    -----------\n","    w2df: dict \n","      Word to Document Frequency Dicationary\n","\n","    posting_locs_list: list\n","      List of posting lists' locations\n","\n","    group: str\n","      'title', 'body' or 'anchor'\n","    \n","    Returns:\n","    -----------\n","    inverted_index.InvertedIndex instance\n","  \"\"\"\n","  # collect all posting lists locations into one super-set\n","  if not is_gcp:  # colab\n","    super_posting_locs = defaultdict(list)\n","    for posting_loc in posting_locs_list:\n","      for k, v in posting_loc.items():\n","        super_posting_locs[k].extend(v)\n","  else:  # GCP\n","    super_posting_locs = defaultdict(list)\n","    for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n","      if not blob.name.endswith(\"pickle\"):\n","        continue\n","      with blob.open(\"rb\") as f:\n","        posting_locs = pickle.load(f)\n","        for k, v in posting_locs.items():\n","          super_posting_locs[k].extend(v)\n","\n","  # Create inverted index instance\n","  inverted = InvertedIndex()\n","  # Adding the posting locations dictionary to the inverted index\n","  inverted.posting_locs = super_posting_locs\n","  # Add the token - df dictionary to the inverted index\n","  inverted.df = w2df_dict\n","  # write the global stats out\n","  inverted.write_index('.', f'{group}_index')\n","  if is_gcp:\n","  # upload to gs\n","    index_src = f\"{group}_index.pkl\"\n","    index_dst = f'gs://{bucket_name}/postings_gcp/{index_src}'\n","    !gsutil cp $index_src $index_dst\n","    !gsutil ls -lh $index_dst\n","  return inverted\n","\n","def generate_graph(pages):\n","  ''' Compute the directed graph generated by wiki links.\n","  Parameters:\n","  -----------\n","    pages: RDD\n","      An RDD where each row consists of one wikipedia articles with 'id' and \n","      'anchor_text'.\n","  Returns:\n","  --------\n","    edges: RDD\n","      An RDD where each row represents an edge in the directed graph created by\n","      the wikipedia links. The first entry should the source page id and the \n","      second entry is the destination page id. No duplicates should be present. \n","    vertices: RDD\n","      An RDD where each row represents a vetrix (node) in the directed graph \n","      created by the wikipedia links. No duplicates should be present. \n","  '''\n","  # YOUR CODE HERE\n","  articles_to_link_lists = pages.mapValues(lambda anchor_text: [link[0] for link in anchor_text])\n","  edges = articles_to_link_lists.flatMap(lambda article_to_link_list: [(article_to_link_list[0], link) for link in article_to_link_list[1]]).distinct()\n","  vertices = edges.flatMap(lambda edge: edge).map(lambda v: (v,)).distinct()\n","  return edges, vertices\n","\n","# Self-made PageRanks, to be tested against the given GraphFrame implementation\n","\"\"\"\n","def divide_rank(v, links, rank):\n","    ranks = [(v, 0)]  # Vertex Rank sent to itself\n","    if len(links) > 0:\n","        rank_sent = rank / len(links)\n","        for l in links:\n","            ranks.append((l, rank_sent))\n","    return ranks\n","\n","def PageRank(resetProbability=0.15, maxIter=10):\n","    page_cnt = pages_links.count()\n","\n","    pr = pages_links.mapValues(lambda weight: 1 / page_cnt)  # Initial PageRank of 1 divided by page_cnt\n","\n","    for i in range(maxIter):  # Instead of until convergence, compute MaxIter times\n","        weighted_pages = pages_links.join(pr).flatMap(lambda title_links_pr: \n","                                                           divide_rank(title_links_pr[0], title_links_pr[1][0], title_links_pr[1][1]))\n","\n","        # Sum rank sent to each node\n","        pr = weighted_pages.reduceByKey(lambda r1, r2: r1 + r2) \\\n","            .mapValues(lambda agg_rank: (resetProbability / page_cnt) + (agg_rank * (1 - resetProbability)))\n","\n","    return pr.sortBy(lambda p: p[1], ascending=False)\n","\n","pr = PageRank(resetProbability=0.15, maxIter=10)\n","pr.show()\n","\"\"\""],"metadata":{"id":"3a2SxV06xAmM","colab":{"base_uri":"https://localhost:8080/","height":157},"executionInfo":{"status":"ok","timestamp":1673004765308,"user_tz":-120,"elapsed":291,"user":{"displayName":"Ido Paretsky","userId":"00866448096400107402"}},"outputId":"e054ab3d-3c91-4c6e-ab4e-dae0174bbf20"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ndef divide_rank(v, links, rank):\\n    ranks = [(v, 0)]  # Vertex Rank sent to itself\\n    if len(links) > 0:\\n        rank_sent = rank / len(links)\\n        for l in links:\\n            ranks.append((l, rank_sent))\\n    return ranks\\n\\ndef PageRank(resetProbability=0.15, maxIter=10):\\n    page_cnt = pages_links.count()\\n\\n    pr = pages_links.mapValues(lambda weight: 1 / page_cnt)  # Initial PageRank of 1 divided by page_cnt\\n\\n    for i in range(maxIter):  # Instead of until convergence, compute MaxIter times\\n        weighted_pages = pages_links.join(pr).flatMap(lambda title_links_pr: \\n                                                           divide_rank(title_links_pr[0], title_links_pr[1][0], title_links_pr[1][1]))\\n\\n        # Sum rank sent to each node\\n        pr = weighted_pages.reduceByKey(lambda r1, r2: r1 + r2)             .mapValues(lambda agg_rank: (resetProbability / page_cnt) + (agg_rank * (1 - resetProbability)))\\n\\n    return pr.sortBy(lambda p: p[1], ascending=False)\\n\\npr = PageRank(resetProbability=0.15, maxIter=10)\\npr.show()\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","source":["### Ranking"],"metadata":{"id":"TsPEAk-X5GRE"}},{"cell_type":"code","source":["def tf_idf_scores(data):\n","    \"\"\"\n","    This function calculates the tfidf for each word in a single document utilizing TfidfVectorizer via sklearn.\n","\n","    Parameters:\n","    -----------\n","      data: list of strings.  # TODO: Rewrite for GCP-scale data using PySpark MapReduce Arch!\n","    \n","    Returns:\n","    --------\n","      Two objects as follows:\n","                                a) DataFrame, documents as rows (i.e., 0,1,2,3, etc'), terms as columns ('bird','bright', etc').\n","                                b) TfidfVectorizer object.\n","\n","    \"\"\"\n","    # YOUR CODE HERE\n","    tf_idf_vectorizer = TfidfVectorizer(stop_words=\"english\")\n","    return pd.DataFrame(tf_idf_vectorizer.fit_transform(data).todense().tolist(), \n","                        columns=tf_idf_vectorizer.get_feature_names_out()), \\\n","           tf_idf_vectorizer\n","    \n","df_tfidfvect, tfidfvectorizer = tf_idf_scores(data)\n","\n","#queries_vector = tfidfvectorizer.transform(queries)\n","\n","\n","def cosine_sim_using_sklearn(queries,tfidf):\n","    \"\"\"\n","    In this function you need to utilize the cosine_similarity function from sklearn.\n","    You need to compute the similarity between the queries and the given documents.\n","    This function will return a DataFrame in the following shape: (# of queries, # of documents).\n","    Each value in the DataFrame will represent the cosine_similarity between given query and document.\n","    \n","    Parameters:\n","    -----------\n","      queries: sparse matrix represent the queries after transformation of tfidfvectorizer.\n","      documents: sparse matrix represent the documents.  # TODO: Rewrite for GCP-scale data using PySpark MapReduce Arch!\n","      \n","    Returns:\n","    --------\n","      DataFrame: This function will return a DataFrame in the following shape: (# of queries, # of documents).\n","      Each value in the DataFrame will represent the cosine_similarity between given query and document.\n","    \"\"\"\n","    # YOUR CODE HERE\n","    return pd.DataFrame(cosine_similarity(X=queries, Y=tfidf))\n","\n","#cosine_sim_df = cosine_sim_using_sklearn(queries_vector,df_tfidfvect)\n","\n","\n","def tokenize(text):\n","    \"\"\"\n","    This function aims in tokenize a text into a list of tokens. Moreover, it filter stopwords.\n","    \n","    Parameters:\n","    -----------\n","    text: string , represting the text to tokenize.    \n","    \n","    Returns:\n","    -----------\n","    list of tokens (e.g., list of tokens).\n","    \"\"\"\n","    list_of_tokens =  [token.group() for token in RE_WORD.finditer(text.lower()) if token.group() not in all_stopwords]    \n","    return list_of_tokens\n","\n","\n","#clean_data = [tokenize(doc) for doc in data]\n","\n","\n","# When preprocessing the data have a dictionary of document length for each document saved in a variable called `DL`.\n","class BM25_from_index:\n","    \"\"\"\n","    Best Match 25.    \n","    ----------\n","    k1 : float, default 1.5\n","\n","    b : float, default 0.75\n","\n","    index: inverted index\n","    \"\"\"\n","\n","    def __init__(self,index,k1=1.5, b=0.75):\n","        self.b = b\n","        self.k1 = k1\n","        self.index = index\n","        self.N = len(DL)\n","        self.AVGDL = sum(DL.values())/self.N\n","        self.words, self.pls = zip(*self.index.posting_lists_iter())        \n","\n","    def calc_idf(self,list_of_tokens):\n","        \"\"\"\n","        This function calculate the idf values according to the BM25 idf formula for each term in the query.\n","        \n","        Parameters:\n","        -----------\n","        query: list of token representing the query. For example: ['look', 'blue', 'sky']\n","        \n","        Returns:\n","        -----------\n","        idf: dictionary of idf scores. As follows: \n","                                                    key: term\n","                                                    value: bm25 idf score\n","        \"\"\"        \n","        idf = {}        \n","        for term in list_of_tokens:            \n","            if term in self.index.df.keys():\n","                n_ti = self.index.df[term]\n","                idf[term] = math.log(1 + (self.N - n_ti + 0.5) / (n_ti + 0.5))\n","            else:\n","                pass                             \n","        return idf\n","        \n","\n","    def search(self, queries,N=3):\n","        \"\"\"\n","        This function calculate the bm25 score for given query and document.\n","        We need to check only documents which are 'candidates' for a given query. \n","        This function return a dictionary of scores as the following:\n","                                                                    key: query_id\n","                                                                    value: a ranked list of pairs (doc_id, score) in the length of N.\n","        \n","        Parameters:\n","        -----------\n","        query: list of token representing the query. For example: ['look', 'blue', 'sky']\n","        doc_id: integer, document id.\n","        \n","        Returns:\n","        -----------\n","        score: float, bm25 score.\n","        \"\"\"\n","        # YOUR CODE HERE\n","        return {i: sorted(list(set([(doc_id, round(self._score(q, doc_id), 5)) for \n","                                    doc_id, score in get_candidate_documents_and_scores(q, self.index, self.words, self.pls)])), \n","                          key=lambda doc_id_score: doc_id_score[1], reverse=True)[:N] \n","                   for i, q in queries.items()}\n","\n","    def _score(self, query, doc_id):\n","        \"\"\"\n","        This function calculate the bm25 score for given query and document.\n","        \n","        Parameters:\n","        -----------\n","        query: list of token representing the query. For example: ['look', 'blue', 'sky']\n","        doc_id: integer, document id.\n","        \n","        Returns:\n","        -----------\n","        score: float, bm25 score.\n","        \"\"\"        \n","        score = 0.0        \n","        doc_len = DL[str(doc_id)]        \n","        self.idf = self.calc_idf(query)\n","\n","        for term in query:\n","            if term in self.index.term_total.keys():                \n","                term_frequencies = dict(self.pls[self.words.index(term)])                \n","                if doc_id in term_frequencies.keys():            \n","                    freq = term_frequencies[doc_id]\n","                    numerator = self.idf[term] * freq * (self.k1 + 1)\n","                    denominator = freq + self.k1 * (1 - self.b + self.b * doc_len / self.AVGDL)\n","                    score += (numerator / denominator)\n","        return score\n","\n","\n","def top_N_documents(df,N):\n","    \"\"\"\n","    This function sort and filter the top N docuemnts (by score) for each query.\n","    \n","    Parameters\n","    ----------    \n","    df: DataFrame (queries as rows, documents as columns)  # TODO: Rewrite for GCP-scale data using PySpark MapReduce Arch!\n","    N: Integer (how many document to retrieve for each query)    \n","\n","    Returns:\n","    ----------\n","    top_N: dictionary is the following stracture:\n","          key - query id.\n","          value - sorted (according to score) list of pairs lengh of N. Eac pair within the list provide the following information (doc id, score)\n","    \"\"\"    \n","    # YOUR CODE HERE\n","    return {i: list(row.sort_values(ascending=False)[:N].items()) for i, row in df.iterrows()}\n","\n","# top_10_docs = top_N_documents(cosine_sim_df,10)\n","\n","\n","# All cran data shall be replaced with the whole wiki dump via (for example):\n","\n","#parquetFile = spark.read.parquet(*paths)\n","#doc_text_pairs = parquetFile.select(\"text\", \"id\").rdd\n","\n","# word_counts = doc_text_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n","# postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n","# postings_filtered = postings.filter(lambda x: len(x[1])>50)\n","# w2df = calculate_df(postings_filtered)\n","# w2df_dict = w2df.collectAsMap()\n","# _ = partition_postings_and_write(postings_filtered).collect()\n","\n","# pages_links = spark.read.parquet(\"gs://wikidata20210801_preprocessed/*\").select(\"id\", \"anchor_text\").rdd\n","\n","# index_titles = InvertedIndex(docs=cran_txt_data_titles)\n","# index_text = InvertedIndex(docs=cran_txt_data_text)\n","# index_anchor = InvertedIndex(docs=cran_txt_data_anchor)\n","# create directories for the different indices \n","# !mkdir body_index title_index anchor_index\n","# index_titles.write('title_index','title')\n","# index_text.write('body_index','body')\n","# index_anchor.write('anchor_index','anchor')\n","\n","#idx_title = InvertedIndex.read_index('title_index', 'title')\n","#idx_body = InvertedIndex.read_index('body_index', 'body')\n","#idx_anchor = InvertedIndex.read_index('anchor_index', 'anchor')\n","## read posting lists from disk - unfit for GCP-scale data!\n","#title_words, title_pls = zip(*idx_title.posting_lists_iter())\n","#body_words, body_pls = zip(*idx_body.posting_lists_iter())\n","#anchor_words, anchor_pls = zip(*idx_anchor.posting_lists_iter())\n","\n","# bm25_title = BM25_from_index(idx_title)\n","# bm25_body = BM25_from_index(idx_body)\n","# bm25_anchor = BM25_from_index(idx_anchor)\n","# bm25_queries_score_train_title = bm25_title.search(cran_txt_query_text_train)\n","# bm25_queries_score_train_body = bm25_body.search(cran_txt_query_text_train)\n","# bm25_queries_score_train_anchor = bm25_anchor.search(cran_txt_query_text_train)\n","\n","\n","def generate_query_tfidf_vector(query_to_search,index):\n","    \"\"\" \n","    Generate a vector representing the query. Each entry within this vector represents a tfidf score.\n","    The terms representing the query will be the unique terms in the index.\n","\n","    We will use tfidf on the query as well. \n","    For calculation of IDF, use log with base 10.\n","    tf will be normalized based on the length of the query.    \n","\n","    Parameters:\n","    -----------\n","    query_to_search: list of tokens (str). This list will be preprocessed in advance (e.g., lower case, filtering stopwords, etc.'). \n","                     Example: 'Hello, I love information retrival' --->  ['hello','love','information','retrieval']\n","\n","    index:           inverted index loaded from the corresponding files.    \n","    \n","    Returns:\n","    -----------\n","    vectorized query with tfidf scores\n","    \"\"\"\n","    \n","    epsilon = .0000001\n","    total_vocab_size = len(index.term_total)\n","    Q = np.zeros((total_vocab_size))\n","    term_vector = list(index.term_total.keys())    \n","    counter = Counter(query_to_search)\n","    for token in np.unique(query_to_search):\n","        if token in index.term_total.keys(): #avoid terms that do not appear in the index.               \n","            tf = counter[token]/len(query_to_search) # term frequency divded by the length of the query\n","            df = index.df[token]            \n","            idf = math.log((len(DL))/(df+epsilon),10) #smoothing\n","            \n","            try:\n","                ind = term_vector.index(token)\n","                Q[ind] = tf*idf                    \n","            except:\n","                pass\n","    return Q\n","\n","def get_posting_iter(index):\n","    \"\"\"\n","    This function returning the iterator working with posting list.\n","    \n","    Parameters:\n","    ----------\n","    index: inverted index    \n","    \"\"\"\n","    words, pls = zip(*index.posting_lists_iter())\n","    return words,pls\n","\n","\n","def get_candidate_documents_and_scores(query_to_search,index,words,pls):\n","    \"\"\"\n","    Generate a dictionary representing a pool of candidate documents for a given query. This function will go through every token in query_to_search\n","    and fetch the corresponding information (e.g., term frequency, document frequency, etc.') needed to calculate TF-IDF from the posting list.\n","    Then it will populate the dictionary 'candidates.'\n","    For calculation of IDF, use log with base 10.\n","    tf will be normalized based on the length of the document.\n","    \n","    Parameters:\n","    -----------\n","    query_to_search: list of tokens (str). This list will be preprocessed in advance (e.g., lower case, filtering stopwords, etc.'). \n","                     Example: 'Hello, I love information retrival' --->  ['hello','love','information','retrieval']\n","\n","    index:           inverted index loaded from the corresponding files.\n","\n","    words,pls: iterator for working with posting.\n","    \n","    Returns:\n","    -----------\n","    dictionary of candidates. In the following format:\n","                                                               key: pair (doc_id,term)\n","                                                               value: tfidf score. \n","    \"\"\"\n","    candidates = {}\n","    for term in np.unique(query_to_search):\n","        if term in words:            \n","            list_of_doc = pls[words.index(term)]            \n","            normlized_tfidf = [(doc_id,(freq/DL[str(doc_id)])*math.log(len(DL)/index.df[term],10)) for doc_id, freq in list_of_doc]\n","            \n","            for doc_id, tfidf in normlized_tfidf:\n","                candidates[(doc_id,term)] = candidates.get((doc_id,term), 0) + tfidf               \n","\n","    return candidates\n","\n","\n","def generate_document_tfidf_matrix(query_to_search,index,words,pls):\n","    \"\"\"\n","    Generate a DataFrame `D` of tfidf scores for a given query. \n","    Rows will be the documents candidates for a given query\n","    Columns will be the unique terms in the index.\n","    The value for a given document and term will be its tfidf score.\n","    \n","    Parameters:\n","    -----------\n","    query_to_search: list of tokens (str). This list will be preprocessed in advance (e.g., lower case, filtering stopwords, etc.'). \n","                     Example: 'Hello, I love information retrival' --->  ['hello','love','information','retrieval']\n","\n","    index:           inverted index loaded from the corresponding files.\n","\n","    \n","    words,pls: iterator for working with posting.\n","\n","    Returns:\n","    -----------\n","    DataFrame of tfidf scores.\n","    \"\"\"\n","    \n","    total_vocab_size = len(index.term_total)\n","    candidates_scores = get_candidate_documents_and_scores(query_to_search,index,words,pls) #We do not need to utilize all document. Only the docuemnts which have corrspoinding terms with the query.\n","    unique_candidates = np.unique([doc_id for doc_id, freq in candidates_scores.keys()])\n","    D = np.zeros((len(unique_candidates), total_vocab_size))\n","    D = pd.DataFrame(D)\n","    \n","    D.index = unique_candidates\n","    D.columns = index.term_total.keys()\n","\n","    for key in candidates_scores:\n","        tfidf = candidates_scores[key]\n","        doc_id, term = key    \n","        D.loc[doc_id][term] = tfidf\n","\n","    return D\n","\n","\n","def cosine_similarity(D,Q):\n","    \"\"\"\n","    Calculate the cosine similarity for each candidate document in D and a given query (e.g., Q).\n","    Generate a dictionary of cosine similarity scores \n","    key: doc_id\n","    value: cosine similarity score\n","    \n","    Parameters:\n","    -----------\n","    D: DataFrame of tfidf scores.\n","\n","    Q: vectorized query with tfidf scores\n","    \n","    Returns:\n","    -----------\n","    dictionary of cosine similarity score as follows:\n","                                                                key: document id (e.g., doc_id)\n","                                                                value: cosine similarty score.\n","    \"\"\"\n","    # YOUR CODE HERE\n","    return {i: sum(R * Q) / (math.sqrt(pow(R, 2).sum() * pow(Q, 2).sum())) for i, R in D.iterrows()}\n","\n","\n","def get_top_n(sim_dict,N=3):\n","    \"\"\" \n","    Sort and return the highest N documents according to the cosine similarity score.\n","    Generate a dictionary of cosine similarity scores \n","   \n","    Parameters:\n","    -----------\n","    sim_dict: a dictionary of similarity score as follows:\n","                                                                key: document id (e.g., doc_id)\n","                                                                value: similarity score. We keep up to 5 digits after the decimal point. (e.g., round(score,5))\n","\n","    N: Integer (how many documents to retrieve). By default N = 3\n","    \n","    Returns:\n","    -----------\n","    a ranked list of pairs (doc_id, score) in the length of N.\n","    \"\"\"\n","    \n","    return sorted([(doc_id,round(score,5)) for doc_id, score in sim_dict.items()], key = lambda x: x[1],reverse=True)[:N]\n","\n","\n","def get_topN_score_for_queries(queries_to_search,index,N=3):\n","    \"\"\" \n","    Generate a dictionary that gathers for every query its topN score.\n","    \n","    Parameters:\n","    -----------\n","    queries_to_search: a dictionary of queries as follows: \n","                                                        key: query_id\n","                                                        value: list of tokens.\n","    index:           inverted index loaded from the corresponding files.    \n","    N: Integer. How many documents to retrieve. This argument is passed to the topN function. By default N = 3, for the topN function. \n","    \n","    Returns:\n","    -----------\n","    return: a dictionary of queries and topN pairs as follows:\n","                                                        key: query_id\n","                                                        value: list of pairs in the following format:(doc_id, score). \n","    \"\"\"\n","    # YOUR CODE HERE\n","    w, p = get_posting_iter(index)  # TODO: not usable on GCP-scale data\n","    return {i: get_top_n(cosine_similarity(\n","                         generate_document_tfidf_matrix(q, index, w, p), generate_query_tfidf_vector(q, index)), N=N) \n","               for i, q in queries_to_search.items()}\n","\n","# tfidf_queries_score_train = get_topN_score_for_queries(cran_txt_query_text_train,idx_title)\n","\n","\n","def merge_results(title_scores,body_scores, anchor_scores, title_weight=1/3, text_weight=1/3, anchor_weight=1/3, N = 3):    \n","    \"\"\"\n","    This function merge and sort documents retrieved by its weighted score (title, body and anchor). \n","\n","    Parameters:\n","    -----------\n","    title_scores: a dictionary build upon the title index of queries and tuples representing scores as follows: \n","                                                                            key: query_id\n","                                                                            value: list of pairs in the following format:(doc_id,score)\n","                \n","    body_scores: a dictionary build upon the body/text index of queries and tuples representing scores as follows: \n","                                                                            key: query_id\n","                                                                            value: list of pairs in the following format:(doc_id,score)\n","    \n","    anchor_scores: a dictionary build upon the anchor index of queries and tuples representing scores as follows: \n","                                                                            key: query_id\n","                                                                            value: list of pairs in the following format:(doc_id,score)\n","\n","    title_weight: float, for weighted average utilizing title, body and anchor scores\n","    text_weight: float, for weighted average utilizing title, body and anchor scores\n","    anchor_weight: float, for weighted average utilizing title, body and anchor scores\n","\n","    N: Integer. How many document to retrieve. This argument is passed to topN function. By default N = 3, for the topN function. \n","    \n","    Returns:\n","    -----------\n","    dictionary of querires and topN pairs as follows:\n","                                                        key: query_id\n","                                                        value: list of pairs in the following format:(doc_id,score). \n","    \"\"\"\n","    # YOUR CODE HERE\n","    titles = {i: [(doc_id, title_weight * score) for doc_id, score in doc_id_score] for i, doc_id_score in title_scores.items()}\n","    bodies = {j: [(doc_id, text_weight * score) for doc_id, score in doc_id_score] for j, doc_id_score in body_scores.items()}\n","    anchors = {j: [(doc_id, anchor_weight * score) for doc_id, score in doc_id_score] for j, doc_id_score in anchor_scores.items()}\n","    merged = {}\n","    for (i, title), (j, body), (k, anchor) in zip(titles.items(), bodies.items(), anchors.items()):\n","      concat = title + body + anchor\n","      merged[i] = defaultdict(int)\n","      for doc_id, score in concat:\n","          merged[i][doc_id] += score\n","      merged[i] = sorted(list(merged[i].items()), key=lambda doc_id_score: doc_id_score[1], reverse=True)[:N]\n","    return merged\n","\n","# thirds = merge_results(bm25_queries_score_train_title,bm25_queries_score_train_body, bm25_queries_score_train_anchor)        \n"],"metadata":{"id":"KEVb3zCuclP4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Run Engine"],"metadata":{"id":"SYvvc0JfPESu"}},{"cell_type":"code","source":["is_gcp = False # Whether a colab debug run or GCP run is desired\n","\n","if not is_gcp:  # colab debug run\n","  # Initializing spark context\n","  # create a spark context and session\n","  conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n","  conf.set(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.2-s_2.12\")\n","  sc = pyspark.SparkContext(conf=conf)\n","  sc.addPyFile(str(Path(spark_jars) / Path(graphframes_jar).name))\n","  spark = SparkSession.builder.getOrCreate()\n","\n","\n","  # Authenticate your user\n","  # The authentication should be done with the email connected to your GCP account\n","  from google.colab import auth\n","  import signal\n","\n","  AUTH_TIMEOUT = 60\n","\n","  def handler(signum, frame):\n","    raise Exception(\"Authentication timeout!\")\n","\n","  signal.signal(signal.SIGALRM, handler)\n","  signal.alarm(AUTH_TIMEOUT)\n","\n","  try:\n","    auth.authenticate_user()\n","  except: \n","    pass\n","\n","\n","  # Copy one wikidumps files \n","  import os\n","  from pathlib import Path\n","  from google.colab import auth\n","  ## RENAME the project_id to yours project id from the project you created in GCP \n","  project_id = 'crypto-lexicon-370515'\n","  !gcloud config set project {project_id}\n","\n","  data_bucket_name = 'wikidata20210801_preprocessed'\n","  try:\n","      if os.environ[\"wikidata20210801_preprocessed\"] is not None:\n","          pass  \n","  except:\n","        !mkdir wikidumps\n","        !gsutil -u {project_id} cp gs://{data_bucket_name}/multistream1_preprocessed.parquet \"wikidumps/\" \n","\n","\n","  try:\n","      if os.environ[\"wikidata20210801_preprocessed\"] is not None:\n","        path = os.environ[\"wikidata20210801_preprocessed\"]+\"/wikidumps/*\"\n","  except:\n","        path = \"wikidumps/*\"\n","\n","  parquetFile = spark.read.parquet(path)\n","  # take the 'title', 'text', 'anchor_text' and 'id' or the first 1000 rows and create an RDD from it\n","  doc_title_text_anchor_quadruplets = parquetFile.limit(1000).select(\"title\", \"text\", \"anchor_text\", \"id\").rdd\n","\n","  from inverted_index import *\n","\n","else:  # GCP run\n","  # Put your bucket name below and make sure you can access it without an error\n","  bucket_name = '318419512_318510252' \n","  full_path = f\"gs://{318419512_318510252}/\"\n","  paths=[]\n","\n","  client = storage.Client()\n","  blobs = client.list_blobs(bucket_name)\n","  for b in blobs:\n","      if b.name != 'graphframes.sh':\n","          paths.append(full_path+b.name)\n","\n","\n","  parquetFile = spark.read.parquet(*paths)\n","  doc_title_text_anchor_quadruplets = parquetFile.select(\"title\", \"text\", \"anchor_text\", \"id\").rdd\n","  # if nothing prints here you forgot to upload the file inverted_index.py to the home dir\n","  %cd -q /home/dataproc\n","  !ls inverted_index.py\n","\n","  # adding our python module to the cluster\n","  sc.addFile(\"/home/dataproc/inverted_index.py\")\n","  sys.path.insert(0,SparkFiles.getRootDirectory())\n","\n","  from inverted_index import InvertedIndex\n","\n","title_w2df_dict, title_posting_locs_list = count_words_and_write_filtered_posting_list(doc_title_text_anchor_quadruplets, 'title')  # Title\n","body_w2df_dict, body_posting_locs_list = count_words_and_write_filtered_posting_list(doc_title_text_anchor_quadruplets, 'body')  # Body\n","anchor_w2df_dict, anchor_posting_locs_list = count_words_and_write_filtered_posting_list(doc_title_text_anchor_quadruplets, 'anchor')  # Anchor Text\n","\n","title_inverted = create_and_upload_inverted_index_instance(title_w2df_dict, title_posting_locs_list, 'title')\n","body_inverted = create_and_upload_inverted_index_instance(body_w2df_dict, body_posting_locs_list, 'body')\n","anchor_inverted = create_and_upload_inverted_index_instance(anchor_w2df_dict, anchor_posting_locs_list, 'anchor')\n","\n","pages_links = spark.read.parquet(\"gs://wikidata20210801_preprocessed/*\").select(\"id\", \"anchor_text\").rdd\n","\n","# construct the graph \n","edges, vertices = generate_graph(pages_links)\n","# compute PageRank\n","edgesDF = edges.toDF(['src', 'dst']).repartition(NUM_BUCKETS, 'src')\n","verticesDF = vertices.toDF(['id']).repartition(NUM_BUCKETS, 'id')\n","g = GraphFrame(verticesDF, edgesDF)\n","pr_results = g.pageRank(resetProbability=0.15, maxIter=6)\n","pr = pr_results.vertices.select(\"id\", \"pagerank\")\n","pr = pr.sort(col('pagerank').desc())\n","pr.repartition(1).write.csv(f'gs://{bucket_name}/pr', compression=\"gzip\")\n","pr.show()"],"metadata":{"id":"h8IJiX5wPDlA","colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"status":"error","timestamp":1673004909264,"user_tz":-120,"elapsed":778,"user":{"displayName":"Ido Paretsky","userId":"00866448096400107402"}},"outputId":"ee64dab3-ad75-4feb-bc66-043f59db7dbe"},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-a1ac35e49736>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;31m# Initializing spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m# create a spark context and session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.ui.port\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"4050\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.jars.packages\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"graphframes:graphframes:0.8.2-spark3.2-s_2.12\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'SparkConf' is not defined"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"rkCQMKS3PDKS"}}]}
>>>>>>> 8d9bac7409e3256dd08256656a651d63d5ca1248
