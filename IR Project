{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Information Retrieval Final Project\n","## Bar Dolev 318419512\n","## Ido Paretsky 318510252\n","\n","### https://github.com/IdoParetsky/Information-Retrieval-Final-Project\n","### https://drive.google.com/drive/u/0/folders/1uxQRHS3lpENpNbJqYE1I4ZWKsu0Nn1uk"],"metadata":{"id":"pBIYmR2y0jQK"}},{"cell_type":"markdown","source":["GitHub Repository init. from within Google Drive"],"metadata":{"id":"QijpDuYg0i0J"}},{"cell_type":"code","source":["\"\"\"\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/Information Retrieval/Project\n","!git init information-retrieval-final-project\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nZpg47mx0iSL","executionInfo":{"status":"ok","timestamp":1672082446810,"user_tz":-120,"elapsed":2333,"user":{"displayName":"Ido Paretsky","userId":"00866448096400107402"}},"outputId":"41d79ccc-e727-484a-b2f7-b350f5dae4be"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/Information Retrieval/Project\n","Reinitialized existing Git repository in /content/drive/MyDrive/Information Retrieval/Project/information-retrieval-final-project/.git/\n","/content/drive/MyDrive/Information Retrieval/Project/information-retrieval-final-project\n"]}]},{"cell_type":"markdown","source":["Commit Command"],"metadata":{"id":"n1sfpWma-Eut"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/Information Retrieval/Project/information-retrieval-final-project/\n","!git config --global user.email \"ido.paretsky@gmail.com\"\n","!git config --global user.name \"IdoParetsky\"\n","!git pull\n","!git add .\n","!git status\n","\n","!git commit -m \"Applied imports and MapReduce Arch\"\n","\n","USERNAME = \"IdoParetsky\"\n","REPOSITORY = \"information-retrieval-final-project\"\n","GIT_TOKEN = \"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\"\n","\n","!git remote add origin https://{GIT_TOKEN}@github.com/{USERNAME}/{REPOSITORY}.git\n","!git remote -v\n","!git push -u origin master"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X_v8hDGz1gnW","outputId":"9e839332-0fee-405a-e3ae-f5a6c382af6e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/Information Retrieval/Project/information-retrieval-final-project\n","On branch master\n","Your branch is ahead of 'origin/master' by 1 commit.\n","  (use \"git push\" to publish your local commits)\n","\n","Changes to be committed:\n","  (use \"git reset HEAD <file>...\" to unstage)\n","\n","\t\u001b[32mmodified:   IR Project\u001b[m\n","\n","[master 46191d6] Applied imports and MapReduce Arch\n"," 1 file changed, 1 insertion(+), 1 deletion(-)\n"," rewrite IR Project (91%)\n","fatal: remote origin already exists.\n","origin\thttps://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@github.com/IdoParetsky/information-retrieval-final-project.git (fetch)\n","origin\thttps://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@github.com/IdoParetsky/information-retrieval-final-project.git (push)\n","remote: Enumerating objects: 4, done.\u001b[K\n","remote: Counting objects: 100% (4/4), done.\u001b[K\n","remote: Compressing objects: 100% (2/2), done.\u001b[K\n","remote: Total 3 (delta 1), reused 0 (delta 0), pack-reused 0\u001b[K\n","Unpacking objects: 100% (3/3), done.\n","From https://github.com/IdoParetsky/information-retrieval-final-project\n","   28a16ef..a8d85d8  master     -> origin/master\n","<n-retrieval-final-project/.git/MERGE_MSG\" 7L, 319C\u001b[2;1Hâ–½\u001b[6n\u001b[2;1H  \u001b[1;1H\u001b[>c\u001b]10;?\u0007\u001b]11;?\u0007\u001b[1;1H\u001b[33mMerge branch 'master' of https://github.com/IdoPar\u001b[metsky/information-retrieval-fii\u001b[2;1Hnal-project\n","\n","\u001b[34m# Please enter a commit message to explain why this merge is necessary,\n","# especially if it merges an updated upstream into a topic branch.\n","#\n","# Lines starting with '#' will be ignored, and an empty message aborts\n","# the commit.\u001b[m\n","\u001b[1m\u001b[34m~                                                                               \u001b[10;1H~                                                                               \u001b[11;1H~                                                                               \u001b[12;1H~                                                                               \u001b[13;1H~                                                                               \u001b[14;1H~                                                                               \u001b[15;1H~                                                                               \u001b[16;1H~                                                                               \u001b[17;1H~                                                                               \u001b[18;1H~                                                                               \u001b[19;1H~                                                                               \u001b[20;1H~                                                                               \u001b[21;1H~                                                                               \u001b[22;1H~                                                                               \u001b[23;1H~                                                                               \u001b[m\u001b[24;63H1,1\u001b[11CAll\u001b[1;1H\u001b[?25h\u0007\u001b[?25l\u001b[33ms\u001b[m\u001b[78Cii\u001b[2;1Hn\u001b[1;1H\u001b[?25h\u001b[?25l\u001b[24;65H5\u001b[1;5H\u001b[?25h\u001b[?25l\u001b[24;63H3,1\u001b[4;1H\u001b[?25h\u001b[?25l\u001b[24;65H8\u001b[4;8H\u001b[?25h\u001b[?25l\u001b[34mg\b\u001b[?25h\u001b[?25l\u001b[m\u001b[24;65H14\u001b[4;14H\u001b[?25h\u001b[?25l\u001b[24;53H^@\u001b[4;14H\u001b[24;53H  \u001b[5;14H\u001b[24;63H4\u001b[5;14H\u001b[?25h\u001b[?25l\u001b[24;53H^@\u001b[5;14H\u001b[24;53H  \u001b[6;1H\u001b[24;63H5,1 \u001b[6;1H\u001b[?25h\u001b[?25l\u001b[24;1H\u001b[1m-- INSERT --\u001b[m\u001b[24;13H\u001b[K\u001b[24;63H5,2\u001b[11CAll\u001b[7;23r\u001b[7;1H\u001b[L\u001b[1;24r\u001b[24;63H\u001b[K\u001b[24;63H6,1\u001b[11CAll\u001b[7;1H\u001b[?25h"]}]},{"cell_type":"markdown","source":["### Imports"],"metadata":{"id":"6wl97Fl8czLX"}},{"cell_type":"code","source":["from functools import partial\n","from xml.etree import ElementTree\n","import csv\n","import gdown\n","\n","#%load_ext google.colab.data_table\n","import bz2\n","from collections import Counter, OrderedDict, defaultdict\n","import heapq\n","import codecs\n","import os\n","from operator import itemgetter\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from pathlib import Path\n","from time import time\n","import hashlib\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","import nltk\n","nltk.download('stopwords')\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import pandas as pd\n","from sklearn.metrics.pairwise import cosine_similarity\n","import re\n","import pickle\n","import numpy as np\n","\n","!gcloud dataproc clusters list --region us-central1\n","!pip install -q google-cloud-storage==1.43.0\n","!pip install -q graphframes\n","!pip install -q pyspark\n","!pip install -U -q PyDrive\n","!apt install openjdk-8-jdk-headless -qq\n","!pip install -q graphframes\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","graphframes_jar = 'https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.2-s_2.12/graphframes-0.8.2-spark3.2-s_2.12.jar'\n","spark_jars = '/usr/local/lib/python3.7/dist-packages/pyspark/jars'\n","!wget -N -P $spark_jars $graphframes_jar\n","import pyspark\n","from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","from graphframes import *\n","\n","import sys\n","from itertools import islice, count, groupby\n","from time import time\n","from google.cloud import storage\n","\n","from tqdm import tqdm\n","from contextlib import closing\n","\n","import json\n","from io import StringIO\n","\n","!gcloud dataproc clusters list --region us-central1\n","!ls -l /usr/lib/spark/jars/graph*"],"metadata":{"id":"7LQSHUUicyup"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Colab Debugging Chunk"],"metadata":{"id":"1uwkAM2enkU-"}},{"cell_type":"code","source":["# Initializing spark context\n","# create a spark context and session\n","conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n","conf.set(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.2-s_2.12\")\n","sc = pyspark.SparkContext(conf=conf)\n","sc.addPyFile(str(Path(spark_jars) / Path(graphframes_jar).name))\n","spark = SparkSession.builder.getOrCreate()\n","\n","\n","# Authenticate your user\n","# The authentication should be done with the email connected to your GCP account\n","from google.colab import auth\n","import signal\n","\n","AUTH_TIMEOUT = 60\n","\n","def handler(signum, frame):\n","  raise Exception(\"Authentication timeout!\")\n","\n","signal.signal(signal.SIGALRM, handler)\n","signal.alarm(AUTH_TIMEOUT)\n","\n","try:\n","   auth.authenticate_user()\n","except: \n","   pass\n","\n","\n","# Copy one wikidumps files \n","import os\n","from pathlib import Path\n","from google.colab import auth\n","## RENAME the project_id to yours project id from the project you created in GCP \n","project_id = 'crypto-lexicon-370515'\n","!gcloud config set project {project_id}\n","\n","data_bucket_name = 'wikidata_preprocessed'\n","try:\n","    if os.environ[\"wikidata_preprocessed\"] is not None:\n","        pass  \n","except:\n","      !mkdir wikidumps\n","      !gsutil -u {project_id} cp gs://{data_bucket_name}/multistream1_preprocessed.parquet \"wikidumps/\" \n","\n","\n","try:\n","    if os.environ[\"wikidata_preprocessed\"] is not None:\n","      path = os.environ[\"wikidata_preprocessed\"]+\"/wikidumps/*\"\n","except:\n","      path = \"wikidumps/*\"\n","\n","parquetFile = spark.read.parquet(path)\n","# take the 'text' and 'id' or the first 1000 rows and create an RDD from it\n","doc_text_pairs = parquetFile.limit(1000).select(\"text\", \"id\").rdd\n","\n","\n","english_stopwords = frozenset(stopwords.words('english'))\n","corpus_stopwords = ['category', 'references', 'also', 'links', 'extenal', 'see', 'thumb']\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","\n","def word_count(text, id):\n","  ''' Count the frequency of each word in `text` (tf) that is not included in \n","  `all_stopwords` and return entries that will go into our posting lists. \n","  Parameters:\n","  -----------\n","    text: str\n","      Text of one document\n","    id: int\n","      Document id\n","  Returns:\n","  --------\n","    List of tuples\n","      A list of (token, (doc_id, tf)) pairs \n","      for example: [(\"Anarchism\", (12, 5)), ...]\n","  '''\n","  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","  # YOUR CODE HERE\n","  tokens = [t for t in tokens if t not in all_stopwords]\n","  word_counts = Counter()\n","  for w in tokens:\n","    count = word_counts.get(w, None)\n","    if not count:\n","        word_counts[w] = (id, 1)\n","    else:\n","      word_counts[w] = (id, word_counts.get(w)[1] + 1)\n","  return list(word_counts.items())\n","\n","word_counts = doc_text_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n","\n","\n","def reduce_word_counts(unsorted_pl):\n","  ''' Returns a sorted posting list by wiki_id.\n","  Parameters:\n","  -----------\n","    unsorted_pl: list of tuples\n","      A list of (wiki_id, tf) tuples \n","  Returns:\n","  --------\n","    list of tuples\n","      A sorted posting list.\n","  '''\n","  # YOUR CODE HERE\n","  return sorted(unsorted_pl, key=lambda t: t[0])\n","\n","postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n","\n","\n","postings_filtered = postings.filter(lambda x: len(x[1])>10)\n","\n","\n","def calculate_df(postings):\n","  ''' Takes a posting list RDD and calculate the df for each token.\n","  Parameters:\n","  -----------\n","    postings: RDD\n","      An RDD where each element is a (token, posting_list) pair.\n","  Returns:\n","  --------\n","    RDD\n","      An RDD where each element is a (token, df) pair.\n","  '''\n","  # YOUR CODE HERE\n","  return postings.map(lambda t: (t[0], len(t[1])))\n","\n","w2df = calculate_df(postings_filtered)\n","w2df_dict = w2df.collectAsMap()\n","\n","NUM_BUCKETS = 248\n","def token2bucket_id(token):\n","  return int(_hash(token),16) % NUM_BUCKETS\n","\n","def partition_postings_and_write(postings):\n","  ''' A function that partitions the posting lists into buckets, writes out \n","  all posting lists in a bucket to disk, and returns the posting locations for \n","  each bucket. Partitioning should be done through the use of `token2bucket` \n","  above. Writing to disk should use the function  `write_a_posting_list`, a \n","  static method implemented in inverted_index_colab.py under the InvertedIndex \n","  class. \n","  Parameters:\n","  -----------\n","    postings: RDD\n","      An RDD where each item is a (w, posting_list) pair.\n","  Returns:\n","  --------\n","    RDD\n","      An RDD where each item is a posting locations dictionary for a bucket. The\n","      posting locations maintain a list for each word of file locations and \n","      offsets its posting list was written to. See `write_a_posting_list` for \n","      more details.\n","  '''\n","  # YOUR CODE HERE\n","  bucket_id_posting_locs_tup_list = []\n","  b_w_pl_rdd = postings.map(lambda w_pl: (token2bucket_id(w_pl[0]), [w_pl])).reduceByKey(lambda a, b: a + b)\n","  return b_w_pl_rdd.map(lambda b_w_pl: InvertedIndex.write_a_posting_list(b_w_pl))\n","\n","posting_locs_list = partition_postings_and_write(postings_filtered).collect()\n","\n","super_posting_locs = defaultdict(list)\n","for posting_loc in posting_locs_list:\n","  for k, v in posting_loc.items():\n","    super_posting_locs[k].extend(v)\n","\n","inverted = InvertedIndex()\n","inverted.posting_locs = super_posting_locs\n","inverted.df = w2df_dict\n","inverted.write_index('.', 'index')\n","\n","\n","TUPLE_SIZE = 6       \n","TF_MASK = 2 ** 16 - 1 # Masking the 16 low bits of an integer\n","from contextlib import closing\n","\n","def read_posting_list(inverted, w):\n","  with closing(MultiFileReader()) as reader:\n","    locs = inverted.posting_locs[w]\n","    b = reader.read(locs, inverted.df[w] * TUPLE_SIZE)\n","    posting_list = []\n","    for i in range(inverted.df[w]):\n","      doc_id = int.from_bytes(b[i*TUPLE_SIZE:i*TUPLE_SIZE+4], 'big')\n","      tf = int.from_bytes(b[i*TUPLE_SIZE+4:(i+1)*TUPLE_SIZE], 'big')\n","      posting_list.append((doc_id, tf))\n","    return posting_list\n","\n","\n","pages_links = spark.read.parquet(path).limit(1000).select(\"id\", \"anchor_text\").rdd\n","\n","def generate_graph(pages):\n","  ''' Compute the directed graph generated by wiki links.\n","  Parameters:\n","  -----------\n","    pages: RDD\n","      An RDD where each row consists of one wikipedia articles with 'id' and \n","      'anchor_text'.\n","  Returns:\n","  --------\n","    edges: RDD\n","      An RDD where each row represents an edge in the directed graph created by\n","      the wikipedia links. The first entry should the source page id and the \n","      second entry is the destination page id. No duplicates should be present. \n","    vertices: RDD\n","      An RDD where each row represents a vetrix (node) in the directed graph \n","      created by the wikipedia links. No duplicates should be present. \n","  '''\n","  # YOUR CODE HERE\n","  articles_to_link_lists = pages.mapValues(lambda anchor_text: [link[0] for link in anchor_text])\n","  edges = articles_to_link_lists.flatMap(lambda article_to_link_list: [(article_to_link_list[0], link) for link in article_to_link_list[1]]).distinct()\n","  vertices = edges.flatMap(lambda edge: edge).map(lambda v: (v,)).distinct()\n","  return edges, vertices\n","\n","edges, vertices = generate_graph(pages_links)\n","\n","edgesDF = edges.toDF(['src', 'dst']).repartition(4, 'src')\n","verticesDF = vertices.toDF(['id']).repartition(4, 'id')\n","g = GraphFrame(verticesDF, edgesDF)\n","pr_results = g.pageRank(resetProbability=0.15, maxIter=10)\n","pr = pr_results.vertices.select(\"id\", \"pagerank\")\n","pr = pr.sort(col('pagerank').desc())\n","pr.repartition(1).write.csv('pr', compression=\"gzip\")\n","pr.show()\n","\n","#If you have decided to do the bonus task - please copy the code here \n","\n","bonus_flag = True # Turn flag on (True) if you have implemented this part\n","\n","t_start = time()\n","\n","# PLACE YOUR CODE HERE\n","def divide_rank(v, links, rank):\n","    ranks = [(v, 0)]  # Vertex Rank sent to itself\n","    if len(links) > 0:\n","        rank_sent = rank / len(links)\n","        for l in links:\n","            ranks.append((l, rank_sent))\n","    return ranks\n","\n","def PageRank(resetProbability=0.15, maxIter=10):\n","    page_cnt = pages_links.count()\n","\n","    pr = pages_links.mapValues(lambda weight: 1 / page_cnt)  # Initial PageRank of 1 divided by page_cnt\n","\n","    for i in range(maxIter):  # Instead of until convergence, compute MaxIter times\n","        weighted_pages = pages_links.join(pr).flatMap(lambda title_links_pr: \n","                                                           divide_rank(title_links_pr[0], title_links_pr[1][0], title_links_pr[1][1]))\n","\n","        # Sum rank sent to each node\n","        pr = weighted_pages.reduceByKey(lambda r1, r2: r1 + r2) \\\n","            .mapValues(lambda agg_rank: (resetProbability / page_cnt) + (agg_rank * (1 - resetProbability)))\n","\n","    return pr.sortBy(lambda p: p[1], ascending=False)\n","\n","pr = PageRank(resetProbability=0.15, maxIter=10)\n","pr.show()\n","\n","pr_time_Bonus = time() - t_start\n"],"metadata":{"id":"9om5ZhuunjgH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### GCP"],"metadata":{"id":"4BeQXPCn1Dkg"}},{"cell_type":"code","source":["# Put your bucket name below and make sure you can access it without an error\n","bucket_name = '318419512_318510252' \n","full_path = f\"gs://{318419512_318510252}/\"\n","paths=[]\n","\n","client = storage.Client()\n","blobs = client.list_blobs(bucket_name)\n","for b in blobs:\n","    if b.name != 'graphframes.sh':\n","        paths.append(full_path+b.name)\n","\n","\n","parquetFile = spark.read.parquet(*paths)\n","doc_text_pairs = parquetFile.select(\"text\", \"id\").rdd\n","\n","# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n","%cd -q /home/dataproc\n","!ls inverted_index_gcp.py\n","\n","# adding our python module to the cluster\n","sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n","sys.path.insert(0,SparkFiles.getRootDirectory())\n","\n","from inverted_index_gcp import InvertedIndex\n","\n","\n","english_stopwords = frozenset(stopwords.words('english'))\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n","                    \"many\", \"however\", \"would\", \"became\"]\n","\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","\n","NUM_BUCKETS = 124  # TODO: Test Project with higher NUM_BUCKETS\n","def token2bucket_id(token):\n","  return int(_hash(token),16) % NUM_BUCKETS\n","\n","# PLACE YOUR CODE HERE\n","def word_count(text, id):\n","  ''' Count the frequency of each word in `text` (tf) that is not included in \n","  `all_stopwords` and return entries that will go into our posting lists. \n","  Parameters:\n","  -----------\n","    text: str\n","      Text of one document\n","    id: int\n","      Document id\n","  Returns:\n","  --------\n","    List of tuples\n","      A list of (token, (doc_id, tf)) pairs \n","      for example: [(\"Anarchism\", (12, 5)), ...]\n","  '''\n","  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","  # YOUR CODE HERE\n","  tokens = [t for t in tokens if t not in all_stopwords]\n","  word_counts = Counter()\n","  for w in tokens:\n","    count = word_counts.get(w, None)\n","    if not count:\n","        word_counts[w] = (id, 1)\n","    else:\n","      word_counts[w] = (id, word_counts.get(w)[1] + 1)\n","  return list(word_counts.items())\n","\n","def reduce_word_counts(unsorted_pl):\n","  ''' Returns a sorted posting list by wiki_id.\n","  Parameters:\n","  -----------\n","    unsorted_pl: list of tuples\n","      A list of (wiki_id, tf) tuples \n","  Returns:\n","  --------\n","    list of tuples\n","      A sorted posting list.\n","  '''\n","  # YOUR CODE HERE\n","  return sorted(unsorted_pl, key=lambda t: t[0]) \n","\n","def calculate_df(postings):\n","  ''' Takes a posting list RDD and calculate the df for each token.\n","  Parameters:\n","  -----------\n","    postings: RDD\n","      An RDD where each element is a (token, posting_list) pair.\n","  Returns:\n","  --------\n","    RDD\n","      An RDD where each element is a (token, df) pair.\n","  '''\n","  # YOUR CODE HERE\n","  return postings.map(lambda t: (t[0], len(t[1])))\n","\n","def partition_postings_and_write(postings):\n","  ''' A function that partitions the posting lists into buckets, writes out \n","  all posting lists in a bucket to disk, and returns the posting locations for \n","  each bucket. Partitioning should be done through the use of `token2bucket` \n","  above. Writing to disk should use the function  `write_a_posting_list`, a \n","  static method implemented in inverted_index_colab.py under the InvertedIndex \n","  class. \n","  Parameters:\n","  -----------\n","    postings: RDD\n","      An RDD where each item is a (w, posting_list) pair.\n","  Returns:\n","  --------\n","    RDD\n","      An RDD where each item is a posting locations dictionary for a bucket. The\n","      posting locations maintain a list for each word of file locations and \n","      offsets its posting list was written to. See `write_a_posting_list` for \n","      more details.\n","  '''\n","  # YOUR CODE HERE\n","  bucket_id_posting_locs_tup_list = []\n","  b_w_pl_rdd = postings.map(lambda w_pl: (token2bucket_id(w_pl[0]), [w_pl])).reduceByKey(lambda a, b: a + b)\n","  return b_w_pl_rdd.map(lambda b_w_pl: InvertedIndex.write_a_posting_list(b_w_pl, '318419512_318510252'))\n","\n","\n","word_counts = doc_text_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n","postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n","postings_filtered = postings.filter(lambda x: len(x[1])>50)\n","w2df = calculate_df(postings_filtered)\n","w2df_dict = w2df.collectAsMap()\n","_ = partition_postings_and_write(postings_filtered).collect()\n","\n","\n","# collect all posting lists locations into one super-set\n","super_posting_locs = defaultdict(list)\n","for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n","  if not blob.name.endswith(\"pickle\"):\n","    continue\n","  with blob.open(\"rb\") as f:\n","    posting_locs = pickle.load(f)\n","    for k, v in posting_locs.items():\n","      super_posting_locs[k].extend(v)\n","\n","\n","# Create inverted index instance\n","inverted = InvertedIndex()\n","# Adding the posting locations dictionary to the inverted index\n","inverted.posting_locs = super_posting_locs\n","# Add the token - df dictionary to the inverted index\n","inverted.df = w2df_dict\n","# write the global stats out\n","inverted.write_index('.', 'index')\n","# upload to gs\n","index_src = \"index.pkl\"\n","index_dst = f'gs://{bucket_name}/postings_gcp/{index_src}'\n","!gsutil cp $index_src $index_dst\n","\n","!gsutil ls -lh $index_dst\n","\n","\n","\n","def generate_graph(pages):\n","  ''' Compute the directed graph generated by wiki links.\n","  Parameters:\n","  -----------\n","    pages: RDD\n","      An RDD where each row consists of one wikipedia articles with 'id' and \n","      'anchor_text'.\n","  Returns:\n","  --------\n","    edges: RDD\n","      An RDD where each row represents an edge in the directed graph created by\n","      the wikipedia links. The first entry should the source page id and the \n","      second entry is the destination page id. No duplicates should be present. \n","    vertices: RDD\n","      An RDD where each row represents a vetrix (node) in the directed graph \n","      created by the wikipedia links. No duplicates should be present. \n","  '''\n","  # YOUR CODE HERE\n","  articles_to_link_lists = pages.mapValues(lambda anchor_text: [link[0] for link in anchor_text])\n","  edges = articles_to_link_lists.flatMap(lambda article_to_link_list: [(article_to_link_list[0], link) for link in article_to_link_list[1]]).distinct()\n","  vertices = edges.flatMap(lambda edge: edge).map(lambda v: (v,)).distinct()\n","  return edges, vertices\n","\n","pages_links = spark.read.parquet(\"gs://wikidata_preprocessed/*\").select(\"id\", \"anchor_text\").rdd\n","\n","# construct the graph \n","edges, vertices = generate_graph(pages_links)\n","# compute PageRank\n","edgesDF = edges.toDF(['src', 'dst']).repartition(124, 'src')\n","verticesDF = vertices.toDF(['id']).repartition(124, 'id')\n","g = GraphFrame(verticesDF, edgesDF)\n","pr_results = g.pageRank(resetProbability=0.15, maxIter=6)\n","pr = pr_results.vertices.select(\"id\", \"pagerank\")\n","pr = pr.sort(col('pagerank').desc())\n","pr.repartition(1).write.csv(f'gs://{bucket_name}/pr', compression=\"gzip\")\n","pr.show()\n","\n","# PLACE YOUR CODE HERE\n","def divide_rank(v, links, rank):\n","    ranks = [(v, 0)]  # Vertex Rank sent to itself\n","    if len(links) > 0:\n","        rank_sent = rank / len(links)\n","        for l in links:\n","            ranks.append((l, rank_sent))\n","    return ranks\n","\n","def PageRank(resetProbability=0.15, maxIter=10):\n","    page_cnt = pages_links.count()\n","\n","    pr = pages_links.mapValues(lambda weight: 1 / page_cnt)  # Initial PageRank of 1 divided by page_cnt\n","\n","    for i in range(maxIter):  # Instead of until convergence, compute MaxIter times\n","        weighted_pages = pages_links.join(pr).flatMap(lambda title_links_pr: \n","                                                           divide_rank(title_links_pr[0], title_links_pr[1][0], title_links_pr[1][1]))\n","\n","        # Sum rank sent to each node\n","        pr = weighted_pages.reduceByKey(lambda r1, r2: r1 + r2) \\\n","            .mapValues(lambda agg_rank: (resetProbability / page_cnt) + (agg_rank * (1 - resetProbability)))\n","\n","    return pr.sortBy(lambda p: p[1], ascending=False)\n","\n","pr = PageRank(resetProbability=0.15, maxIter=10)\n","pr.show()"],"metadata":{"id":"3a2SxV06xAmM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Ranking"],"metadata":{"id":"TsPEAk-X5GRE"}},{"cell_type":"code","source":[],"metadata":{"id":"KEVb3zCuclP4"},"execution_count":null,"outputs":[]}]}