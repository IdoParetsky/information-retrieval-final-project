{"cells":[{"cell_type":"markdown","metadata":{"id":"pBIYmR2y0jQK"},"source":["# Information Retrieval Final Project\n","## Bar Dolev 318419512\n","## Ido Paretsky 318510252\n","\n","### https://github.com/IdoParetsky/information-retrieval-final-project\n","### https://drive.google.com/drive/u/0/folders/1LUf_YLUbEo4Qj1CTqvKgqYiMgHy9Ejgf"]},{"cell_type":"markdown","metadata":{"id":"QijpDuYg0i0J"},"source":["### GitHub Repository init. from within Google Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"elapsed":435,"status":"ok","timestamp":1673004939384,"user":{"displayName":"Ido Paretsky","userId":"00866448096400107402"},"user_tz":-120},"id":"nZpg47mx0iSL","outputId":"9c38bc54-783b-4f55-fbbc-4341c1bf36c0"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\\n%cd /content/drive/MyDrive/Information Retrieval/Project\\n!git init information-retrieval-final-project\\n\""]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/Information Retrieval/Project\n","!git init information-retrieval-final-project\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"n1sfpWma-Eut"},"source":["Commit Command"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X_v8hDGz1gnW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673630279485,"user_tz":-120,"elapsed":18907,"user":{"displayName":"Ido Paretsky","userId":"00866448096400107402"}},"outputId":"dcbeb5c6-0ec6-4634-eb01-61372f9ede6c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/Information Retrieval/Project/information-retrieval-final-project\n","remote: Enumerating objects: 6, done.\u001b[K\n","remote: Counting objects: 100% (6/6), done.\u001b[K\n","remote: Compressing objects: 100% (4/4), done.\u001b[K\n","remote: Total 4 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\n","Unpacking objects: 100% (4/4), done.\n","From https://github.com/IdoParetsky/information-retrieval-final-project\n","   e475cbe..d041733  master     -> origin/master\n","Updating e475cbe..d041733\n","Fast-forward\n"," gcp_cache/link to gcp_pagerank_pageviews.csv.gz.txt | 1 \u001b[32m+\u001b[m\n"," 1 file changed, 1 insertion(+)\n"," create mode 100644 gcp_cache/link to gcp_pagerank_pageviews.csv.gz.txt\n","[master 5eec292] Removed filtering from title and anchor, applying casting to id_title_pageranl_pageviews_dict, finalized search_frontend - weights gridsearch pending\n"," 259 files changed, 899 insertions(+), 274 deletions(-)\n"," rewrite IR_Project (96%)\n"," rewrite colab_cache/__pycache__/colab_search_frontend.cpython-38.pyc (70%)\n"," create mode 100644 colab_cache/colab_bins/0_000.bin\n"," create mode 100644 colab_cache/colab_bins/100_000.bin\n"," create mode 100644 colab_cache/colab_bins/101_000.bin\n"," create mode 100644 colab_cache/colab_bins/102_000.bin\n"," create mode 100644 colab_cache/colab_bins/103_000.bin\n"," create mode 100644 colab_cache/colab_bins/104_000.bin\n"," create mode 100644 colab_cache/colab_bins/105_000.bin\n"," create mode 100644 colab_cache/colab_bins/106_000.bin\n"," create mode 100644 colab_cache/colab_bins/108_000.bin\n"," create mode 100644 colab_cache/colab_bins/109_000.bin\n"," create mode 100644 colab_cache/colab_bins/10_000.bin\n"," create mode 100644 colab_cache/colab_bins/110_000.bin\n"," create mode 100644 colab_cache/colab_bins/111_000.bin\n"," create mode 100644 colab_cache/colab_bins/112_000.bin\n"," create mode 100644 colab_cache/colab_bins/113_000.bin\n"," create mode 100644 colab_cache/colab_bins/114_000.bin\n"," create mode 100644 colab_cache/colab_bins/116_000.bin\n"," create mode 100644 colab_cache/colab_bins/117_000.bin\n"," create mode 100644 colab_cache/colab_bins/118_000.bin\n"," create mode 100644 colab_cache/colab_bins/119_000.bin\n"," create mode 100644 colab_cache/colab_bins/11_000.bin\n"," create mode 100644 colab_cache/colab_bins/120_000.bin\n"," create mode 100644 colab_cache/colab_bins/121_000.bin\n"," create mode 100644 colab_cache/colab_bins/122_000.bin\n"," create mode 100644 colab_cache/colab_bins/123_000.bin\n"," create mode 100644 colab_cache/colab_bins/12_000.bin\n"," create mode 100644 colab_cache/colab_bins/13_000.bin\n"," create mode 100644 colab_cache/colab_bins/14_000.bin\n"," create mode 100644 colab_cache/colab_bins/15_000.bin\n"," create mode 100644 colab_cache/colab_bins/16_000.bin\n"," create mode 100644 colab_cache/colab_bins/17_000.bin\n"," create mode 100644 colab_cache/colab_bins/18_000.bin\n"," create mode 100644 colab_cache/colab_bins/19_000.bin\n"," create mode 100644 colab_cache/colab_bins/1_000.bin\n"," create mode 100644 colab_cache/colab_bins/20_000.bin\n"," create mode 100644 colab_cache/colab_bins/21_000.bin\n"," create mode 100644 colab_cache/colab_bins/22_000.bin\n"," create mode 100644 colab_cache/colab_bins/23_000.bin\n"," rewrite colab_cache/colab_bins/248_000.bin (72%)\n"," rewrite colab_cache/colab_bins/249_000.bin (89%)\n"," create mode 100644 colab_cache/colab_bins/24_000.bin\n"," rewrite colab_cache/colab_bins/250_000.bin (82%)\n"," rewrite colab_cache/colab_bins/251_000.bin (82%)\n"," rewrite colab_cache/colab_bins/252_000.bin (77%)\n"," rewrite colab_cache/colab_bins/253_000.bin (99%)\n"," rewrite colab_cache/colab_bins/254_000.bin (76%)\n"," rewrite colab_cache/colab_bins/255_000.bin (89%)\n"," rewrite colab_cache/colab_bins/256_000.bin (97%)\n"," rewrite colab_cache/colab_bins/257_000.bin (100%)\n"," rewrite colab_cache/colab_bins/258_000.bin (80%)\n"," rewrite colab_cache/colab_bins/259_000.bin (84%)\n"," create mode 100644 colab_cache/colab_bins/25_000.bin\n"," rewrite colab_cache/colab_bins/260_000.bin (85%)\n"," rewrite colab_cache/colab_bins/261_000.bin (85%)\n"," rewrite colab_cache/colab_bins/262_000.bin (64%)\n"," rewrite colab_cache/colab_bins/263_000.bin (94%)\n"," rewrite colab_cache/colab_bins/265_000.bin (93%)\n"," rewrite colab_cache/colab_bins/266_000.bin (81%)\n"," rewrite colab_cache/colab_bins/267_000.bin (86%)\n"," rewrite colab_cache/colab_bins/268_000.bin (65%)\n"," rewrite colab_cache/colab_bins/269_000.bin (77%)\n"," create mode 100644 colab_cache/colab_bins/26_000.bin\n"," rewrite colab_cache/colab_bins/270_000.bin (89%)\n"," rewrite colab_cache/colab_bins/271_000.bin (82%)\n"," rewrite colab_cache/colab_bins/272_000.bin (98%)\n"," rewrite colab_cache/colab_bins/273_000.bin (65%)\n"," rewrite colab_cache/colab_bins/274_000.bin (76%)\n"," rewrite colab_cache/colab_bins/275_000.bin (98%)\n"," rewrite colab_cache/colab_bins/276_000.bin (92%)\n"," rewrite colab_cache/colab_bins/277_000.bin (90%)\n"," rewrite colab_cache/colab_bins/278_000.bin (84%)\n"," rewrite colab_cache/colab_bins/279_000.bin (91%)\n"," create mode 100644 colab_cache/colab_bins/27_000.bin\n"," rewrite colab_cache/colab_bins/280_000.bin (87%)\n"," rewrite colab_cache/colab_bins/281_000.bin (76%)\n"," rewrite colab_cache/colab_bins/282_000.bin (95%)\n"," rewrite colab_cache/colab_bins/283_000.bin (77%)\n"," rewrite colab_cache/colab_bins/284_000.bin (93%)\n"," rewrite colab_cache/colab_bins/285_000.bin (76%)\n"," rewrite colab_cache/colab_bins/286_000.bin (80%)\n"," rewrite colab_cache/colab_bins/287_000.bin (73%)\n"," rewrite colab_cache/colab_bins/289_000.bin (84%)\n"," create mode 100644 colab_cache/colab_bins/28_000.bin\n"," rewrite colab_cache/colab_bins/290_000.bin (88%)\n"," rewrite colab_cache/colab_bins/291_000.bin (90%)\n"," rewrite colab_cache/colab_bins/292_000.bin (80%)\n"," rewrite colab_cache/colab_bins/293_000.bin (90%)\n"," rewrite colab_cache/colab_bins/294_000.bin (97%)\n"," rewrite colab_cache/colab_bins/295_000.bin (96%)\n"," rewrite colab_cache/colab_bins/296_000.bin (96%)\n"," rewrite colab_cache/colab_bins/297_000.bin (82%)\n"," rewrite colab_cache/colab_bins/298_000.bin (92%)\n"," rewrite colab_cache/colab_bins/299_000.bin (83%)\n"," create mode 100644 colab_cache/colab_bins/29_000.bin\n"," create mode 100644 colab_cache/colab_bins/2_000.bin\n"," rewrite colab_cache/colab_bins/300_000.bin (69%)\n"," rewrite colab_cache/colab_bins/301_000.bin (61%)\n"," rewrite colab_cache/colab_bins/302_000.bin (75%)\n"," rewrite colab_cache/colab_bins/304_000.bin (64%)\n"," rewrite colab_cache/colab_bins/305_000.bin (79%)\n"," rewrite colab_cache/colab_bins/306_000.bin (82%)\n"," rewrite colab_cache/colab_bins/307_000.bin (78%)\n"," rewrite colab_cache/colab_bins/308_000.bin (82%)\n"," rewrite colab_cache/colab_bins/309_000.bin (90%)\n"," create mode 100644 colab_cache/colab_bins/30_000.bin\n"," rewrite colab_cache/colab_bins/310_000.bin (75%)\n"," rewrite colab_cache/colab_bins/312_000.bin (79%)\n"," rewrite colab_cache/colab_bins/313_000.bin (88%)\n"," rewrite colab_cache/colab_bins/314_000.bin (96%)\n"," rewrite colab_cache/colab_bins/315_000.bin (77%)\n"," rewrite colab_cache/colab_bins/316_000.bin (68%)\n"," rewrite colab_cache/colab_bins/317_000.bin (87%)\n"," rewrite colab_cache/colab_bins/318_000.bin (84%)\n"," rewrite colab_cache/colab_bins/319_000.bin (73%)\n"," create mode 100644 colab_cache/colab_bins/31_000.bin\n"," rewrite colab_cache/colab_bins/321_000.bin (93%)\n"," rewrite colab_cache/colab_bins/322_000.bin (95%)\n"," rewrite colab_cache/colab_bins/324_000.bin (84%)\n"," rewrite colab_cache/colab_bins/325_000.bin (81%)\n"," rewrite colab_cache/colab_bins/326_000.bin (100%)\n"," rewrite colab_cache/colab_bins/327_000.bin (96%)\n"," rewrite colab_cache/colab_bins/328_000.bin (85%)\n"," rewrite colab_cache/colab_bins/329_000.bin (82%)\n"," create mode 100644 colab_cache/colab_bins/32_000.bin\n"," rewrite colab_cache/colab_bins/330_000.bin (92%)\n"," rewrite colab_cache/colab_bins/331_000.bin (67%)\n"," rewrite colab_cache/colab_bins/332_000.bin (87%)\n"," rewrite colab_cache/colab_bins/333_000.bin (95%)\n"," rewrite colab_cache/colab_bins/334_000.bin (93%)\n"," rewrite colab_cache/colab_bins/335_000.bin (91%)\n"," rewrite colab_cache/colab_bins/336_000.bin (83%)\n"," rewrite colab_cache/colab_bins/337_000.bin (76%)\n"," rewrite colab_cache/colab_bins/338_000.bin (81%)\n"," rewrite colab_cache/colab_bins/339_000.bin (66%)\n"," create mode 100644 colab_cache/colab_bins/33_000.bin\n"," rewrite colab_cache/colab_bins/340_000.bin (98%)\n"," rewrite colab_cache/colab_bins/341_000.bin (79%)\n"," rewrite colab_cache/colab_bins/342_000.bin (80%)\n"," rewrite colab_cache/colab_bins/343_000.bin (68%)\n"," rewrite colab_cache/colab_bins/344_000.bin (93%)\n"," rewrite colab_cache/colab_bins/345_000.bin (96%)\n"," rewrite colab_cache/colab_bins/346_000.bin (90%)\n"," rewrite colab_cache/colab_bins/347_000.bin (90%)\n"," rewrite colab_cache/colab_bins/348_000.bin (92%)\n"," rewrite colab_cache/colab_bins/349_000.bin (98%)\n"," create mode 100644 colab_cache/colab_bins/34_000.bin\n"," rewrite colab_cache/colab_bins/350_000.bin (95%)\n"," rewrite colab_cache/colab_bins/351_000.bin (68%)\n"," rewrite colab_cache/colab_bins/352_000.bin (73%)\n"," rewrite colab_cache/colab_bins/353_000.bin (89%)\n"," rewrite colab_cache/colab_bins/354_000.bin (95%)\n"," rewrite colab_cache/colab_bins/356_000.bin (90%)\n"," rewrite colab_cache/colab_bins/357_000.bin (100%)\n"," rewrite colab_cache/colab_bins/358_000.bin (79%)\n"," rewrite colab_cache/colab_bins/359_000.bin (87%)\n"," create mode 100644 colab_cache/colab_bins/35_000.bin\n"," rewrite colab_cache/colab_bins/360_000.bin (73%)\n"," rewrite colab_cache/colab_bins/361_000.bin (84%)\n"," rewrite colab_cache/colab_bins/362_000.bin (84%)\n"," rewrite colab_cache/colab_bins/363_000.bin (69%)\n"," rewrite colab_cache/colab_bins/364_000.bin (93%)\n"," rewrite colab_cache/colab_bins/365_000.bin (87%)\n"," rewrite colab_cache/colab_bins/366_000.bin (92%)\n"," rewrite colab_cache/colab_bins/367_000.bin (68%)\n"," rewrite colab_cache/colab_bins/368_000.bin (96%)\n"," rewrite colab_cache/colab_bins/369_000.bin (81%)\n"," create mode 100644 colab_cache/colab_bins/36_000.bin\n"," rewrite colab_cache/colab_bins/370_000.bin (97%)\n"," rewrite colab_cache/colab_bins/371_000.bin (68%)\n"," create mode 100644 colab_cache/colab_bins/37_000.bin\n"," create mode 100644 colab_cache/colab_bins/39_000.bin\n"," create mode 100644 colab_cache/colab_bins/3_000.bin\n"," create mode 100644 colab_cache/colab_bins/41_000.bin\n"," create mode 100644 colab_cache/colab_bins/42_000.bin\n"," create mode 100644 colab_cache/colab_bins/43_000.bin\n"," create mode 100644 colab_cache/colab_bins/44_000.bin\n"," create mode 100644 colab_cache/colab_bins/46_000.bin\n"," create mode 100644 colab_cache/colab_bins/47_000.bin\n"," create mode 100644 colab_cache/colab_bins/48_000.bin\n"," create mode 100644 colab_cache/colab_bins/49_000.bin\n"," create mode 100644 colab_cache/colab_bins/4_000.bin\n"," create mode 100644 colab_cache/colab_bins/50_000.bin\n"," create mode 100644 colab_cache/colab_bins/51_000.bin\n"," create mode 100644 colab_cache/colab_bins/52_000.bin\n"," create mode 100644 colab_cache/colab_bins/53_000.bin\n"," create mode 100644 colab_cache/colab_bins/54_000.bin\n"," create mode 100644 colab_cache/colab_bins/56_000.bin\n"," create mode 100644 colab_cache/colab_bins/57_000.bin\n"," create mode 100644 colab_cache/colab_bins/58_000.bin\n"," create mode 100644 colab_cache/colab_bins/59_000.bin\n"," create mode 100644 colab_cache/colab_bins/5_000.bin\n"," create mode 100644 colab_cache/colab_bins/60_000.bin\n"," create mode 100644 colab_cache/colab_bins/61_000.bin\n"," create mode 100644 colab_cache/colab_bins/62_000.bin\n"," create mode 100644 colab_cache/colab_bins/63_000.bin\n"," create mode 100644 colab_cache/colab_bins/64_000.bin\n"," create mode 100644 colab_cache/colab_bins/65_000.bin\n"," create mode 100644 colab_cache/colab_bins/66_000.bin\n"," create mode 100644 colab_cache/colab_bins/67_000.bin\n"," create mode 100644 colab_cache/colab_bins/68_000.bin\n"," create mode 100644 colab_cache/colab_bins/69_000.bin\n"," create mode 100644 colab_cache/colab_bins/6_000.bin\n"," create mode 100644 colab_cache/colab_bins/70_000.bin\n"," create mode 100644 colab_cache/colab_bins/71_000.bin\n"," create mode 100644 colab_cache/colab_bins/72_000.bin\n"," create mode 100644 colab_cache/colab_bins/73_000.bin\n"," create mode 100644 colab_cache/colab_bins/74_000.bin\n"," create mode 100644 colab_cache/colab_bins/75_000.bin\n"," create mode 100644 colab_cache/colab_bins/76_000.bin\n"," create mode 100644 colab_cache/colab_bins/77_000.bin\n"," create mode 100644 colab_cache/colab_bins/78_000.bin\n"," create mode 100644 colab_cache/colab_bins/79_000.bin\n"," create mode 100644 colab_cache/colab_bins/7_000.bin\n"," create mode 100644 colab_cache/colab_bins/81_000.bin\n"," create mode 100644 colab_cache/colab_bins/82_000.bin\n"," create mode 100644 colab_cache/colab_bins/83_000.bin\n"," create mode 100644 colab_cache/colab_bins/84_000.bin\n"," create mode 100644 colab_cache/colab_bins/85_000.bin\n"," create mode 100644 colab_cache/colab_bins/86_000.bin\n"," create mode 100644 colab_cache/colab_bins/87_000.bin\n"," create mode 100644 colab_cache/colab_bins/88_000.bin\n"," create mode 100644 colab_cache/colab_bins/89_000.bin\n"," create mode 100644 colab_cache/colab_bins/8_000.bin\n"," create mode 100644 colab_cache/colab_bins/90_000.bin\n"," create mode 100644 colab_cache/colab_bins/91_000.bin\n"," create mode 100644 colab_cache/colab_bins/92_000.bin\n"," create mode 100644 colab_cache/colab_bins/93_000.bin\n"," create mode 100644 colab_cache/colab_bins/94_000.bin\n"," create mode 100644 colab_cache/colab_bins/95_000.bin\n"," create mode 100644 colab_cache/colab_bins/96_000.bin\n"," create mode 100644 colab_cache/colab_bins/97_000.bin\n"," create mode 100644 colab_cache/colab_bins/99_000.bin\n"," rewrite colab_cache/colab_indexes/anchor_index.pkl (62%)\n"," create mode 100644 colab_cache/id_title_pr_pv_dict_cast.pkl\n"," rewrite colab_cache/run_frontend_in_colab.ipynb (99%)\n"," create mode 100644 gcp_cache/ngrok\n"," create mode 100644 gcp_cache/ngrok-stable-linux-amd64.zip\n"," create mode 100644 gcp_cache/search_frontend.py\n","fatal: remote origin already exists.\n","origin\thttps://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@github.com/IdoParetsky/information-retrieval-final-project.git (fetch)\n","origin\thttps://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@github.com/IdoParetsky/information-retrieval-final-project.git (push)\n","Counting objects: 264, done.\n","Delta compression using up to 2 threads.\n","Compressing objects: 100% (238/238), done.\n","Writing objects: 100% (264/264), 1.77 MiB | 2.20 MiB/s, done.\n","Total 264 (delta 31), reused 0 (delta 0)\n","remote: Resolving deltas: 100% (31/31), completed with 30 local objects.\u001b[K\n","To https://github.com/IdoParetsky/information-retrieval-final-project.git\n","   d041733..5eec292  master -> master\n","Branch 'master' set up to track remote branch 'master' from 'origin'.\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/Information Retrieval/Project/information-retrieval-final-project/\n","!git config --global user.email \"ido.paretsky@gmail.com\"\n","!git config --global user.name \"IdoParetsky\"\n","!git pull\n","#!git add IR_Project\n","#!git status\n","\n","!git commit -m \"Removed filtering from title and anchor, applying casting to id_title_pageranl_pageviews_dict, finalized search_frontend - weights gridsearch pending\"\n","\n","USERNAME = \"IdoParetsky\"\n","REPOSITORY = \"information-retrieval-final-project\"\n","GIT_TOKEN = \"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\"\n","\n","!git remote add origin https://{GIT_TOKEN}@github.com/{USERNAME}/{REPOSITORY}.git\n","!git remote -v\n","!git push -u origin master"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/Information Retrieval/Project/information-retrieval-final-project/\n","!git config --global user.email \"ido.paretsky@gmail.com\"\n","!git config --global user.name \"IdoParetsky\"\n","#!git add --all -- :!wikidumps/ :!gcp_cache/postings_gcp.zip :!gcp_cache/gcp_pagerank_pageviews.csv.gz  :!gcp_cache/gcp_pagerank_pageviews.csv.gz :!gcp_cache/id_title_pr_pv_dict_cast.pkl :!gcp_cache/gcp_indexes/anchor_index.pkl :!gcp_cache/gcp_indexes/body_index.pkl :!gcp_cache/gcp_indexes/title_index.pkl # Exclude giant files\n","!git status"],"metadata":{"id":"WK4U1yse1zjQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673701040875,"user_tz":-120,"elapsed":65800,"user":{"displayName":"Ido Paretsky","userId":"00866448096400107402"}},"outputId":"1cb23cc3-a273-4512-ac74-77a325ad11af"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/Information Retrieval/Project/information-retrieval-final-project\n","On branch master\n","Your branch is up to date with 'origin/master'.\n","\n","Changes not staged for commit:\n","  (use \"git add/rm <file>...\" to update what will be committed)\n","  (use \"git checkout -- <file>...\" to discard changes in working directory)\n","\n","\t\u001b[31mmodified:   IR_Project\u001b[m\n","\t\u001b[31mmodified:   colab_cache/run_frontend_in_colab.ipynb\u001b[m\n","\t\u001b[31mdeleted:    gcp_cache/gcp_indexes/anchor_index.pkl\u001b[m\n","\t\u001b[31mdeleted:    gcp_cache/gcp_indexes/body_index.pkl\u001b[m\n","\t\u001b[31mdeleted:    gcp_cache/gcp_indexes/title_index.pkl\u001b[m\n","\t\u001b[31mmodified:   gcp_cache/inverted_index_gcp.py\u001b[m\n","\t\u001b[31mmodified:   gcp_cache/run_frontend_in_gcp.sh\u001b[m\n","\t\u001b[31mmodified:   gcp_cache/search_frontend.py\u001b[m\n","\n","Untracked files:\n","  (use \"git add <file>...\" to include in what will be committed)\n","\n","\t\u001b[31mgcp_cache/gcp_pagerank_pageviews.csv.gz\u001b[m\n","\t\u001b[31mgcp_cache/id_title_pr_pv_dict_cast.pkl\u001b[m\n","\t\u001b[31mnew_train.json\u001b[m\n","\t\u001b[31mwikidumps/\u001b[m\n","\n","no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"]}]},{"cell_type":"markdown","metadata":{"id":"6wl97Fl8czLX"},"source":["### Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7LQSHUUicyup","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673552545745,"user_tz":-120,"elapsed":84693,"user":{"displayName":"Ido Paretsky","userId":"00866448096400107402"}},"outputId":"0e91ed94-90ca-449e-9530-cbf0eb9be7ad"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1;31mERROR:\u001b[0m (gcloud.dataproc.clusters.list) The required property [project] is not currently set.\n","It can be set on a per-command basis by re-running your command with the [--project] flag.\n","\n","You may set it for your current workspace by running:\n","\n","  $ gcloud config set project VALUE\n","\n","or it can be set temporarily by the environment variable [CLOUDSDK_CORE_PROJECT]\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.6/106.6 KB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 KB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'apt autoremove' to remove it.\n","The following additional packages will be installed:\n","  openjdk-8-jre-headless\n","Suggested packages:\n","  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra\n","  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n","  fonts-wqy-zenhei fonts-indic\n","The following NEW packages will be installed:\n","  openjdk-8-jdk-headless openjdk-8-jre-headless\n","0 upgraded, 2 newly installed, 0 to remove and 21 not upgraded.\n","Need to get 36.6 MB of archives.\n","After this operation, 143 MB of additional disk space will be used.\n","Selecting previously unselected package openjdk-8-jre-headless:amd64.\n","(Reading database ... 124016 files and directories currently installed.)\n","Preparing to unpack .../openjdk-8-jre-headless_8u352-ga-1~18.04_amd64.deb ...\n","Unpacking openjdk-8-jre-headless:amd64 (8u352-ga-1~18.04) ...\n","Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n","Preparing to unpack .../openjdk-8-jdk-headless_8u352-ga-1~18.04_amd64.deb ...\n","Unpacking openjdk-8-jdk-headless:amd64 (8u352-ga-1~18.04) ...\n","Setting up openjdk-8-jre-headless:amd64 (8u352-ga-1~18.04) ...\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n","Setting up openjdk-8-jdk-headless:amd64 (8u352-ga-1~18.04) ...\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n","--2023-01-12 19:42:21--  https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.2-s_2.12/graphframes-0.8.2-spark3.2-s_2.12.jar\n","Resolving repos.spark-packages.org (repos.spark-packages.org)... 108.156.83.37, 108.156.83.15, 108.156.83.116, ...\n","Connecting to repos.spark-packages.org (repos.spark-packages.org)|108.156.83.37|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 247880 (242K) [binary/octet-stream]\n","Saving to: ‘/usr/local/lib/python3.7/dist-packages/pyspark/jars/graphframes-0.8.2-spark3.2-s_2.12.jar’\n","\n","graphframes-0.8.2-s 100%[===================>] 242.07K  --.-KB/s    in 0.06s   \n","\n","2023-01-12 19:42:21 (4.07 MB/s) - ‘/usr/local/lib/python3.7/dist-packages/pyspark/jars/graphframes-0.8.2-spark3.2-s_2.12.jar’ saved [247880/247880]\n","\n","\u001b[1;31mERROR:\u001b[0m (gcloud.dataproc.clusters.list) The required property [project] is not currently set.\n","It can be set on a per-command basis by re-running your command with the [--project] flag.\n","\n","You may set it for your current workspace by running:\n","\n","  $ gcloud config set project VALUE\n","\n","or it can be set temporarily by the environment variable [CLOUDSDK_CORE_PROJECT]\n","ls: cannot access '/usr/lib/spark/jars/graph*': No such file or directory\n"]}],"source":["from functools import partial\n","from xml.etree import ElementTree\n","import csv\n","#import gdown\n","import math\n","\n","#%load_ext google.colab.data_table\n","import bz2\n","from collections import Counter, OrderedDict, defaultdict\n","import heapq\n","import codecs\n","import os\n","from operator import itemgetter\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from pathlib import Path\n","from time import time\n","import hashlib\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","import nltk\n","nltk.download('stopwords')\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import pandas as pd\n","from sklearn.metrics.pairwise import cosine_similarity\n","import re\n","import pickle\n","import numpy as np\n","\n","!gcloud dataproc clusters list --region us-central1\n","!pip install -q google-cloud-storage==1.43.0\n","!pip install -q graphframes\n","!pip install -q pyspark\n","!pip install -U -q PyDrive\n","!apt install openjdk-8-jdk-headless -qq\n","!pip install -q graphframes\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","graphframes_jar = 'https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.2-s_2.12/graphframes-0.8.2-spark3.2-s_2.12.jar'\n","spark_jars = '/usr/local/lib/python3.7/dist-packages/pyspark/jars'\n","!wget -N -P $spark_jars $graphframes_jar\n","import pyspark\n","from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","import pyspark.sql.functions as f\n","from graphframes import *\n","\n","import sys\n","from itertools import islice, count, groupby, chain\n","from time import time\n","from google.cloud import storage\n","\n","from tqdm import tqdm\n","from contextlib import closing\n","\n","import json\n","from io import StringIO\n","\n","!gcloud dataproc clusters list --region us-central1\n","!ls -l /usr/lib/spark/jars/graph*"]},{"cell_type":"markdown","metadata":{"id":"4BeQXPCn1Dkg"},"source":["### Utilities"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":182},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1673552545746,"user":{"displayName":"Ido Paretsky","userId":"00866448096400107402"},"user_tz":-120},"id":"3a2SxV06xAmM","outputId":"59a23956-25e0-42aa-b8b5-cace034620c6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ndef divide_rank(v, links, rank):\\n    ranks = [(v, 0)]  # Vertex Rank sent to itself\\n    if len(links) > 0:\\n        rank_sent = rank / len(links)\\n        for l in links:\\n            ranks.append((l, rank_sent))\\n    return ranks\\n\\ndef PageRank(resetProbability=0.15, maxIter=10):\\n    page_cnt = pages_links.count()\\n\\n    pr = pages_links.mapValues(lambda weight: 1 / page_cnt)  # Initial PageRank of 1 divided by page_cnt\\n\\n    for i in range(maxIter):  # Instead of until convergence, compute MaxIter times\\n        weighted_pages = pages_links.join(pr).flatMap(lambda title_links_pr: \\n                                                           divide_rank(title_links_pr[0], title_links_pr[1][0], title_links_pr[1][1]))\\n\\n        # Sum rank sent to each node\\n        pr = weighted_pages.reduceByKey(lambda r1, r2: r1 + r2)             .mapValues(lambda agg_rank: (resetProbability / page_cnt) + (agg_rank * (1 - resetProbability)))\\n\\n    return pr.sortBy(lambda p: p[1], ascending=False)\\n\\npr = PageRank(resetProbability=0.15, maxIter=10)\\npr.show()\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}],"source":["english_stopwords = frozenset(stopwords.words('english'))\n","# We queried ChatGPT for Wikipedia-specific StopWords and added some of our own\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n","                    \"first\", \"see\", \"people\", \"one\", \"two\", \n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n","                    \"many\", \"however\", \"would\", \"became\", \"page\", \"article\",\n","                    \"reference\", \"source\", \"content\",\n","                    \"fact\", \"year\", \"date\", \"place\", \"wiki\",\n","                    \"edit\", \"version\", \"user\", \"talk\", \"discussion\", \"template\",\n","                    \"category\", \"project\", \"author\", \"writer\",\n","                    \"creator\", \"publisher\", \"editor\", \"publication\", \"edition\",\n","                    \"issue\", \"chapter\"]\n","\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","\n","group_to_idx = {'title': 0, 'body': 1, 'anchor': 2, 'id': 3}\n","\n","NUM_BUCKETS = 124  \n","def token2bucket_id(token, group):\n","  return int(_hash(token),16) % NUM_BUCKETS + (NUM_BUCKETS * group_to_idx[group])  # group: 0 - 'title', 1 - 'body', 2 - 'anchor'\n","\n","# PLACE YOUR CODE HERE\n","def word_count(group, id):\n","  ''' Count the frequency of each word in group (tf of title, text or anchor_text) that is not included in \n","  `all_stopwords` and return entries that will go into our posting lists. \n","  Parameters:\n","  -----------\n","    group: str\n","      Title, Text or Anchor Text of one document\n","    id: int\n","      Document id\n","  Returns:\n","  --------\n","    List of tuples\n","      A list of (token, (doc_id, tf)) pairs \n","      for example: [(\"Anarchism\", (12, 5)), ...]\n","  '''\n","  if type(group) != str:\n","    group = ''.join([f'{x + \" \" if type(x)==str else \"\"}{y + \" \" if type(y) ==str else \"\"}' for x, y in group])\n","  tokens = [token.group() for token in RE_WORD.finditer(group.lower())]\n","  tokens = [t for t in tokens if t not in all_stopwords]\n","  word_counts = Counter()\n","  for w in tokens:\n","    count = word_counts.get(w, None)\n","    if not count:\n","        word_counts[w] = (id, 1)\n","    else:\n","      word_counts[w] = (id, word_counts.get(w)[1] + 1)\n","  return list(word_counts.items())\n","\n","def calculate_dl(group, id):\n","  ''' Calculate each document's length\n","  Parameters:\n","  -----------\n","    group: str\n","      Title, Text or Anchor Text of one document\n","    id: int\n","      Document id\n","  Returns:\n","  --------\n","    List of tuples\n","      A list of (doc_id, doc_len) pairs \n","      for example: [(12, 50), ...]\n","  '''\n","  if type(group) != str:\n","    group = ''.join([f'{x + \" \" if type(x)==str else \"\"}{y + \" \" if type(y) ==str else \"\"}' for x, y in group])\n","  tokens = [token.group() for token in RE_WORD.finditer(group.lower())]\n","  tokens = [t for t in tokens if t not in all_stopwords]\n","  return [(id, (len(tokens)))]\n"," \n","def reduce_word_counts(unsorted_pl):\n","  ''' Returns a sorted posting list by wiki_id.\n","  Parameters:\n","  -----------\n","    unsorted_pl: list of tuples\n","      A list of (wiki_id, tf) tuples \n","  Returns:\n","  --------\n","    list of tuples\n","      A sorted posting list.\n","  '''\n","  # YOUR CODE HERE\n","  return sorted(unsorted_pl, key=lambda t: t[0]) \n","\n","def calculate_df(postings):\n","  ''' Takes a posting list RDD and calculate the df for each token.\n","  Parameters:\n","  -----------\n","    postings: RDD\n","      An RDD where each element is a (token, posting_list) pair.\n","  Returns:\n","  --------\n","    RDD\n","      An RDD where each element is a (token, df) pair.\n","  '''\n","  # YOUR CODE HERE\n","  return postings.map(lambda t: (t[0], len(t[1])))\n","\n","def partition_postings_and_write(postings, group):\n","  ''' A function that partitions the posting lists into buckets, writes out \n","  all posting lists in a bucket to disk, and returns the posting locations for \n","  each bucket. Partitioning should be done through the use of `token2bucket` \n","  above. Writing to disk should use the function  `write_a_posting_list`, a \n","  static method implemented in inverted_index_colab/gcp.py under the InvertedIndex \n","  class. \n","  Parameters:\n","  -----------\n","    postings: RDD\n","      An RDD where each item is a (w, posting_list) pair.\n","    \n","    group: string \n","      'title', 'body' or 'anchor'\n","\n","  Returns:\n","  --------\n","    RDD\n","      An RDD where each item is a posting locations dictionary for a bucket. The\n","      posting locations maintain a list for each word of file locations and \n","      offsets its posting list was written to. See `write_a_posting_list_volab/gcp` for \n","      more details.\n","  '''\n","  # YOUR CODE HERE\n","  bucket_id_posting_locs_tup_list = []\n","  b_w_pl_rdd = postings.map(lambda w_pl: (token2bucket_id(w_pl[0], group), [w_pl])).reduceByKey(lambda a, b: a + b)\n","  if not is_gcp:\n","    return b_w_pl_rdd.map(lambda b_w_pl: InvertedIndex.write_a_posting_list(b_w_pl))\n","  else:\n","    return b_w_pl_rdd.map(lambda b_w_pl: InvertedIndex.write_a_posting_list(b_w_pl, '318419512_318510252'))\n","\n","def count_words_and_write_filtered_posting_list(doc_title_text_anchor_quadruplets, group):\n","  \"\"\"\n","    Binds together the usage of word_count, reduce_word_counts, posting list filtering, calculate_df,\n","    mapping w2df into a dictionary and writing the posting list to prevent code duplication, as these\n","    operations to be applied thrice - on Title, Body and Anchor Text.\n","    \n","    Parameters:\n","    -----------\n","    group: string \n","      'title', 'body' or 'anchor'. ('id' is used in every iteration)\n","    \n","    Returns:\n","    -----------\n","    2-Tuple of (w2df_dict, posting_locs_list)\n","  \"\"\"\n","  word_counts = doc_title_text_anchor_quadruplets.flatMap(lambda x: word_count(x[group_to_idx[group]], x[group_to_idx['id']]))\n","  DL_dict = doc_title_text_anchor_quadruplets.flatMap(lambda x: calculate_dl(x[group_to_idx[group]], x[group_to_idx['id']])).collectAsMap()\n","  postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n","  postings_filtered = postings.filter(lambda x: len(x[1])>(10 if not is_gcp else 50)) if group == 'body' else postings\n","  w2df = calculate_df(postings_filtered)\n","  w2df_dict = w2df.collectAsMap()\n","  posting_locs_list = partition_postings_and_write(postings_filtered, group).collect() if not is_gcp else []\n","  if is_gcp:\n","    _ = partition_postings_and_write(postings_filtered, group).collect()  # .pkl file saved to bucket\n","\n","  return w2df_dict, posting_locs_list, DL_dict\n","\n","def create_and_upload_inverted_index_instance(w2df_dict, posting_locs_list, DL_dict, group):\n","  \"\"\"\n","    Collects all posting lists locations into one super-set and create an inverted index instance\n","    \n","    Parameters:\n","    -----------\n","    w2df: dict \n","      Word to Document Frequency Dicationary\n","\n","    posting_locs_list: list\n","      List of posting lists' locations\n","\n","    DL: RDD\n","      RDD of doc_id and doc_len\n","\n","    group: str\n","      'title', 'body' or 'anchor'\n","    \n","    Returns:\n","    -----------\n","    inverted_index.InvertedIndex instance\n","  \"\"\"\n","  # collect all posting lists locations into one super-set\n","  if not is_gcp:  # colab\n","    super_posting_locs = defaultdict(list)\n","    for posting_loc in posting_locs_list:\n","      for k, v in posting_loc.items():\n","        super_posting_locs[k].extend(v)\n","  else:  # GCP\n","    super_posting_locs = defaultdict(list)\n","    for blob in client.list_blobs(bucket_name, prefix='postings_gcp_fixed'):\n","      if not blob.name.endswith(\"pickle\"):\n","        continue\n","      with blob.open(\"rb\") as f:\n","        posting_locs = pickle.load(f)\n","        for k, v in posting_locs.items():\n","          super_posting_locs[k].extend(v)\n","\n","  # Create inverted index instance\n","  inverted = InvertedIndex()\n","  # Adding the posting locations dictionary to the inverted index\n","  inverted.posting_locs = super_posting_locs\n","  # Add the token - df dictionary to the inverted index\n","  inverted.df = w2df_dict\n","  inverted.DL = DL_dict\n","  # write the global stats out\n","  inverted.write_index(f'.', f'{group}_index')\n","  if is_gcp:\n","  # upload to gs\n","    index_src = f\"{group}_index.pkl\"\n","    index_dst = f'gs://{bucket_name}/postings_gcp_fixed/{index_src}'\n","    !gsutil cp $index_src $index_dst\n","    !gsutil ls -lh $index_dst\n","  return inverted\n","\n","def generate_graph(pages):\n","  ''' Compute the directed graph generated by wiki links.\n","  Parameters:\n","  -----------\n","    pages: RDD\n","      An RDD where each row consists of one wikipedia articles with 'id' and \n","      'anchor_text'.\n","  Returns:\n","  --------\n","    edges: RDD\n","      An RDD where each row represents an edge in the directed graph created by\n","      the wikipedia links. The first entry should the source page id and the \n","      second entry is the destination page id. No duplicates should be present. \n","    vertices: RDD\n","      An RDD where each row represents a vetrix (node) in the directed graph \n","      created by the wikipedia links. No duplicates should be present. \n","  '''\n","  # YOUR CODE HERE\n","  articles_to_link_lists = pages.mapValues(lambda anchor_text: [link[0] for link in anchor_text])\n","  edges = articles_to_link_lists.flatMap(lambda article_to_link_list: [(article_to_link_list[0], link) for link in article_to_link_list[1]]).distinct()\n","  vertices = edges.flatMap(lambda edge: edge).map(lambda v: (v,)).distinct()\n","  return edges, vertices\n","\n","# Self-made PageRanks, to be tested against the given GraphFrame implementation\n","\"\"\"\n","def divide_rank(v, links, rank):\n","    ranks = [(v, 0)]  # Vertex Rank sent to itself\n","    if len(links) > 0:\n","        rank_sent = rank / len(links)\n","        for l in links:\n","            ranks.append((l, rank_sent))\n","    return ranks\n","\n","def PageRank(resetProbability=0.15, maxIter=10):\n","    page_cnt = pages_links.count()\n","\n","    pr = pages_links.mapValues(lambda weight: 1 / page_cnt)  # Initial PageRank of 1 divided by page_cnt\n","\n","    for i in range(maxIter):  # Instead of until convergence, compute MaxIter times\n","        weighted_pages = pages_links.join(pr).flatMap(lambda title_links_pr: \n","                                                           divide_rank(title_links_pr[0], title_links_pr[1][0], title_links_pr[1][1]))\n","\n","        # Sum rank sent to each node\n","        pr = weighted_pages.reduceByKey(lambda r1, r2: r1 + r2) \\\n","            .mapValues(lambda agg_rank: (resetProbability / page_cnt) + (agg_rank * (1 - resetProbability)))\n","\n","    return pr.sortBy(lambda p: p[1], ascending=False)\n","\n","pr = PageRank(resetProbability=0.15, maxIter=10)\n","pr.show()\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"TsPEAk-X5GRE"},"source":["### Ranking"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KEVb3zCuclP4","colab":{"base_uri":"https://localhost:8080/","height":246},"executionInfo":{"status":"error","timestamp":1673116408772,"user_tz":-120,"elapsed":1070,"user":{"displayName":"Bar Lazar Dolev","userId":"11475429273908169423"}},"outputId":"e6f04f4c-5e24-4253-d65b-a9f3be0ebb78"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-4bc459697f3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m            \u001b[0mtf_idf_vectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mdf_tfidfvect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidfvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_idf_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#queries_vector = tfidfvectorizer.transform(queries)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"]}],"source":["def tf_idf_scores(data):\n","    \"\"\"\n","    This function calculates the tfidf for each word in a single document utilizing TfidfVectorizer via sklearn.\n","\n","    Parameters:\n","    -----------\n","      data: list of strings.  # TODO: Rewrite for GCP-scale data using PySpark MapReduce Arch!\n","    \n","    Returns:\n","    --------\n","      Two objects as follows:\n","                                a) DataFrame, documents as rows (i.e., 0,1,2,3, etc'), terms as columns ('bird','bright', etc').\n","                                b) TfidfVectorizer object.\n","\n","    \"\"\"\n","    # YOUR CODE HERE\n","    tf_idf_vectorizer = TfidfVectorizer(stop_words=\"english\")\n","    return pd.DataFrame(tf_idf_vectorizer.fit_transform(data).todense().tolist(), \n","                        columns=tf_idf_vectorizer.get_feature_names_out()), \\\n","           tf_idf_vectorizer\n","    \n","df_tfidfvect, tfidfvectorizer = tf_idf_scores(data)\n","\n","#queries_vector = tfidfvectorizer.transform(queries)\n","\n","\n","def cosine_sim_using_sklearn(queries,tfidf):\n","    \"\"\"\n","    In this function you need to utilize the cosine_similarity function from sklearn.\n","    You need to compute the similarity between the queries and the given documents.\n","    This function will return a DataFrame in the following shape: (# of queries, # of documents).\n","    Each value in the DataFrame will represent the cosine_similarity between given query and document.\n","    \n","    Parameters:\n","    -----------\n","      queries: sparse matrix represent the queries after transformation of tfidfvectorizer.\n","      documents: sparse matrix represent the documents.  # TODO: Rewrite for GCP-scale data using PySpark MapReduce Arch!\n","      \n","    Returns:\n","    --------\n","      DataFrame: This function will return a DataFrame in the following shape: (# of queries, # of documents).\n","      Each value in the DataFrame will represent the cosine_similarity between given query and document.\n","    \"\"\"\n","    # YOUR CODE HERE\n","    return pd.DataFrame(cosine_similarity(X=queries, Y=tfidf))\n","\n","#cosine_sim_df = cosine_sim_using_sklearn(queries_vector,df_tfidfvect)\n","\n","\n","def tokenize(text):\n","    \"\"\"\n","    This function aims in tokenize a text into a list of tokens. Moreover, it filter stopwords.\n","    \n","    Parameters:\n","    -----------\n","    text: string , represting the text to tokenize.    \n","    \n","    Returns:\n","    -----------\n","    list of tokens (e.g., list of tokens).\n","    \"\"\"\n","    list_of_tokens =  [token.group() for token in RE_WORD.finditer(text.lower()) if token.group() not in all_stopwords]    \n","    return list_of_tokens\n","\n","\n","#clean_data = [tokenize(doc) for doc in data]\n","\n","\n","# When preprocessing the data have a dictionary of document length for each document saved in a variable called `DL`.\n","class BM25_from_index:\n","    \"\"\"\n","    Best Match 25.    \n","    ----------\n","    DL : Document Length Dictionary {doc id: doc_length} \n","    k1 : float, default 1.5\n","\n","    b : float, default 0.75\n","\n","    index: inverted index\n","    \"\"\"\n","\n","    def __init__(self,index, DL, k1=1.5, b=0.75):\n","        self.b = b\n","        self.k1 = k1\n","        self.index = index\n","        self.N = len(DL)\n","        self.AVGDL = sum(DL.values())/self.N\n","        self.words, self.pls = zip(*self.index.posting_lists_iter())        \n","\n","    def calc_idf(self,list_of_tokens):\n","        \"\"\"\n","        This function calculate the idf values according to the BM25 idf formula for each term in the query.\n","        \n","        Parameters:\n","        -----------\n","        query: list of token representing the query. For example: ['look', 'blue', 'sky']\n","        \n","        Returns:\n","        -----------\n","        idf: dictionary of idf scores. As follows: \n","                                                    key: term\n","                                                    value: bm25 idf score\n","        \"\"\"        \n","        idf = {}        \n","        for term in list_of_tokens:            \n","            if term in self.index.df.keys():\n","                n_ti = self.index.df[term]\n","                idf[term] = math.log(1 + (self.N - n_ti + 0.5) / (n_ti + 0.5))\n","            else:\n","                pass                             \n","        return idf\n","        \n","\n","    def search(self, queries,N=3):\n","        \"\"\"\n","        This function calculate the bm25 score for given query and document.\n","        We need to check only documents which are 'candidates' for a given query. \n","        This function return a dictionary of scores as the following:\n","                                                                    key: query_id\n","                                                                    value: a ranked list of pairs (doc_id, score) in the length of N.\n","        \n","        Parameters:\n","        -----------\n","        query: list of token representing the query. For example: ['look', 'blue', 'sky']\n","        doc_id: integer, document id.\n","        \n","        Returns:\n","        -----------\n","        score: float, bm25 score.\n","        \"\"\"\n","        # YOUR CODE HERE\n","        return {i: sorted(list(set([(doc_id, round(self._score(q, doc_id), 5)) for \n","                                    doc_id, score in get_candidate_documents_and_scores(q, self.index, self.words, self.pls)])), \n","                          key=lambda doc_id_score: doc_id_score[1], reverse=True)[:N] \n","                   for i, q in queries.items()}\n","\n","    def _score(self, query, doc_id):\n","        \"\"\"\n","        This function calculate the bm25 score for given query and document.\n","        \n","        Parameters:\n","        -----------\n","        query: list of token representing the query. For example: ['look', 'blue', 'sky']\n","        doc_id: integer, document id.\n","        \n","        Returns:\n","        -----------\n","        score: float, bm25 score.\n","        \"\"\"        \n","        score = 0.0        \n","        doc_len = DL[str(doc_id)]        \n","        self.idf = self.calc_idf(query)\n","\n","        for term in query:\n","            if term in self.index.term_total.keys():                \n","                term_frequencies = dict(self.pls[self.words.index(term)])                \n","                if doc_id in term_frequencies.keys():            \n","                    freq = term_frequencies[doc_id]\n","                    numerator = self.idf[term] * freq * (self.k1 + 1)\n","                    denominator = freq + self.k1 * (1 - self.b + self.b * doc_len / self.AVGDL)\n","                    score += (numerator / denominator)\n","        return score\n","\n","\n","def top_N_documents(df,N):\n","    \"\"\"\n","    This function sort and filter the top N docuemnts (by score) for each query.\n","    \n","    Parameters\n","    ----------    \n","    df: DataFrame (queries as rows, documents as columns)  # TODO: Rewrite for GCP-scale data using PySpark MapReduce Arch!\n","    N: Integer (how many document to retrieve for each query)    \n","\n","    Returns:\n","    ----------\n","    top_N: dictionary is the following stracture:\n","          key - query id.\n","          value - sorted (according to score) list of pairs lengh of N. Eac pair within the list provide the following information (doc id, score)\n","    \"\"\"    \n","    # YOUR CODE HERE\n","    return {i: list(row.sort_values(ascending=False)[:N].items()) for i, row in df.iterrows()}\n","\n","# top_10_docs = top_N_documents(cosine_sim_df,10)\n","\n","\n","# All cran data shall be replaced with the whole wiki dump via (for example):\n","\n","#parquetFile = spark.read.parquet(*paths)\n","#doc_text_pairs = parquetFile.select(\"text\", \"id\").rdd\n","\n","# word_counts = doc_text_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n","# postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n","# postings_filtered = postings.filter(lambda x: len(x[1])>50)\n","# w2df = calculate_df(postings_filtered)\n","# w2df_dict = w2df.collectAsMap()\n","# _ = partition_postings_and_write(postings_filtered).collect()\n","\n","# pages_links = spark.read.parquet(\"gs://wikidata20210801_preprocessed/*\").select(\"id\", \"anchor_text\").rdd\n","\n","# create directories for the different indices \n","# !mkdir body_index title_index anchor_index\n","# title_inverted.write_index('title_index','title')\n","# body_inverted.write_index('body_index','body')\n","# anchor_inverted.write_index('anchor_index','anchor')\n","\n","#idx_title = InvertedIndex.read_index('title_index', 'title')\n","#idx_body = InvertedIndex.read_index('body_index', 'body')\n","#idx_anchor = InvertedIndex.read_index('anchor_index', 'anchor')\n","## read posting lists from disk - unfit for GCP-scale data!\n","#title_words, title_pls = zip(*idx_title.posting_lists_iter())\n","#body_words, body_pls = zip(*idx_body.posting_lists_iter())\n","#anchor_words, anchor_pls = zip(*idx_anchor.posting_lists_iter())\n","\n","# bm25_title = BM25_from_index(idx_title, title_inverted.DL)\n","# bm25_body = BM25_from_index(idx_body, body_inverted.DL)\n","# bm25_anchor = BM25_from_index(idx_anchor, anchor_inverted.DL)\n","# bm25_queries_score_train_title = bm25_title.search(cran_txt_query_text_train)\n","# bm25_queries_score_train_body = bm25_body.search(cran_txt_query_text_train)\n","# bm25_queries_score_train_anchor = bm25_anchor.search(cran_txt_query_text_train)\n","\n","\n","def generate_query_tfidf_vector(query_to_search,index):\n","    \"\"\" \n","    Generate a vector representing the query. Each entry within this vector represents a tfidf score.\n","    The terms representing the query will be the unique terms in the index.\n","\n","    We will use tfidf on the query as well. \n","    For calculation of IDF, use log with base 10.\n","    tf will be normalized based on the length of the query.    \n","\n","    Parameters:\n","    -----------\n","    query_to_search: list of tokens (str). This list will be preprocessed in advance (e.g., lower case, filtering stopwords, etc.'). \n","                     Example: 'Hello, I love information retrival' --->  ['hello','love','information','retrieval']\n","\n","    index:           inverted index loaded from the corresponding files.    \n","    \n","    Returns:\n","    -----------\n","    vectorized query with tfidf scores\n","    \"\"\"\n","    \n","    epsilon = .0000001\n","    total_vocab_size = len(index.term_total)\n","    Q = np.zeros((total_vocab_size))\n","    term_vector = list(index.term_total.keys())    \n","    counter = Counter(query_to_search)\n","    for token in np.unique(query_to_search):\n","        if token in index.term_total.keys(): #avoid terms that do not appear in the index.               \n","            tf = counter[token]/len(query_to_search) # term frequency divded by the length of the query\n","            df = index.df[token]            \n","            idf = math.log((len(DL))/(df+epsilon),10) #smoothing\n","            \n","            try:\n","                ind = term_vector.index(token)\n","                Q[ind] = tf*idf                    \n","            except:\n","                pass\n","    return Q\n","\n","def get_posting_iter(index):\n","    \"\"\"\n","    This function returning the iterator working with posting list.\n","    \n","    Parameters:\n","    ----------\n","    index: inverted index    \n","    \"\"\"\n","    words, pls = zip(*index.posting_lists_iter())\n","    return words,pls\n","\n","\n","def get_candidate_documents_and_scores(query_to_search,index,words,pls):\n","    \"\"\"\n","    Generate a dictionary representing a pool of candidate documents for a given query. This function will go through every token in query_to_search\n","    and fetch the corresponding information (e.g., term frequency, document frequency, etc.') needed to calculate TF-IDF from the posting list.\n","    Then it will populate the dictionary 'candidates.'\n","    For calculation of IDF, use log with base 10.\n","    tf will be normalized based on the length of the document.\n","    \n","    Parameters:\n","    -----------\n","    query_to_search: list of tokens (str). This list will be preprocessed in advance (e.g., lower case, filtering stopwords, etc.'). \n","                     Example: 'Hello, I love information retrival' --->  ['hello','love','information','retrieval']\n","\n","    index:           inverted index loaded from the corresponding files.\n","\n","    words,pls: iterator for working with posting.\n","    \n","    Returns:\n","    -----------\n","    dictionary of candidates. In the following format:\n","                                                               key: pair (doc_id,term)\n","                                                               value: tfidf score. \n","    \"\"\"\n","    candidates = {}\n","    for term in np.unique(query_to_search):\n","        if term in words:            \n","            list_of_doc = pls[words.index(term)]            \n","            normlized_tfidf = [(doc_id,(freq/DL[str(doc_id)])*math.log(len(DL)/index.df[term],10)) for doc_id, freq in list_of_doc]\n","            \n","            for doc_id, tfidf in normlized_tfidf:\n","                candidates[(doc_id,term)] = candidates.get((doc_id,term), 0) + tfidf               \n","\n","    return candidates\n","\n","\n","def generate_document_tfidf_matrix(query_to_search,index,words,pls):\n","    \"\"\"\n","    Generate a DataFrame `D` of tfidf scores for a given query. \n","    Rows will be the documents candidates for a given query\n","    Columns will be the unique terms in the index.\n","    The value for a given document and term will be its tfidf score.\n","    \n","    Parameters:\n","    -----------\n","    query_to_search: list of tokens (str). This list will be preprocessed in advance (e.g., lower case, filtering stopwords, etc.'). \n","                     Example: 'Hello, I love information retrival' --->  ['hello','love','information','retrieval']\n","\n","    index:           inverted index loaded from the corresponding files.\n","\n","    \n","    words,pls: iterator for working with posting.\n","\n","    Returns:\n","    -----------\n","    DataFrame of tfidf scores.\n","    \"\"\"\n","    \n","    total_vocab_size = len(index.term_total)\n","    candidates_scores = get_candidate_documents_and_scores(query_to_search,index,words,pls) #We do not need to utilize all document. Only the docuemnts which have corrspoinding terms with the query.\n","    unique_candidates = np.unique([doc_id for doc_id, freq in candidates_scores.keys()])\n","    D = np.zeros((len(unique_candidates), total_vocab_size))\n","    D = pd.DataFrame(D)\n","    \n","    D.index = unique_candidates\n","    D.columns = index.term_total.keys()\n","\n","    for key in candidates_scores:\n","        tfidf = candidates_scores[key]\n","        doc_id, term = key    \n","        D.loc[doc_id][term] = tfidf\n","\n","    return D\n","\n","\n","def cosine_similarity(D,Q):\n","    \"\"\"\n","    Calculate the cosine similarity for each candidate document in D and a given query (e.g., Q).\n","    Generate a dictionary of cosine similarity scores \n","    key: doc_id\n","    value: cosine similarity score\n","    \n","    Parameters:\n","    -----------\n","    D: DataFrame of tfidf scores.\n","\n","    Q: vectorized query with tfidf scores\n","    \n","    Returns:\n","    -----------\n","    dictionary of cosine similarity score as follows:\n","                                                                key: document id (e.g., doc_id)\n","                                                                value: cosine similarty score.\n","    \"\"\"\n","    # YOUR CODE HERE\n","    return {i: sum(R * Q) / (math.sqrt(pow(R, 2).sum() * pow(Q, 2).sum())) for i, R in D.iterrows()}\n","\n","\n","def get_top_n(sim_dict,N=3):\n","    \"\"\" \n","    Sort and return the highest N documents according to the cosine similarity score.\n","    Generate a dictionary of cosine similarity scores \n","   \n","    Parameters:\n","    -----------\n","    sim_dict: a dictionary of similarity score as follows:\n","                                                                key: document id (e.g., doc_id)\n","                                                                value: similarity score. We keep up to 5 digits after the decimal point. (e.g., round(score,5))\n","\n","    N: Integer (how many documents to retrieve). By default N = 3\n","    \n","    Returns:\n","    -----------\n","    a ranked list of pairs (doc_id, score) in the length of N.\n","    \"\"\"\n","    \n","    return sorted([(doc_id,round(score,5)) for doc_id, score in sim_dict.items()], key = lambda x: x[1],reverse=True)[:N]\n","\n","\n","def get_topN_score_for_queries(queries_to_search,index,N=3):\n","    \"\"\" \n","    Generate a dictionary that gathers for every query its topN score.\n","    \n","    Parameters:\n","    -----------\n","    queries_to_search: a dictionary of queries as follows: \n","                                                        key: query_id\n","                                                        value: list of tokens.\n","    index:           inverted index loaded from the corresponding files.    \n","    N: Integer. How many documents to retrieve. This argument is passed to the topN function. By default N = 3, for the topN function. \n","    \n","    Returns:\n","    -----------\n","    return: a dictionary of queries and topN pairs as follows:\n","                                                        key: query_id\n","                                                        value: list of pairs in the following format:(doc_id, score). \n","    \"\"\"\n","    # YOUR CODE HERE\n","    w, p = get_posting_iter(index)  # TODO: not usable on GCP-scale data\n","    return {i: get_top_n(cosine_similarity(\n","                         generate_document_tfidf_matrix(q, index, w, p), generate_query_tfidf_vector(q, index)), N=N) \n","               for i, q in queries_to_search.items()}\n","\n","# tfidf_queries_score_train = get_topN_score_for_queries(cran_txt_query_text_train,idx_title)\n","\n","\n","def merge_results(title_scores,body_scores, anchor_scores, title_weight=1/3, text_weight=1/3, anchor_weight=1/3, N = 3):    \n","    \"\"\"\n","    This function merge and sort documents retrieved by its weighted score (title, body and anchor). \n","\n","    Parameters:\n","    -----------\n","    title_scores: a dictionary build upon the title index of queries and tuples representing scores as follows: \n","                                                                            key: query_id\n","                                                                            value: list of pairs in the following format:(doc_id,score)\n","                \n","    body_scores: a dictionary build upon the body/text index of queries and tuples representing scores as follows: \n","                                                                            key: query_id\n","                                                                            value: list of pairs in the following format:(doc_id,score)\n","    \n","    anchor_scores: a dictionary build upon the anchor index of queries and tuples representing scores as follows: \n","                                                                            key: query_id\n","                                                                            value: list of pairs in the following format:(doc_id,score)\n","\n","    title_weight: float, for weighted average utilizing title, body and anchor scores\n","    text_weight: float, for weighted average utilizing title, body and anchor scores\n","    anchor_weight: float, for weighted average utilizing title, body and anchor scores\n","\n","    N: Integer. How many document to retrieve. This argument is passed to topN function. By default N = 3, for the topN function. \n","    \n","    Returns:\n","    -----------\n","    dictionary of querires and topN pairs as follows:\n","                                                        key: query_id\n","                                                        value: list of pairs in the following format:(doc_id,score). \n","    \"\"\"\n","    # YOUR CODE HERE\n","    titles = {i: [(doc_id, title_weight * score) for doc_id, score in doc_id_score] for i, doc_id_score in title_scores.items()}\n","    bodies = {j: [(doc_id, text_weight * score) for doc_id, score in doc_id_score] for j, doc_id_score in body_scores.items()}\n","    anchors = {j: [(doc_id, anchor_weight * score) for doc_id, score in doc_id_score] for j, doc_id_score in anchor_scores.items()}\n","    merged = {}\n","    for (i, title), (j, body), (k, anchor) in zip(titles.items(), bodies.items(), anchors.items()):\n","      concat = title + body + anchor\n","      merged[i] = defaultdict(int)\n","      for doc_id, score in concat:\n","          merged[i][doc_id] += score\n","      merged[i] = sorted(list(merged[i].items()), key=lambda doc_id_score: doc_id_score[1], reverse=True)[:N]\n","    return merged\n","\n","# thirds = merge_results(bm25_queries_score_train_title,bm25_queries_score_train_body, bm25_queries_score_train_anchor)        \n"]},{"cell_type":"markdown","source":["### Colab / GCP Moderator"],"metadata":{"id":"lXenH0KUDo2R"}},{"cell_type":"code","source":["is_gcp = False # Whether a colab debug run or GCP run is desired"],"metadata":{"id":"Hql356ioDpTA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SYvvc0JfPESu"},"source":["### Create Inverted Index"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h8IJiX5wPDlA","executionInfo":{"status":"ok","timestamp":1673552640923,"user_tz":-120,"elapsed":94771,"user":{"displayName":"Ido Paretsky","userId":"00866448096400107402"}},"outputId":"b61592d3-b7f9-499a-bbba-3efbd41c12fa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Updated property [core/project].\n","Copying gs://wikidata20210801_preprocessed/multistream1_preprocessed.parquet...\n","/ [1 files][316.7 MiB/316.7 MiB]                                                \n","Operation completed over 1 objects/316.7 MiB.                                    \n"]}],"source":["if not is_gcp:  # colab debug run\n","  # Initializing spark context\n","  # create a spark context and session\n","  conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n","  conf.set(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.2-s_2.12\")\n","  sc = SparkContext.getOrCreate(conf=conf)\n","  sc.addPyFile(str(Path(spark_jars) / Path(graphframes_jar).name))\n","  spark = SparkSession.builder.getOrCreate()\n","\n","\n","  # Authenticate your user\n","  # The authentication should be done with the email connected to your GCP account\n","  from google.colab import auth\n","  import signal\n","\n","  AUTH_TIMEOUT = 6000\n","\n","  def handler(signum, frame):\n","    raise Exception(\"Authentication timeout!\")\n","\n","  signal.signal(signal.SIGALRM, handler)\n","  signal.alarm(AUTH_TIMEOUT)\n","\n","  try:\n","    auth.authenticate_user()\n","  except: \n","    pass\n","\n","\n","  # Copy one wikidumps files \n","  import os\n","  from pathlib import Path\n","  from google.colab import auth\n","  ## RENAME the project_id to yours project id from the project you created in GCP \n","  project_id = 'crypto-lexicon-370515'\n","  !gcloud config set project {project_id}\n","\n","  data_bucket_name = 'wikidata20210801_preprocessed'\n","  try:\n","      if os.environ[\"wikidata20210801_preprocessed\"] is not None:\n","          pass  \n","  except:\n","        !mkdir wikidumps\n","        !gsutil -u {project_id} cp gs://{data_bucket_name}/multistream1_preprocessed.parquet \"wikidumps/\" \n","\n","\n","  try:\n","      if os.environ[\"wikidata20210801_preprocessed\"] is not None:\n","        path = os.environ[\"wikidata20210801_preprocessed\"]+\"/wikidumps/*\"\n","  except:\n","        path = \"wikidumps/*\"\n","\n","  parquetFile = spark.read.parquet(path)\n","  # take the 'title', 'text', 'anchor_text' and 'id' or the first 1000 rows and create an RDD from it\n","  doc_title_text_anchor_quadruplets = parquetFile.limit(1000).select(\"title\", \"text\", \"anchor_text\", \"id\").rdd\n","\n","  from inverted_index_colab import *\n","\n","else:  # GCP run\n","  # Put your bucket name below and make sure you can access it without an error\n","  bucket_name = '318419512_318510252' \n","  full_path = 'gs://318419512_318510252/'\n","  paths=[]\n","\n","  client = storage.Client()\n","  blobs = client.list_blobs(bucket_name)\n","  for b in blobs:\n","      if b.name.startswith(\"multistream\"):  # Parse wikidump data only\n","          paths.append(full_path+b.name)\n","\n","\n","  parquetFile = spark.read.parquet(*paths)\n","  doc_title_text_anchor_quadruplets = parquetFile.select(\"title\", \"text\", \"anchor_text\", \"id\").rdd\n","  # if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n","  %cd -q /home/dataproc\n","  !ls inverted_index_gcp.py\n","\n","  # adding our python module to the cluster\n","  sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n","  sys.path.insert(0,SparkFiles.getRootDirectory())\n","\n","  from inverted_index_gcp import InvertedIndex\n","\n","title_w2df_dict, title_posting_locs_list, title_DL_dict = count_words_and_write_filtered_posting_list(doc_title_text_anchor_quadruplets, 'title')  # Title\n","title_inverted = create_and_upload_inverted_index_instance(title_w2df_dict, title_posting_locs_list, title_DL_dict, 'title')\n","\n","body_w2df_dict, body_posting_locs_list, body_DL_dict = count_words_and_write_filtered_posting_list(doc_title_text_anchor_quadruplets, 'body')  # Body\n","body_inverted = create_and_upload_inverted_index_instance(body_w2df_dict, body_posting_locs_list, body_DL_dict, 'body')\n","\n","anchor_w2df_dict, anchor_posting_locs_list, anchor_DL_dict = count_words_and_write_filtered_posting_list(doc_title_text_anchor_quadruplets, 'anchor')  # Anchor Text\n","anchor_inverted = create_and_upload_inverted_index_instance(anchor_w2df_dict, anchor_posting_locs_list, anchor_DL_dict, 'anchor')"]},{"cell_type":"markdown","source":["### Download August 2021 PageViews"],"metadata":{"id":"BHVqGTLMfPq4"}},{"cell_type":"code","source":["# Paths\n","# Using user page views (as opposed to spiders and automated traffic) for the \n","# month of August 2021\n","pv_path = 'https://dumps.wikimedia.org/other/pageview_complete/monthly/2021/2021-08/pageviews-202108-user.bz2'\n","p = Path(pv_path) \n","pv_name = p.name\n","pv_temp = f'{p.stem}-4dedup.txt'\n","pv_clean = f'{p.stem}.pkl'\n","\"\"\"\n","# Download the file (2.3GB) \n","!wget -N $pv_path\n","# Filter for English pages, and keep just two fields: article ID (3) and monthly \n","# total number of page views (5). Then, remove lines with article id or page \n","# view values that are not a sequence of digits.\n","!bzcat $pv_name | grep \"^en\\.wikipedia\" | cut -d' ' -f3,5 | grep -P \"^\\d+\\s\\d+$\" > $pv_temp\n","# Create a Counter (dictionary) that sums up the pages views for the same \n","# article, resulting in a mapping from article id to total page views.\n","wid2pv = Counter()\n","with open(pv_temp, 'rt') as f:\n","  for line in f:\n","    parts = line.split(' ')\n","    wid2pv.update({int(parts[0]): int(parts[1])})\n","# write out the counter as binary file (pickle it)\n","with open(pv_clean, 'wb') as f:\n","  pickle.dump(wid2pv, f)\n","\"\"\"\n","# read in the counter\n","if not is_gcp:\n","  with open(pv_clean, 'rb') as f:  # PageViews saved in different paths in colab / GCP\n","    wid2pv = pickle.loads(f.read())\n","else:\n","  os.environ['KMP_DUPLICATE_LIB_OK']='True'  # Prevents kernel death\n","  client = storage.Client()\n","  blobs = client.list_blobs(bucket_name)\n","  for b in blobs:\n","    if b.name.startswith(\"pageviews\"):  # Parse pageviews data only\n","        pv = b\n","    wid2pv =  pickle.loads(pv.download_as_bytes())\n","\n","pageviews = spark.createDataFrame(data=wid2pv.items(), schema = [\"id\", \"pageviews\"])"],"metadata":{"id":"XsSXSxqxfTFk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Calculate PageRank"],"metadata":{"id":"5KZehKm-Dhwv"}},{"cell_type":"code","source":["if not is_gcp:\n","  pages_links = doc_title_text_anchor_quadruplets.toDF().select(\"id\", \"anchor_text\").rdd\n","else:\n","  pages_links = doc_title_text_anchor_quadruplets.toDF().select(\"id\", \"anchor_text\").rdd\n","\n","# construct the graph \n","edges, vertices = generate_graph(pages_links)\n","# compute PageRank\n","edgesDF = edges.toDF(['src', 'dst']).repartition(4 if not is_gcp else NUM_BUCKETS, 'src')\n","verticesDF = vertices.toDF(['id']).repartition(4 if not is_gcp else NUM_BUCKETS, 'id')\n","g = GraphFrame(verticesDF, edgesDF)\n","pr_results = g.pageRank(resetProbability=0.15, maxIter=6)\n","pr = pr_results.vertices.select(\"id\", \"pagerank\")\n","pr = pr.sort(col('pagerank').desc())\n","pr = pr.join(doc_title_text_anchor_quadruplets.toDF().select('title', 'id'), ['id'])  # Add 'title' column to df (inner join)\n","pr = pr.join(pageviews, ['id'], \"left\")  # Add 'title' column to df (left join - so we won't lose documents whose PageViews are unavailable)\n","if not is_gcp:  # colab debug run\n","  pr.repartition(1).write.mode(\"overwrite\").option(\"header\", True).csv('pr', compression=\"gzip\")\n","else:  # GCP\n","  pr.repartition(1).write.option(\"header\", True).csv(f'gs://{bucket_name}/pr', compression=\"gzip\")\n","pr.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a5o2FpcFDiEF","executionInfo":{"status":"ok","timestamp":1673286252475,"user_tz":-120,"elapsed":57766,"user":{"displayName":"Ido Paretsky","userId":"00866448096400107402"}},"outputId":"e1bd8fc2-9f49-41d0-bc07-834d447706f3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/pyspark/sql/dataframe.py:148: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n","  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"]},{"output_type":"stream","name":"stdout","text":["+----+------------------+--------------------+---------+\n","|  id|          pagerank|               title|pageviews|\n","+----+------------------+--------------------+---------+\n","|  25|1.0035612986927371|              Autism|   114746|\n","|  39|1.0026870119556135|              Albedo|    24532|\n","| 307|1.0254783346689493|     Abraham Lincoln|   343343|\n","| 309|0.9925092965518939|An American in Paris|     2961|\n","| 316|0.9953212472574045|Academy Award for...|     7167|\n","| 324|1.0288431319935691|      Academy Awards|   223758|\n","| 330|0.9925092965518939|             Actrius|       39|\n","| 332|0.9925092965518939|     Animalia (book)|     1199|\n","| 339|1.0378111940404706|            Ayn Rand|   126270|\n","| 586|1.0751315280059792|               ASCII|   129712|\n","| 595| 0.996008755380578|        Andre Agassi|   112215|\n","| 621|1.0110466331111123|           Amphibian|    53451|\n","| 748|1.0269635040224874|   Amateur astronomy|     3429|\n","| 854|1.4336147286826126|            Anatolia|    77058|\n","| 929| 1.023519846198868|               Alpha|    41093|\n","|1023|0.9951320818197642|  Anarcho-capitalism|    43304|\n","|1130|1.0064197074782166|            Avicenna|    50390|\n","|1174| 1.056715749202429|           Aphrodite|   108484|\n","|1187|1.0292090128011269|               Alloy|    25513|\n","|1371|0.9937399922581601|            Ambracia|      777|\n","+----+------------------+--------------------+---------+\n","only showing top 20 rows\n","\n"]}]},{"cell_type":"code","source":["# POST requests testing\n","import requests\n","requests.get('http://104.197.198.70:8080/search_body?query=american').json()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":328},"id":"Z2fDU63GPddV","executionInfo":{"status":"error","timestamp":1673700511053,"user_tz":-120,"elapsed":354,"user":{"displayName":"Ido Paretsky","userId":"00866448096400107402"}},"outputId":"f882387c-4c92-411e-8b75-65d6a10a2b05"},"execution_count":9,"outputs":[{"output_type":"error","ename":"JSONDecodeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-e143be948397>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# POST requests testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'http://104.197.198.70:8080/search_body?query=american'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    898\u001b[0m                     \u001b[0;31m# used.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.8/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.8/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.8/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"]}]},{"cell_type":"code","source":["# POST requests testing\n","import requests\n","requests.post('http://104.197.198.70:8080/get_pagerank', json=[12, 25]).json()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6XuDWNpufre0","executionInfo":{"status":"ok","timestamp":1673700601303,"user_tz":-120,"elapsed":289,"user":{"displayName":"Ido Paretsky","userId":"00866448096400107402"}},"outputId":"660fedef-7e03-4504-fd49-42ea9309acbb"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['Anarchism', 147.67148], ['Autism', 59.443592]]"]},"metadata":{},"execution_count":13}]}],"metadata":{"colab":{"provenance":[{"file_id":"1QPJ-IKdhwECSNcfJCOoqdCuP1fcilYPf","timestamp":1673084051920}],"collapsed_sections":["TsPEAk-X5GRE"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}