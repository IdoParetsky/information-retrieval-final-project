{"cells":[{"cell_type":"markdown","metadata":{"id":"pBIYmR2y0jQK"},"source":["# Information Retrieval Final Project\n","## Bar Dolev 318419512\n","## Ido Paretsky 318510252\n","\n","### https://github.com/IdoParetsky/information-retrieval-final-project\n","### https://drive.google.com/drive/u/0/folders/1LUf_YLUbEo4Qj1CTqvKgqYiMgHy9Ejgf"]},{"cell_type":"markdown","metadata":{"id":"QijpDuYg0i0J"},"source":["### GitHub Repository init. from within Google Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"elapsed":435,"status":"ok","timestamp":1673004939384,"user":{"displayName":"Ido Paretsky","userId":"00866448096400107402"},"user_tz":-120},"id":"nZpg47mx0iSL","outputId":"9c38bc54-783b-4f55-fbbc-4341c1bf36c0"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\\n%cd /content/drive/MyDrive/Information Retrieval/Project\\n!git init information-retrieval-final-project\\n\""]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/Information Retrieval/Project\n","!git init information-retrieval-final-project\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"n1sfpWma-Eut"},"source":["###Git Commit & Push"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"X_v8hDGz1gnW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673824797671,"user_tz":-120,"elapsed":4055,"user":{"displayName":"Ido Paretsky","userId":"00866448096400107402"}},"outputId":"a2e3f18e-fe67-44c7-942d-3d37d16526cb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/Information Retrieval/Project/information-retrieval-final-project\n","error: cannot pull with rebase: You have unstaged changes.\n","error: please commit or stash them.\n","On branch master\n","Your branch and 'origin/master' have diverged,\n","and have 2 and 5 different commits each, respectively.\n","  (use \"git pull\" to merge the remote branch into yours)\n","\n","Changes to be committed:\n","  (use \"git reset HEAD <file>...\" to unstage)\n","\n","\t\u001b[32mmodified:   IR_Project\u001b[m\n","\n","Changes not staged for commit:\n","  (use \"git add/rm <file>...\" to update what will be committed)\n","  (use \"git checkout -- <file>...\" to discard changes in working directory)\n","\n","\t\u001b[31mmodified:   README.md\u001b[m\n","\t\u001b[31mdeleted:    colab_cache/run_frontend_in_colab.ipynb\u001b[m\n","\t\u001b[31mdeleted:    gcp_cache/gcp_indexes/anchor_index.pkl\u001b[m\n","\t\u001b[31mdeleted:    gcp_cache/gcp_indexes/body_index.pkl\u001b[m\n","\t\u001b[31mdeleted:    gcp_cache/gcp_indexes/title_index.pkl\u001b[m\n","\t\u001b[31mmodified:   gcp_cache/inverted_index_gcp.py\u001b[m\n","\t\u001b[31mdeleted:    gcp_cache/link to gcp_pagerank_pageviews.csv.gz.txt\u001b[m\n","\t\u001b[31mdeleted:    gcp_cache/link to postings_gcp.zip.txt\u001b[m\n","\t\u001b[31mdeleted:    gcp_cache/ngrok\u001b[m\n","\t\u001b[31mdeleted:    gcp_cache/ngrok-stable-linux-amd64.zip\u001b[m\n","\t\u001b[31mmodified:   gcp_cache/search_frontend.py\u001b[m\n","\n","Untracked files:\n","  (use \"git add <file>...\" to include in what will be committed)\n","\n","\t\u001b[31mReport.gdoc\u001b[m\n","\t\u001b[31mReport.pptx\u001b[m\n","\t\u001b[31mcolab_cache/run_frontend_in_colab_test_gcp.ipynb\u001b[m\n","\t\u001b[31mgcp_cache/gcp_pagerank_pageviews.csv.gz\u001b[m\n","\t\u001b[31mgcp_cache/id_title_pr_pv_dict_cast.pkl\u001b[m\n","\t\u001b[31mwikidumps/\u001b[m\n","\n","[master e2e2644] Final Commit, over and out\n"," 1 file changed, 1 insertion(+), 1 deletion(-)\n"," rewrite IR_Project (95%)\n","fatal: remote origin already exists.\n","origin\thttps://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@github.com/IdoParetsky/information-retrieval-final-project.git (fetch)\n","origin\thttps://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@github.com/IdoParetsky/information-retrieval-final-project.git (push)\n","To https://github.com/IdoParetsky/information-retrieval-final-project.git\n"," ! [rejected]        master -> master (non-fast-forward)\n","error: failed to push some refs to 'https://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@github.com/IdoParetsky/information-retrieval-final-project.git'\n","hint: Updates were rejected because the tip of your current branch is behind\n","hint: its remote counterpart. Integrate the remote changes (e.g.\n","hint: 'git pull ...') before pushing again.\n","hint: See the 'Note about fast-forwards' in 'git push --help' for details.\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/Information Retrieval/Project/information-retrieval-final-project/\n","!git config --global user.email \"ido.paretsky@gmail.com\"\n","!git config --global user.name \"IdoParetsky\"\n","!git pull --rebase\n","!git add IR_Project\n","!git status\n","\n","!git commit -m \"Final Commit, over and out\"\n","\n","USERNAME = \"IdoParetsky\"\n","REPOSITORY = \"information-retrieval-final-project\"\n","GIT_TOKEN = \"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\"\n","\n","!git remote add origin https://{GIT_TOKEN}@github.com/{USERNAME}/{REPOSITORY}.git\n","!git remote -v\n","!git push -u origin master"]},{"cell_type":"markdown","source":["### Git Add & Status"],"metadata":{"id":"PnhLGcidjCqt"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/Information Retrieval/Project/information-retrieval-final-project/\n","!git config --global user.email \"ido.paretsky@gmail.com\"\n","!git config --global user.name \"IdoParetsky\"\n","!git pull\n","# Exclude giant files\n","!git add --all -- :! Report.pptx :!Report.gdoc :!wikidumps/ :!gcp_cache/postings_gcp.zip :!gcp_cache/gcp_pagerank_pageviews.csv.gz  :!gcp_cache/gcp_pagerank_pageviews.csv.gz :!gcp_cache/id_title_pr_pv_dict_cast.pkl :!gcp_cache/gcp_indexes/anchor_index.pkl :!gcp_cache/gcp_indexes/body_index.pkl :!gcp_cache/gcp_indexes/title_index.pkl\n","!git status"],"metadata":{"id":"WK4U1yse1zjQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673825036405,"user_tz":-120,"elapsed":3892,"user":{"displayName":"Ido Paretsky","userId":"00866448096400107402"}},"outputId":"a792fdcf-a8ab-4828-c55b-43e6c4759c72"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/Information Retrieval/Project/information-retrieval-final-project\n","error: You have not concluded your merge (MERGE_HEAD exists).\n","hint: Please, commit your changes before merging.\n","fatal: Exiting because of unfinished merge.\n","On branch master\n","Your branch and 'origin/master' have diverged,\n","and have 3 and 5 different commits each, respectively.\n","  (use \"git pull\" to merge the remote branch into yours)\n","\n","All conflicts fixed but you are still merging.\n","  (use \"git commit\" to conclude merge)\n","\n","Changes to be committed:\n","\n","\t\u001b[32mnew file:   Report.docx\u001b[m\n","\t\u001b[32mnew file:   Report.pptx\u001b[m\n","\t\u001b[32mdeleted:    gcp_cache/gcp_indexes/anchor_index.pkl\u001b[m\n","\t\u001b[32mdeleted:    gcp_cache/gcp_indexes/body_index.pkl\u001b[m\n","\t\u001b[32mdeleted:    gcp_cache/gcp_indexes/title_index.pkl\u001b[m\n","\t\u001b[32mmodified:   gcp_cache/link to gcp_pagerank_pageviews.csv.gz.txt\u001b[m\n","\t\u001b[32mmodified:   gcp_cache/link to postings_gcp.zip.txt\u001b[m\n","\t\u001b[32mnew file:   link to id_title_pr_pv_dict_cast.pkl.txt\u001b[m\n","\n","Changes not staged for commit:\n","  (use \"git add/rm <file>...\" to update what will be committed)\n","  (use \"git checkout -- <file>...\" to discard changes in working directory)\n","\n","\t\u001b[31mmodified:   IR_Project\u001b[m\n","\t\u001b[31mmodified:   README.md\u001b[m\n","\t\u001b[31mdeleted:    colab_cache/run_frontend_in_colab.ipynb\u001b[m\n","\t\u001b[31mmodified:   gcp_cache/inverted_index_gcp.py\u001b[m\n","\t\u001b[31mdeleted:    gcp_cache/ngrok\u001b[m\n","\t\u001b[31mdeleted:    gcp_cache/ngrok-stable-linux-amd64.zip\u001b[m\n","\t\u001b[31mmodified:   gcp_cache/search_frontend.py\u001b[m\n","\n","Untracked files:\n","  (use \"git add <file>...\" to include in what will be committed)\n","\n","\t\u001b[31mReport.gdoc\u001b[m\n","\t\u001b[31mcolab_cache/run_frontend_in_colab_test_gcp.ipynb\u001b[m\n","\t\u001b[31mgcp_cache/gcp_pagerank_pageviews.csv.gz\u001b[m\n","\t\u001b[31mgcp_cache/id_title_pr_pv_dict_cast.pkl\u001b[m\n","\t\u001b[31mwikidumps/\u001b[m\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"6wl97Fl8czLX"},"source":["### Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7LQSHUUicyup"},"outputs":[],"source":["from functools import partial\n","from xml.etree import ElementTree\n","import csv\n","import math\n","\n","import bz2\n","from collections import Counter, OrderedDict, defaultdict\n","import heapq\n","import codecs\n","import os\n","from operator import itemgetter\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from pathlib import Path\n","from time import time\n","import hashlib\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","import nltk\n","nltk.download('stopwords')\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import pandas as pd\n","from sklearn.metrics.pairwise import cosine_similarity\n","import re\n","import pickle\n","import numpy as np\n","\n","!gcloud dataproc clusters list --region us-central1\n","!pip install -q google-cloud-storage==1.43.0\n","!pip install -q graphframes\n","!pip install -q pyspark\n","!pip install -U -q PyDrive\n","!apt install openjdk-8-jdk-headless -qq\n","!pip install -q graphframes\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","graphframes_jar = 'https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.2-s_2.12/graphframes-0.8.2-spark3.2-s_2.12.jar'\n","spark_jars = '/usr/local/lib/python3.7/dist-packages/pyspark/jars'\n","!wget -N -P $spark_jars $graphframes_jar\n","import pyspark\n","from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","import pyspark.sql.functions as f\n","from graphframes import *\n","\n","import sys\n","from itertools import islice, count, groupby, chain\n","from time import time\n","from google.cloud import storage\n","\n","from tqdm import tqdm\n","from contextlib import closing\n","\n","import json\n","from io import StringIO\n","\n","!gcloud dataproc clusters list --region us-central1\n","!ls -l /usr/lib/spark/jars/graph*"]},{"cell_type":"markdown","metadata":{"id":"4BeQXPCn1Dkg"},"source":["### Utilities"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3a2SxV06xAmM"},"outputs":[],"source":["english_stopwords = frozenset(stopwords.words('english'))\n","# We queried ChatGPT for Wikipedia-specific StopWords and added some of our own\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n","                    \"first\", \"see\", \"people\", \"one\", \"two\", \n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n","                    \"many\", \"however\", \"would\", \"became\", \"page\", \"article\",\n","                    \"reference\", \"source\", \"content\",\n","                    \"fact\", \"year\", \"date\", \"place\", \"wiki\",\n","                    \"edit\", \"version\", \"user\", \"talk\", \"discussion\", \"template\",\n","                    \"category\", \"project\", \"author\", \"writer\",\n","                    \"creator\", \"publisher\", \"editor\", \"publication\", \"edition\",\n","                    \"issue\", \"chapter\"]\n","\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","\n","group_to_idx = {'title': 0, 'body': 1, 'anchor': 2, 'id': 3}\n","\n","NUM_BUCKETS = 124  \n","def token2bucket_id(token, group):\n","  return int(_hash(token),16) % NUM_BUCKETS + (NUM_BUCKETS * group_to_idx[group])  # group: 0 - 'title', 1 - 'body', 2 - 'anchor'\n","\n","# PLACE YOUR CODE HERE\n","def word_count(group, id):\n","  ''' Count the frequency of each word in group (tf of title, text or anchor_text) that is not included in \n","  `all_stopwords` and return entries that will go into our posting lists. \n","  Parameters:\n","  -----------\n","    group: str\n","      Title, Text or Anchor Text of one document\n","    id: int\n","      Document id\n","  Returns:\n","  --------\n","    List of tuples\n","      A list of (token, (doc_id, tf)) pairs \n","      for example: [(\"Anarchism\", (12, 5)), ...]\n","  '''\n","  if type(group) != str:\n","    group = ''.join([f'{x + \" \" if type(x)==str else \"\"}{y + \" \" if type(y) ==str else \"\"}' for x, y in group])\n","  tokens = [token.group() for token in RE_WORD.finditer(group.lower())]\n","  tokens = [t for t in tokens if t not in all_stopwords]\n","  word_counts = Counter()\n","  for w in tokens:\n","    count = word_counts.get(w, None)\n","    if not count:\n","        word_counts[w] = (id, 1)\n","    else:\n","      word_counts[w] = (id, word_counts.get(w)[1] + 1)\n","  return list(word_counts.items())\n","\n","def calculate_dl(group, id):\n","  ''' Calculate each document's length\n","  Parameters:\n","  -----------\n","    group: str\n","      Title, Text or Anchor Text of one document\n","    id: int\n","      Document id\n","  Returns:\n","  --------\n","    List of tuples\n","      A list of (doc_id, doc_len) pairs \n","      for example: [(12, 50), ...]\n","  '''\n","  if type(group) != str:\n","    group = ''.join([f'{x + \" \" if type(x)==str else \"\"}{y + \" \" if type(y) ==str else \"\"}' for x, y in group])\n","  tokens = [token.group() for token in RE_WORD.finditer(group.lower())]\n","  tokens = [t for t in tokens if t not in all_stopwords]\n","  return [(id, (len(tokens)))]\n"," \n","def reduce_word_counts(unsorted_pl):\n","  ''' Returns a sorted posting list by wiki_id.\n","  Parameters:\n","  -----------\n","    unsorted_pl: list of tuples\n","      A list of (wiki_id, tf) tuples \n","  Returns:\n","  --------\n","    list of tuples\n","      A sorted posting list.\n","  '''\n","  # YOUR CODE HERE\n","  return sorted(unsorted_pl, key=lambda t: t[0]) \n","\n","def calculate_df(postings):\n","  ''' Takes a posting list RDD and calculate the df for each token.\n","  Parameters:\n","  -----------\n","    postings: RDD\n","      An RDD where each element is a (token, posting_list) pair.\n","  Returns:\n","  --------\n","    RDD\n","      An RDD where each element is a (token, df) pair.\n","  '''\n","  # YOUR CODE HERE\n","  return postings.map(lambda t: (t[0], len(t[1])))\n","\n","def partition_postings_and_write(postings, group):\n","  ''' A function that partitions the posting lists into buckets, writes out \n","  all posting lists in a bucket to disk, and returns the posting locations for \n","  each bucket. Partitioning should be done through the use of `token2bucket` \n","  above. Writing to disk should use the function  `write_a_posting_list`, a \n","  static method implemented in inverted_index_colab/gcp.py under the InvertedIndex \n","  class. \n","  Parameters:\n","  -----------\n","    postings: RDD\n","      An RDD where each item is a (w, posting_list) pair.\n","    \n","    group: string \n","      'title', 'body' or 'anchor'\n","\n","  Returns:\n","  --------\n","    RDD\n","      An RDD where each item is a posting locations dictionary for a bucket. The\n","      posting locations maintain a list for each word of file locations and \n","      offsets its posting list was written to. See `write_a_posting_list_volab/gcp` for \n","      more details.\n","  '''\n","  # YOUR CODE HERE\n","  bucket_id_posting_locs_tup_list = []\n","  b_w_pl_rdd = postings.map(lambda w_pl: (token2bucket_id(w_pl[0], group), [w_pl])).groupByKey().mapValues(lambda w_pl_list: list(chain.from_iterable(w_pl_list)))\n","  #b_w_pl_rdd = postings.map(lambda w_pl: (token2bucket_id(w_pl[0], group), [w_pl])).reduceByKey(lambda a, b: a + b)\n","  if not is_gcp:\n","    return b_w_pl_rdd.map(lambda b_w_pl: InvertedIndex.write_a_posting_list(b_w_pl))\n","  else:\n","    return b_w_pl_rdd.map(lambda b_w_pl: InvertedIndex.write_a_posting_list(b_w_pl, '318419512_318510252'))\n","\n","def count_words_and_write_filtered_posting_list(doc_title_text_anchor_quadruplets, group):\n","  \"\"\"\n","    Binds together the usage of word_count, reduce_word_counts, posting list filtering, calculate_df,\n","    mapping w2df into a dictionary and writing the posting list to prevent code duplication, as these\n","    operations to be applied thrice - on Title, Body and Anchor Text.\n","    \n","    Parameters:\n","    -----------\n","    group: string \n","      'title', 'body' or 'anchor'. ('id' is used in every iteration)\n","    \n","    Returns:\n","    -----------\n","    2-Tuple of (w2df_dict, posting_locs_list)\n","  \"\"\"\n","  word_counts = doc_title_text_anchor_quadruplets.flatMap(lambda x: word_count(x[group_to_idx[group]], x[group_to_idx['id']]))\n","  DL_dict = doc_title_text_anchor_quadruplets.flatMap(lambda x: calculate_dl(x[group_to_idx[group]], x[group_to_idx['id']])).collectAsMap()\n","  postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n","  postings_filtered = postings.filter(lambda x: len(x[1])>(10 if not is_gcp else 50)) if group == 'body' else postings\n","  w2df = calculate_df(postings_filtered)\n","  w2df_dict = w2df.collectAsMap()\n","  posting_locs_list = partition_postings_and_write(postings_filtered, group).collect() if not is_gcp else []\n","  if is_gcp:\n","    _ = partition_postings_and_write(postings_filtered, group).collect()  # .pkl file saved to bucket\n","\n","  return w2df_dict, posting_locs_list, DL_dict\n","\n","def create_and_upload_inverted_index_instance(w2df_dict, posting_locs_list, DL_dict, group):\n","  \"\"\"\n","    Collects all posting lists locations into one super-set and create an inverted index instance\n","    \n","    Parameters:\n","    -----------\n","    w2df: dict \n","      Word to Document Frequency Dicationary\n","\n","    posting_locs_list: list\n","      List of posting lists' locations\n","\n","    DL: RDD\n","      RDD of doc_id and doc_len\n","\n","    group: str\n","      'title', 'body' or 'anchor'\n","    \n","    Returns:\n","    -----------\n","    inverted_index.InvertedIndex instance\n","  \"\"\"\n","  # collect all posting lists locations into one super-set\n","  if not is_gcp:  # colab\n","    super_posting_locs = defaultdict(list)\n","    for posting_loc in posting_locs_list:\n","      for k, v in posting_loc.items():\n","        super_posting_locs[k].extend(v)\n","  else:  # GCP\n","    super_posting_locs = defaultdict(list)\n","    for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n","      if not blob.name.endswith(\"pickle\"):\n","        continue\n","      with blob.open(\"rb\") as f:\n","        posting_locs = pickle.load(f)\n","        for k, v in posting_locs.items():\n","          super_posting_locs[k].extend(v)\n","\n","  # Create inverted index instance\n","  inverted = InvertedIndex()\n","  # Adding the posting locations dictionary to the inverted index\n","  inverted.posting_locs = super_posting_locs\n","  # Add the token - df dictionary to the inverted index\n","  inverted.df = w2df_dict\n","  inverted.DL = DL_dict\n","  # write the global stats out\n","  inverted.write_index(f'.', f'{group}_index')\n","  if is_gcp:\n","  # upload to gs\n","    index_src = f\"{group}_index.pkl\"\n","    index_dst = f'gs://{bucket_name}/postings_gcp/{index_src}'\n","    !gsutil cp $index_src $index_dst\n","    !gsutil ls -lh $index_dst\n","  return inverted\n","\n","def generate_graph(pages):\n","  ''' Compute the directed graph generated by wiki links.\n","  Parameters:\n","  -----------\n","    pages: RDD\n","      An RDD where each row consists of one wikipedia articles with 'id' and \n","      'anchor_text'.\n","  Returns:\n","  --------\n","    edges: RDD\n","      An RDD where each row represents an edge in the directed graph created by\n","      the wikipedia links. The first entry should the source page id and the \n","      second entry is the destination page id. No duplicates should be present. \n","    vertices: RDD\n","      An RDD where each row represents a vetrix (node) in the directed graph \n","      created by the wikipedia links. No duplicates should be present. \n","  '''\n","  # YOUR CODE HERE\n","  articles_to_link_lists = pages.mapValues(lambda anchor_text: [link[0] for link in anchor_text])\n","  edges = articles_to_link_lists.flatMap(lambda article_to_link_list: [(article_to_link_list[0], link) for link in article_to_link_list[1]]).distinct()\n","  vertices = edges.flatMap(lambda edge: edge).map(lambda v: (v,)).distinct()\n","  return edges, vertices\n","\n","# Self-made PageRanks, ended up using the given GraphFrame implementation\n","\"\"\"\n","def divide_rank(v, links, rank):\n","    ranks = [(v, 0)]  # Vertex Rank sent to itself\n","    if len(links) > 0:\n","        rank_sent = rank / len(links)\n","        for l in links:\n","            ranks.append((l, rank_sent))\n","    return ranks\n","\n","def PageRank(resetProbability=0.15, maxIter=10):\n","    page_cnt = pages_links.count()\n","\n","    pr = pages_links.mapValues(lambda weight: 1 / page_cnt)  # Initial PageRank of 1 divided by page_cnt\n","\n","    for i in range(maxIter):  # Instead of until convergence, compute MaxIter times\n","        weighted_pages = pages_links.join(pr).flatMap(lambda title_links_pr: \n","                                                           divide_rank(title_links_pr[0], title_links_pr[1][0], title_links_pr[1][1]))\n","\n","        # Sum rank sent to each node\n","        pr = weighted_pages.reduceByKey(lambda r1, r2: r1 + r2) \\\n","            .mapValues(lambda agg_rank: (resetProbability / page_cnt) + (agg_rank * (1 - resetProbability)))\n","\n","    return pr.sortBy(lambda p: p[1], ascending=False)\n","\n","pr = PageRank(resetProbability=0.15, maxIter=10)\n","pr.show()\n","\"\"\""]},{"cell_type":"markdown","source":["### Colab / GCP Moderator"],"metadata":{"id":"lXenH0KUDo2R"}},{"cell_type":"code","source":["is_gcp = False # Whether a colab debug run or GCP run is desired"],"metadata":{"id":"Hql356ioDpTA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SYvvc0JfPESu"},"source":["### Create Inverted Index"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h8IJiX5wPDlA","executionInfo":{"status":"ok","timestamp":1673552640923,"user_tz":-120,"elapsed":94771,"user":{"displayName":"Ido Paretsky","userId":"00866448096400107402"}},"outputId":"b61592d3-b7f9-499a-bbba-3efbd41c12fa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Updated property [core/project].\n","Copying gs://wikidata20210801_preprocessed/multistream1_preprocessed.parquet...\n","/ [1 files][316.7 MiB/316.7 MiB]                                                \n","Operation completed over 1 objects/316.7 MiB.                                    \n"]}],"source":["if not is_gcp:  # colab debug run\n","  # Initializing spark context\n","  # create a spark context and session\n","  conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n","  conf.set(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.2-s_2.12\")\n","  sc = SparkContext.getOrCreate(conf=conf)\n","  sc.addPyFile(str(Path(spark_jars) / Path(graphframes_jar).name))\n","  spark = SparkSession.builder.getOrCreate()\n","\n","\n","  # Authenticate your user\n","  # The authentication should be done with the email connected to your GCP account\n","  from google.colab import auth\n","  import signal\n","\n","  AUTH_TIMEOUT = 6000\n","\n","  def handler(signum, frame):\n","    raise Exception(\"Authentication timeout!\")\n","\n","  signal.signal(signal.SIGALRM, handler)\n","  signal.alarm(AUTH_TIMEOUT)\n","\n","  try:\n","    auth.authenticate_user()\n","  except: \n","    pass\n","\n","\n","  # Copy one wikidumps files \n","  import os\n","  from pathlib import Path\n","  from google.colab import auth\n","  ## RENAME the project_id to yours project id from the project you created in GCP \n","  project_id = 'crypto-lexicon-370515'\n","  !gcloud config set project {project_id}\n","\n","  data_bucket_name = 'wikidata20210801_preprocessed'\n","  try:\n","      if os.environ[\"wikidata20210801_preprocessed\"] is not None:\n","          pass  \n","  except:\n","        !mkdir wikidumps\n","        !gsutil -u {project_id} cp gs://{data_bucket_name}/multistream1_preprocessed.parquet \"wikidumps/\" \n","\n","\n","  try:\n","      if os.environ[\"wikidata20210801_preprocessed\"] is not None:\n","        path = os.environ[\"wikidata20210801_preprocessed\"]+\"/wikidumps/*\"\n","  except:\n","        path = \"wikidumps/*\"\n","\n","  parquetFile = spark.read.parquet(path)\n","  # take the 'title', 'text', 'anchor_text' and 'id' or the first 1000 rows and create an RDD from it\n","  doc_title_text_anchor_quadruplets = parquetFile.limit(1000).select(\"title\", \"text\", \"anchor_text\", \"id\").rdd\n","\n","  from inverted_index_colab import *\n","\n","else:  # GCP run\n","  # Put your bucket name below and make sure you can access it without an error\n","  bucket_name = '318419512_318510252' \n","  full_path = 'gs://318419512_318510252/'\n","  paths=[]\n","\n","  client = storage.Client()\n","  blobs = client.list_blobs(bucket_name)\n","  for b in blobs:\n","      if b.name.startswith(\"multistream\"):  # Parse wikidump data only\n","          paths.append(full_path+b.name)\n","\n","\n","  parquetFile = spark.read.parquet(*paths)\n","  doc_title_text_anchor_quadruplets = parquetFile.select(\"title\", \"text\", \"anchor_text\", \"id\").rdd\n","  # if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n","  %cd -q /home/dataproc\n","  !ls inverted_index_gcp.py\n","\n","  # adding our python module to the cluster\n","  sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n","  sys.path.insert(0,SparkFiles.getRootDirectory())\n","\n","  from inverted_index_gcp import InvertedIndex\n","\n","title_w2df_dict, title_posting_locs_list, title_DL_dict = count_words_and_write_filtered_posting_list(doc_title_text_anchor_quadruplets, 'title')  # Title\n","title_inverted = create_and_upload_inverted_index_instance(title_w2df_dict, title_posting_locs_list, title_DL_dict, 'title')\n","\n","body_w2df_dict, body_posting_locs_list, body_DL_dict = count_words_and_write_filtered_posting_list(doc_title_text_anchor_quadruplets, 'body')  # Body\n","body_inverted = create_and_upload_inverted_index_instance(body_w2df_dict, body_posting_locs_list, body_DL_dict, 'body')\n","\n","anchor_w2df_dict, anchor_posting_locs_list, anchor_DL_dict = count_words_and_write_filtered_posting_list(doc_title_text_anchor_quadruplets, 'anchor')  # Anchor Text\n","anchor_inverted = create_and_upload_inverted_index_instance(anchor_w2df_dict, anchor_posting_locs_list, anchor_DL_dict, 'anchor')"]},{"cell_type":"markdown","source":["### Download August 2021 PageViews"],"metadata":{"id":"BHVqGTLMfPq4"}},{"cell_type":"code","source":["# Paths\n","# Using user page views (as opposed to spiders and automated traffic) for the \n","# month of August 2021\n","pv_path = 'https://dumps.wikimedia.org/other/pageview_complete/monthly/2021/2021-08/pageviews-202108-user.bz2'\n","p = Path(pv_path) \n","pv_name = p.name\n","pv_temp = f'{p.stem}-4dedup.txt'\n","pv_clean = f'{p.stem}.pkl'\n","\"\"\"\n","# Download the file (2.3GB) \n","!wget -N $pv_path\n","# Filter for English pages, and keep just two fields: article ID (3) and monthly \n","# total number of page views (5). Then, remove lines with article id or page \n","# view values that are not a sequence of digits.\n","!bzcat $pv_name | grep \"^en\\.wikipedia\" | cut -d' ' -f3,5 | grep -P \"^\\d+\\s\\d+$\" > $pv_temp\n","# Create a Counter (dictionary) that sums up the pages views for the same \n","# article, resulting in a mapping from article id to total page views.\n","wid2pv = Counter()\n","with open(pv_temp, 'rt') as f:\n","  for line in f:\n","    parts = line.split(' ')\n","    wid2pv.update({int(parts[0]): int(parts[1])})\n","# write out the counter as binary file (pickle it)\n","with open(pv_clean, 'wb') as f:\n","  pickle.dump(wid2pv, f)\n","\"\"\"\n","# read in the counter\n","if not is_gcp:\n","  with open(pv_clean, 'rb') as f:  # PageViews saved in different paths in colab / GCP\n","    wid2pv = pickle.loads(f.read())\n","else:\n","  os.environ['KMP_DUPLICATE_LIB_OK']='True'  # Prevents kernel death\n","  client = storage.Client()\n","  blobs = client.list_blobs(bucket_name)\n","  for b in blobs:\n","    if b.name.startswith(\"pageviews\"):  # Parse pageviews data only\n","        pv = b\n","    wid2pv =  pickle.loads(pv.download_as_bytes())\n","\n","pageviews = spark.createDataFrame(data=wid2pv.items(), schema = [\"id\", \"pageviews\"])"],"metadata":{"id":"XsSXSxqxfTFk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Calculate PageRank"],"metadata":{"id":"5KZehKm-Dhwv"}},{"cell_type":"code","source":["if not is_gcp:\n","  pages_links = doc_title_text_anchor_quadruplets.toDF().select(\"id\", \"anchor_text\").rdd\n","else:\n","  pages_links = doc_title_text_anchor_quadruplets.toDF().select(\"id\", \"anchor_text\").rdd\n","\n","# construct the graph \n","edges, vertices = generate_graph(pages_links)\n","# compute PageRank\n","edgesDF = edges.toDF(['src', 'dst']).repartition(4 if not is_gcp else NUM_BUCKETS, 'src')\n","verticesDF = vertices.toDF(['id']).repartition(4 if not is_gcp else NUM_BUCKETS, 'id')\n","g = GraphFrame(verticesDF, edgesDF)\n","pr_results = g.pageRank(resetProbability=0.15, maxIter=6)\n","pr = pr_results.vertices.select(\"id\", \"pagerank\")\n","pr = pr.sort(col('pagerank').desc())\n","pr = pr.join(doc_title_text_anchor_quadruplets.toDF().select('title', 'id'), ['id'])  # Add 'title' column to df (inner join)\n","pr = pr.join(pageviews, ['id'], \"left\")  # Add 'title' column to df (left join - so we won't lose documents whose PageViews are unavailable)\n","if not is_gcp:  # colab debug run\n","  pr.repartition(1).write.mode(\"overwrite\").option(\"header\", True).csv('pr', compression=\"gzip\")\n","else:  # GCP\n","  pr.repartition(1).write.option(\"header\", True).csv(f'gs://{bucket_name}/pr', compression=\"gzip\")\n","pr.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a5o2FpcFDiEF","executionInfo":{"status":"ok","timestamp":1673286252475,"user_tz":-120,"elapsed":57766,"user":{"displayName":"Ido Paretsky","userId":"00866448096400107402"}},"outputId":"e1bd8fc2-9f49-41d0-bc07-834d447706f3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/pyspark/sql/dataframe.py:148: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n","  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"]},{"output_type":"stream","name":"stdout","text":["+----+------------------+--------------------+---------+\n","|  id|          pagerank|               title|pageviews|\n","+----+------------------+--------------------+---------+\n","|  25|1.0035612986927371|              Autism|   114746|\n","|  39|1.0026870119556135|              Albedo|    24532|\n","| 307|1.0254783346689493|     Abraham Lincoln|   343343|\n","| 309|0.9925092965518939|An American in Paris|     2961|\n","| 316|0.9953212472574045|Academy Award for...|     7167|\n","| 324|1.0288431319935691|      Academy Awards|   223758|\n","| 330|0.9925092965518939|             Actrius|       39|\n","| 332|0.9925092965518939|     Animalia (book)|     1199|\n","| 339|1.0378111940404706|            Ayn Rand|   126270|\n","| 586|1.0751315280059792|               ASCII|   129712|\n","| 595| 0.996008755380578|        Andre Agassi|   112215|\n","| 621|1.0110466331111123|           Amphibian|    53451|\n","| 748|1.0269635040224874|   Amateur astronomy|     3429|\n","| 854|1.4336147286826126|            Anatolia|    77058|\n","| 929| 1.023519846198868|               Alpha|    41093|\n","|1023|0.9951320818197642|  Anarcho-capitalism|    43304|\n","|1130|1.0064197074782166|            Avicenna|    50390|\n","|1174| 1.056715749202429|           Aphrodite|   108484|\n","|1187|1.0292090128011269|               Alloy|    25513|\n","|1371|0.9937399922581601|            Ambracia|      777|\n","+----+------------------+--------------------+---------+\n","only showing top 20 rows\n","\n"]}]},{"cell_type":"code","source":["# GET requests testing\n","import requests\n","requests.get('http://104.197.198.70:8080/search?query=elon+musk&norm_cos_sim=f').json()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z2fDU63GPddV","executionInfo":{"status":"ok","timestamp":1673773074342,"user_tz":-120,"elapsed":292,"user":{"displayName":"Ido Paretsky","userId":"00866448096400107402"}},"outputId":"b8e0964e-cfdf-49f7-f0d4-8f7785a1fd59"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[909036, 'Elon Musk'],\n"," [65175052, 'Views of Elon Musk'],\n"," [47190535, 'Elon Musk: Tesla, SpaceX, and the Quest for a Fantastic Future'],\n"," [65212863, 'List of awards and honors received by Elon Musk'],\n"," [66430882, 'Elon Musk in popular culture'],\n"," [68146980, 'Statue of Elon Musk'],\n"," [3875219, 'MuSK protein'],\n"," [318270, 'Musk deer'],\n"," [21523886, 'Kimbal Musk'],\n"," [8046414, 'Justine Musk'],\n"," [6973013, 'White-bellied musk deer'],\n"," [4555268, 'Siberian musk deer'],\n"," [50399439, 'Maye Musk'],\n"," [33986468, 'Tosca Musk'],\n"," [24019437, 'Synthetic musk'],\n"," [256010, 'Musk duck'],\n"," [29414816, 'Deer musk'],\n"," [21450611, 'Alpine musk deer'],\n"," [2456541, 'Musk strawberry'],\n"," [32383521, 'Kashmir musk deer'],\n"," [4214774, 'Musk lorikeet'],\n"," [220806, 'Musk'],\n"," [9134374, 'Dwarf musk deer'],\n"," [12508270, 'Giant musk turtle'],\n"," [3954738, 'Musk beetle'],\n"," [21389299, 'Loggerhead musk turtle'],\n"," [12540820, 'Black musk deer'],\n"," [20339194, 'Musk railway station'],\n"," [3524396, 'Askot Musk Deer Sanctuary'],\n"," [22176717, 'Musk xylene'],\n"," [32383649, 'Anhui musk deer'],\n"," [54154698, 'Le Musk'],\n"," [22248960, 'Musk, Victoria'],\n"," [12533085, \"Fraser's musk shrew\"],\n"," [27898072, 'New Zealand musk duck'],\n"," [12532233, \"Doucet's musk shrew\"],\n"," [5566541, 'Narrow-bridged musk turtle'],\n"," [12508292, 'Flattened musk turtle'],\n"," [57095288, 'Operation Musk Ox'],\n"," [43407192, 'The Musk Who Fell to Earth'],\n"," [5552715, 'Razor-backed musk turtle'],\n"," [13478982, 'Jōvan Musk'],\n"," [3040653, 'Musk stick'],\n"," [67208058, 'Jack Musk'],\n"," [12532560, 'Lesser red musk shrew'],\n"," [12532456, \"Peters's musk shrew\"],\n"," [56144073, 'Intermediate musk turtle'],\n"," [12508283, 'Mexican musk turtle'],\n"," [12532290, 'Greater red musk shrew'],\n"," [12532195, 'Reddish-gray musk shrew'],\n"," [12532418, 'Bicolored musk shrew'],\n"," [12533186, 'Lesser gray-brown musk shrew'],\n"," [12532811, 'Swamp musk shrew'],\n"," [41309419, 'Musk (disambiguation)'],\n"," [12533197, 'Desert musk shrew'],\n"," [12532805, 'Makwassie musk shrew'],\n"," [8048478, 'Musk mallow'],\n"," [12532223, 'Long-tailed musk shrew'],\n"," [12532886, 'Ugandan musk shrew'],\n"," [30137135, 'Musk station (Ontario)'],\n"," [67803326, 'Musk and Amber'],\n"," [5399134, 'Musk turtle'],\n"," [1001122, 'Elon University'],\n"," [127628, 'Elon, North Carolina'],\n"," [1321802, 'Amos Elon'],\n"," [1923823, 'Menachem Elon'],\n"," [22723639, 'Elon Phoenix football'],\n"," [15016135, 'Elon Phoenix'],\n"," [28371257, 'Elon Lindenstrauss'],\n"," [584836, 'Binyamin Elon'],\n"," [27668868, \"Elon Phoenix men's basketball\"],\n"," [6866267, 'Elon Moreh'],\n"," [1872269, 'Elon J. Farnsworth'],\n"," [3968318, 'Alumni Gym (Elon University)'],\n"," [26923199, \"Elon Phoenix men's soccer\"],\n"," [369869, 'Elon Peace Plan'],\n"," [4363181, 'Elon University School of Law'],\n"," [580391, 'Elon (Judges)'],\n"," [13473526, 'Elon Farnsworth (Michigan Attorney General)'],\n"," [2665945, 'Elon Gold'],\n"," [19579980, 'Elon R. Brown'],\n"," [21169599, 'Elon Phoenix baseball'],\n"," [13531757, 'Elon Lages Lima'],\n"," [22453622, \"Elon Phoenix women's basketball\"],\n"," [19629636, 'Elon Howard Eaton'],\n"," [7051653, 'Elon Hogsett'],\n"," [4625240, 'Mordechai Elon'],\n"," [25134620, 'Elon Galusha'],\n"," [14679086, 'Emuna Elon'],\n"," [33964400, \"2011–12 Elon Phoenix men's basketball team\"],\n"," [32151967, '2011 Elon Phoenix football team'],\n"," [803102, 'Elon (name)'],\n"," [5094567, 'Elon Gasper'],\n"," [22263517, 'Johnston Hall (Elon College, North Carolina)'],\n"," [19885589, 'Elon James White'],\n"," [4666466, 'Elon'],\n"," [24956646, 'Belk Library (Elon University)'],\n"," [27243750, 'Beth Elon'],\n"," [7085339, 'Campus of Elon University'],\n"," [23101038, 'Elon University Fire of the Carolinas Marching Band']]"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["# POST requests testing\n","import requests\n","requests.post('http://104.197.198.70:8080/get_pageview', json=[309]).json()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6XuDWNpufre0","executionInfo":{"status":"ok","timestamp":1673772965763,"user_tz":-120,"elapsed":271,"user":{"displayName":"Ido Paretsky","userId":"00866448096400107402"}},"outputId":"a5d014a1-0104-4b44-c981-545fe9ceb9c9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['An American in Paris', 2961.0]]"]},"metadata":{},"execution_count":2}]}],"metadata":{"colab":{"provenance":[{"file_id":"1QPJ-IKdhwECSNcfJCOoqdCuP1fcilYPf","timestamp":1673084051920}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}